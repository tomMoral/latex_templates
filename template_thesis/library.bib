Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@book{Oculo21,
author = {Leigh, R. John and Zee, David S.},
publisher = {Oxford University Press, USA},
title = {{The neurology of eye movements}},
year = {2015}
}
@article{larochelle2009exploring,
author = {Larochelle, Hugo and Bengio, Yoshua and Louradour, J{\'{e}}r{\^{o}}me and Lamblin, Pascal},
journal = {Journal of Machine Learning Research (JMLR)},
number = {Jan},
pages = {1--40},
title = {{Exploring strategies for training deep neural networks}},
volume = {10},
year = {2009}
}
@inproceedings{Alvarez-Meza2013,
address = {Bruges, Belgium},
author = {{\'{A}}lvarez-Meza, A. M. and Acosta-Medina, C. D. and Castellanos-Dom{\'{i}}nguez, G.},
booktitle = {European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
file = {:home/tom/Work/phd/Papers/{\'{A}}lvarez-Meza, Acosta-Medina, Castellanos-Dom{\'{i}}nguez_2013_Automatic Singular Spectrum Analysis for Time-Series Decomposition.pdf:pdf},
isbn = {9782874190810},
pages = {131--136},
title = {{Automatic Singular Spectrum Analysis for Time-Series Decomposition}},
year = {2013}
}
@article{Arora2013,
archivePrefix = {arXiv},
arxivId = {1308.6273},
author = {Arora, Sanjeev and Ge, R and Moitra, A},
eprint = {1308.6273},
file = {:home/tom/Work/phd/Papers/Arora, Ge, Moitra_2013_New Algorithms for Learning Incoherent and Overcomplete Dictionaries.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
pages = {1--20},
title = {{New Algorithms for Learning Incoherent and Overcomplete Dictionaries}},
url = {http://arxiv.org/abs/1308.6273},
volume = {35},
year = {2013}
}
@inproceedings{Wang2015,
abstract = {Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized computation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic modeling and obtain competitive results.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1506.04448},
author = {Wang, Yining and Tung, Hsiao-Yu and Smola, Alexander and Anandkumar, Animashree},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1506.04448},
keywords = {count sketch,randomized methods,spectral methods,tensor cp decomposition,topic},
pages = {991--999},
title = {{Fast and Guaranteed Tensor Decomposition via Sketching}},
url = {http://arxiv.org/abs/1506.04448},
year = {2015}
}
@inproceedings{Chainais2013,
address = {St. Martin, France},
annote = {Diffusion based algorithm to estimate a dictionary over a network of workers.
There is no proof and little experiements.},
archivePrefix = {arXiv},
arxivId = {arXiv:1304.3568v1},
author = {Chainais, Pierre and Richard, Cedric},
booktitle = {IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)},
doi = {10.1109/CAMSAP.2013.6714025},
eprint = {arXiv:1304.3568v1},
file = {:home/tom/Work/phd/Papers/Chainais, Richard_2013_Learning a common dictionary over a sensor network.pdf:pdf},
isbn = {9781467331463},
keywords = {adaptive networks,block coordinate descent,dictionary learning,diffusion,dis-,matrix factorization,sparse coding,tributed estimation},
pages = {133--136},
title = {{Learning a common dictionary over a sensor network}},
year = {2013}
}
@inproceedings{scholkopf2001generalized,
author = {Sch{\"{o}}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J},
booktitle = {International Conference on Computational Learning Theory (COLT)},
organization = {Springer},
pages = {416--426},
title = {{A generalized representer theorem}},
year = {2001}
}
@article{Lin2015,
abstract = {The alternating direction method of multipliers (ADMM) is widely used in solving structured convex optimization problems. Despite of its success in practice, the convergence properties of the standard ADMM for minimizing the sum of $N$ $(N\geq 3)$ convex functions with $N$ block variables linked by linear constraints, have remained unclear for a very long time. In this paper, we present convergence and convergence rate results for the standard ADMM applied to solve $N$-block $(N\geq 3)$ convex minimization problem, under the condition that one of these functions is convex (not necessarily strongly convex) and the other $N-1$ functions are strongly convex. Specifically, in that case the ADMM is proven to converge with rate $O(1/t)$ in a certain ergodic sense, and $o(1/t)$ in non-ergodic sense, where $t$ denotes the number of iterations.},
archivePrefix = {arXiv},
arxivId = {1408.4265},
author = {Lin, Tian Yi and Ma, Shi Qian and Zhang, Shu Zhong},
doi = {10.1007/s40305-015-0092-0},
eprint = {1408.4265},
file = {:home/tom/Work/phd/Papers/Lin, Ma, Zhang_2015_On the Sublinear Convergence Rate of Multi-block ADMM.pdf:pdf},
issn = {21946698},
journal = {Journal of the Operations Research Society of China},
keywords = {Alternating direction method of multipliers,Convex optimization,Sublinear convergence rate},
number = {3},
pages = {251--274},
title = {{On the Sublinear Convergence Rate of Multi-block ADMM}},
volume = {3},
year = {2015}
}
@article{Vapnik1971,
author = {Vapnik, V. N. and Chervonenkis, A. Ya},
file = {:home/tom/Work/phd/Papers/Vapnik, Chervonenkis_1971_On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities.pdf:pdf},
journal = {Theory of Probability and its Applications},
number = {2},
pages = {264--280},
title = {{On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities}},
volume = {XVI},
year = {1971}
}
@inproceedings{Hyvarinen2016,
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {1605.06336},
author = {Hyvarinen, Aapo and Morioka, Hiroshi},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1605.06336},
file = {:home/tom/Work/phd/Papers/Hyvarinen, Morioka_2016_Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA.pdf:pdf},
issn = {10495258},
pages = {3765--3773},
title = {{Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA}},
url = {http://arxiv.org/abs/1605.06336},
year = {2016}
}
@inproceedings{Qu2014,
abstract = {We consider the problem of recovering the sparsest vector in a subspace {$}\backslashmathcal{\{}S{\}} \backslashsubseteq \backslashmathbb{\{}R{\}}{^}p{$} with {$}\backslashmathrm{\{}dim{\}}(\backslashmathcal{\{}S{\}}) = n {<} p{$}. This problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse PCA, and other problems in signal processing and machine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds {$}1/\backslashsqrt{\{}n{\}}{$}. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is {$}\backslashOmega(1){$}. To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1412.4659},
author = {Qu, Qing and Sun, Ju and Wright, John},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1109/TIT.2016.2601599},
eprint = {1412.4659},
file = {:home/tom/Work/phd/Papers/Qu, Sun, Wright_2014_Finding a sparse vector in a subspace Linear sparsity using alternating directions.pdf:pdf},
isbn = {0001413104},
issn = {10495258},
pages = {3401--3409},
title = {{Finding a sparse vector in a subspace: Linear sparsity using alternating directions}},
url = {http://arxiv.org/abs/1412.4659},
year = {2014}
}
@inproceedings{Bristow2013,
address = {Portland, OR, USA},
author = {Bristow, Hilton and Eriksson, Anders and Lucey, Simon},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2013.57},
file = {:home/tom/Work/phd/Papers/Bristow, Eriksson, Lucey_2013_Fast convolutional sparse coding.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
keywords = {ADMM,convolution,deep learning,fourier,sparse coding},
pages = {391--398},
title = {{Fast convolutional sparse coding}},
year = {2013}
}
@misc{ChenYanpingandKeoghEamonnandHuBingandBegumNurjahanandBagnallAnthonyandMueenAbdullahandBatista,
author = {Chen, Yanping and Keogh, Eamonn and Hu, Bing and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo},
title = {{UCR Time Series Classification/Clustering}},
url = {http://www.cs.ucr.edu/$\sim$eamonn/time_series_data/},
urldate = {2015-09-14},
year = {2015}
}
@misc{Robert2015,
address = {Waltham, MA, USA},
author = {Robert, Matthieu and Contal, Emile and Moreau, Thomas and Vayatis, Nicolas and Vidal, Pierre-Paul},
booktitle = {Gordon Research conference on Eye Movement},
howpublished = {Oral Presentation, Gordon Research Conference on Eye Movement},
title = {{The Why and How of Recording Eye Movement from Very Early Childhood}},
year = {2015}
}
@article{Fuchs2004,
author = {Fuchs, Jean Jacques},
doi = {10.1109/TIT.2004.828141},
file = {:home/tom/Work/phd/Papers/Fuchs_2004_On sparse representations in arbitrary redundant bases.pdf:pdf},
isbn = {0018-9448},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Basis pursuit,Global matched filter,Linear program,Quadratic program,Redundant dictionaries,Sparse representations},
number = {6},
pages = {1341--1344},
title = {{On sparse representations in arbitrary redundant bases}},
volume = {50},
year = {2004}
}
@article{Hahnloser2000,
author = {Hahnloser, R H and Sarpeshkar, R and Mahowald, M a and Douglas, R J and Seung, H S},
doi = {10.1038/35016072},
file = {:home/tom/Work/phd/Papers/Hahnloser et al._2000_Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit.pdf:pdf},
isbn = {0028-0836 (Print)\r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {6789},
pages = {947--951},
pmid = {10879535},
title = {{Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit.}},
volume = {405},
year = {2000}
}
@article{tudor2004many,
author = {Tudor-Locke, Catrine and {Bassett Jr}, David R},
journal = {Sports Medicine},
number = {1},
pages = {1--8},
publisher = {Springer},
title = {{How many steps/day are enough?}},
volume = {34},
year = {2004}
}
@article{pearson2004quantification,
author = {Pearson, O R and Busse, M E and {Van Deursen}, R W M and Wiles, C M},
journal = {QJM},
number = {8},
pages = {463--475},
publisher = {Oxford Univ Press},
title = {{Quantification of walking mobility in neurological disorders}},
volume = {97},
year = {2004}
}
@book{Bertsekas1997,
author = {Bertsekas, Dimitri P. and Tsitsiklis, John},
file = {:home/tom/Work/phd/Papers/Bertsekas, Tsitsiklis_1997_Parallel and Distributed Computation Numerical Methods (Optimization and Neural Computation).pdf:pdf},
pages = {1----738},
title = {{Parallel and Distributed Computation: Numerical Methods (Optimization and Neural Computation)}},
year = {1997}
}
@article{Daubechies2004,
archivePrefix = {arXiv},
arxivId = {math/0307152},
author = {Daubechies, Ingrid and Defrise, Michel and {De Mol}, Christine},
doi = {10.1002/cpa.20042},
eprint = {0307152},
file = {:home/tom/Work/phd/Papers/Daubechies, Defrise, De Mol_2004_An iterative thresholding algorithm for linear inverse problems with a sparsity constraint.pdf:pdf},
isbn = {0010-3640},
issn = {00103640},
journal = {Communications on Pure and Applied Mathematics},
number = {11},
pages = {1413--1457},
primaryClass = {math},
title = {{An iterative thresholding algorithm for linear inverse problems with a sparsity constraint}},
volume = {57},
year = {2004}
}
@article{Barrois2015,
author = {Barrois, Remi and Oudre, Laurent and Moreau, Thomas and Truong, Charles and Vayatis, Nicolas and Buffat, St\'ephane and Yelnik, Alain and de Waele, Catherine and Gregory, Thomas and Laporte, Serge and Others},
journal = {Computer methods in biomechanics and biomedical engineering},
number = {Sup1},
pages = {1880--1881},
publisher = {Taylor \& Francis},
title = {{Quantify osteoarthritis gait at the doctor's office: a simple pelvis accelerometer based method independent from footwear and aging}},
volume = {18},
year = {2015}
}
@article{Achille2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01353v3},
author = {Achille, Alessandro and Soatto, Stefano},
eprint = {arXiv:1611.01353v3},
file = {:home/tom/Work/phd/Papers/Achille, Soatto_2016_Information Dropout Learning Optimal Representations Through Noisy Computation.pdf:pdf},
journal = {arXiv preprint},
number = {01353},
title = {{Information Dropout: Learning Optimal Representations Through Noisy Computation}},
volume = {arXiv:1611},
year = {2016}
}
@inproceedings{Wang2012,
address = {Portland, USA},
author = {Wang, Dong and Technologies, Language and Tejedor, Javier and Computer, Human},
booktitle = {Annual Conference of the International Speech Communication Association},
file = {:home/tom/Work/phd/Papers/Wang et al._2012_Heterogeneous Convolutive Non-Negative Sparse Coding.pdf:pdf},
pages = {2150--2153},
title = {{Heterogeneous Convolutive Non-Negative Sparse Coding}},
year = {2012}
}
@article{Gordon1988,
author = {Gordon, Y.},
doi = {10.1007/BFb0081732},
file = {:home/tom/Work/phd/Papers/Gordon_1988_On Milman's inequality and random subspaces which escape through a mesh in Rn.pdf:pdf},
isbn = {978-3-540-19353-1},
journal = {Geometric Aspects of Functional Analysis},
keywords = {Mathematics and Statistics},
pages = {84--106},
title = {{On Milman's inequality and random subspaces which escape through a mesh in Rn}},
url = {http://www.springerlink.com/content/8248580l241p33vt/},
volume = {1317},
year = {1988}
}
@article{Oculo5,
author = {Donin, JF},
journal = {Canadian Journal of Ophthalmology},
number = {2},
pages = {212--215},
title = {{Acquired monocular nystagmus in children}},
volume = {Jul},
year = {1967}
}
@article{Montavon2018,
author = {Montavon, Gr{\'{e}}goire and Samek, Wojciech and M{\"{u}}ller, Klaus-robert},
doi = {10.1016/j.dsp.2017.10.011},
file = {:home/tom/Work/phd/Papers/Montavon, Samek, M{\"{u}}ller_2018_Methods for interpreting and understanding deep neural networks.pdf:pdf},
issn = {1051-2004},
journal = {Digital Signal Processing},
keywords = {activation maximization,deep neural networks,sensitivity analysis},
pages = {1--15},
publisher = {Elsevier Inc.},
title = {{Methods for interpreting and understanding deep neural networks}},
url = {https://doi.org/10.1016/j.dsp.2017.10.011},
volume = {73},
year = {2018}
}
@article{Colagrossi2012,
author = {Colagrossi, Andrea and Bouscasse, B. and Antuono, M. and Marrone, S.},
doi = {10.1016/j.cpc.2012.02.032},
file = {:home/tom/Work/phd/Papers/Colagrossi et al._2012_Particle packing algorithm for SPH schemes.pdf:pdf},
isbn = {0010-4655},
issn = {00104655},
journal = {Computer Physics Communications},
keywords = {Lagrangian systems,Meshless methods,Particle initialization,Smoothed particle hydrodynamics},
number = {8},
pages = {1641--1653},
publisher = {Elsevier B.V.},
title = {{Particle packing algorithm for SPH schemes}},
url = {http://dx.doi.org/10.1016/j.cpc.2012.02.032},
volume = {183},
year = {2012}
}
@article{Garcia-Cardona2017,
archivePrefix = {arXiv},
arxivId = {1709.02893},
author = {Garcia-Cardona, Cristina and Wohlberg, Brendt},
eprint = {1709.02893},
file = {:home/tom/Work/phd/Papers/Garcia-Cardona, Wohlberg_2017_Convolutional Dictionary Learning.pdf:pdf},
journal = {arXiv preprint},
number = {00106},
title = {{Convolutional Dictionary Learning}},
url = {http://arxiv.org/abs/1709.02893},
volume = {arXiv:1709},
year = {2017}
}
@article{Johnson1984,
author = {Johnson, William B. and Lindenstrauss, Joram},
file = {:home/tom/Work/phd/Papers/Johnson, Lindenstrauss_1984_Extensions of Lipschitz Mappings into a Hilbert Space.pdf:pdf},
journal = {Contemporary Mathematics},
number = {1},
pages = {189--206},
title = {{Extensions of Lipschitz Mappings into a Hilbert Space}},
volume = {26},
year = {1984}
}
@article{Sakoe1978,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sakoe, Hiroaki and Chiba, Seibi},
doi = {10.1109/TASSP.1978.1163055},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Work/phd/Papers/Sakoe, Chiba_1978_Dynamic Programming Algorithm Optimization for Spoken Word Recognition.pdf:pdf},
isbn = {9788578110796},
issn = {00963518},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
number = {1},
pages = {43--49},
pmid = {25246403},
title = {{Dynamic Programming Algorithm Optimization for Spoken Word Recognition}},
volume = {26},
year = {1978}
}
@inproceedings{Sakoe1971,
address = {Budapest, Hungary},
author = {Sakoe, Hiroaki and Chiba, Seibi},
booktitle = {International Congress on Acoustics},
keywords = {2000 book nlp},
pages = {65--69},
publisher = {{Akad{\'{e}}miai} {Kiad{\'{o}}}},
title = {{A Dynamic Programming Approach to Continuous Speech Recognition}},
volume = {3},
year = {1971}
}
@inproceedings{Scherrer2012,
abstract = {Large-scale `1-regularized loss minimization problems arise in high-dimensional applications such as compressed sensing and high-dimensional supervised learn- ing, including classification and regression problems. High-performance algo- rithms and implementations are critical to efficiently solving these problems. Building upon previous work on coordinate descent algorithms for `1-regularized problems, we introduce a novel family of algorithms called block-greedy coor- dinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-Greedy. We give a unified convergence analysis for the family of block-greedy algorithms. The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clus- tered so that the maximum inner product between features in different blocks is small. Our theoretical convergence analysis is supported with experimental re- sults using data from diverse real-world applications. We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale `1-regularization problems.},
address = {South Lake Tahoe, United States},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.4174v1},
author = {Scherrer, Chad and Tewari, Ambuj and Halappanavar, Mahantesh and Haglin, David J.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {arXiv:1212.4174v1},
file = {:home/tom/Work/phd/Papers/Scherrer et al._2012_Feature Clustering for Accelerating Parallel Coordinate Descent.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
pages = {28--36},
title = {{Feature Clustering for Accelerating Parallel Coordinate Descent}},
year = {2012}
}
@book{Golyandina2002,
author = {Golyandina, Nina and Nekrutkin, Vladimir and Zhigljavsky, Anatoly A},
file = {:home/tom/Work/phd/Papers/Golyandina, Nekrutkin, Zhigljavsky_2001_Analysis of Time Series Structure SSA and Related Techniques.pdf:pdf},
publisher = {CRC Press},
title = {{Analysis of Time Series Structure: SSA and Related Techniques}},
year = {2001}
}
@article{Boley2013,
author = {Boley, Daniel},
file = {:home/tom/Work/phd/Papers/Boley_2013_On the linear convergence of the alternating direction method of multipliers.pdf:pdf},
journal = {SIAM Journal on Optimization},
keywords = {1,10,1137,120878951,65k05,90c05,90c20,admm,ams subject classifications,doi,introduction,linear programming,problems arise in many,quadratic programming,very large-scale convex optimization},
number = {4},
pages = {2183--2207},
title = {{On the linear convergence of the alternating direction method of multipliers}},
volume = {23},
year = {2013}
}
@inproceedings{Gori1991,
address = {Seattle, WA, USA},
author = {Gori, M and Tesi, A},
booktitle = {International Joint Conference on Neural Networks (IJCNN)},
organization = {IEEE},
pages = {896},
title = {{Backpropagation converges for multi-layered networks and linearly-separable patterns}},
volume = {2},
year = {1991}
}
@article{Mikolajczyk2005,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Mikolajczyk, Krystian and Schmid, C. and A, Cordelia Schmid.},
doi = {10.1109/TPAMI.2005.188},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Work/phd/Papers/Mikolajczyk, Schmid, A_2005_A performance evaluation of local descriptors.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {10},
pages = {1615--1630},
pmid = {16237996},
title = {{A performance evaluation of local descriptors}},
url = {http://www-dsp.elet.polimi.it/VA-TLC/Articoli/mikolajczyk_pami2004-A performance evaluation of local descriptors.pdf%5Cnhttp://www.google.com/url?sa=t&source=web&cd=1&ved=0CBcQFjAA&url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5507&rep=},
volume = {27},
year = {2005}
}
@article{Baldi1989,
author = {Baldi, Pierre and Hornik, Kurt},
doi = {10.1016/0893-6080(89)90014-2},
file = {:home/tom/Work/phd/Papers/Baldi, Hornik_1989_Neural networks and principal component analysis Learning from examples without local minima.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back propagation,Learning,Neural networks,Principal component analysis},
number = {1},
pages = {53--58},
title = {{Neural networks and principal component analysis: Learning from examples without local minima}},
volume = {2},
year = {1989}
}
@article{Moreau2016,
abstract = {One of the main challenges of deep learning methods is the choice of an appropriate training strategy. In particular, additional steps, such as unsupervised pre-training, have been shown to greatly improve the performances of deep structures. In this paper, we introduce a new training step,the post-training, which takes place after the training and where only specific layers are trained. In particular, we focus on the particular case -- named Last Kernel -- where only the last layer is trained. This step aims to find the optimal use of data representation learned during the other phases of the training. We show that Last Kernel can be effortlessly added to most learning strategies, is computationally inexpensive, does not cause overfitting and often produces significant improvement. Additionally, we show that with commonly used losses and activation functions, Last Kernel solves a convex closed optimization problems, offering rapid convergence -- or even closed-form solutions.},
archivePrefix = {arXiv},
arxivId = {1611.04499},
author = {Moreau, Thomas and Audiffren, Julien},
eprint = {1611.04499},
file = {:home/tom/Work/phd/Papers/Moreau, Audiffren_2016_Post Training in Deep Learning with Last Kernel.pdf:pdf},
journal = {arXiv preprint},
number = {04499},
title = {{Post Training in Deep Learning with Last Kernel}},
url = {http://arxiv.org/abs/1611.04499},
volume = {arXiv:1611},
year = {2016}
}
@article{Galiana1991,
author = {Galiana, Henrietta L.},
doi = {10.1109/10.81578},
file = {:home/tom/Work/phd/Papers/Galiana_1991_A nystagmus strategy to linearize the vestibulo-ocular reflex.pdf:pdf},
isbn = {0018929419},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
number = {6},
pages = {532--543},
pmid = {1879842},
title = {{A nystagmus strategy to linearize the vestibulo-ocular reflex}},
volume = {38},
year = {1991}
}
@article{Necoara2013,
abstract = {In this paper we propose a parallel coordinate descent algorithm for solving smooth convex optimization problems with separable constraints that may arise, e.g. in distributed model predictive control (MPC) for linear network systems. Our algorithm is based on block coordinate descent updates in parallel and has a very simple iteration. We prove (sub)linear rate of convergence for the new algorithm under standard assumptions for smooth convex optimization. Further, our algorithm uses local information and thus is suitable for distributed implementations. Moreover, it has low iteration complexity, which makes it appropriate for embedded control. An MPC scheme based on this new parallel algorithm is derived, for which every subsystem in the network can compute feasible and stabilizing control inputs using distributed and cheap computations. For ensuring stability of the MPC scheme, we use a terminal cost formulation derived from a distributed synthesis. Preliminary numerical tests show better performance for our optimization algorithm than other existing methods. ?? 2013 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1302.3092v1},
author = {Necoara, Ion and Clipici, Dragos},
doi = {10.1016/j.jprocont.2012.12.012},
eprint = {arXiv:1302.3092v1},
file = {:home/tom/Work/phd/Papers/Necoara, Clipici_2013_Efficient parallel coordinate descent algorithm for convex optimization problems with separable constraints Applic.pdf:pdf},
issn = {09591524},
journal = {Journal of Process Control},
keywords = {(Sub)linear convergence rate,Coordinate descent optimization,Distributed model predictive control,Embedded control,Parallel algorithm},
number = {3},
pages = {243--253},
publisher = {Elsevier Ltd},
title = {{Efficient parallel coordinate descent algorithm for convex optimization problems with separable constraints: Application to distributed MPC}},
url = {http://dx.doi.org/10.1016/j.jprocont.2012.12.012},
volume = {23},
year = {2013}
}
@article{salarian2004gait,
author = {Salarian, A and Russmann, H and Vingerhoets, F and Dehollain, C and Blanc, Y and Burkhard, P and Aminian, K},
journal = {IEEE Transactions on Biomedical Engineering},
number = {8},
pages = {1434--1443},
publisher = {IEEE},
title = {{Gait assessment in Parkinson's disease: toward an ambulatory system for long-term monitoring}},
volume = {51},
year = {2004}
}
@article{Gorodnitsky1997,
author = {Gorodnitsky, Irina F. and Rao, Bhaskar D.},
doi = {10.1109/78.558475},
file = {:home/tom/Work/phd/Papers/Gorodnitsky, Rao_1997_Sparse signal reconstruction from limited data using FOCUSS A re-weighted minimum norm algorithm.pdf:pdf},
isbn = {1053-587X},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
number = {3},
pages = {600--616},
pmid = {1000198971},
title = {{Sparse signal reconstruction from limited data using FOCUSS: A re-weighted minimum norm algorithm}},
volume = {45},
year = {1997}
}
@article{Gabay1976,
archivePrefix = {arXiv},
arxivId = {1104.0262},
author = {Gabay, Daniel and Mercier, Bertrand},
doi = {10.1016/0898-1221(76)90003-1},
eprint = {1104.0262},
file = {:home/tom/Work/phd/Papers/Gabay, Mercier_1976_A dual algorithm for the solution of nonlinear variational problems via finite element approximation.pdf:pdf},
isbn = {0278-0062 VO - 30},
issn = {08981221},
journal = {Computers \& Mathematics with Applications},
number = {1},
pages = {17--40},
pmid = {270766200012},
title = {{A dual algorithm for the solution of nonlinear variational problems via finite element approximation}},
url = {http://www.sciencedirect.com/science/article/pii/0898122176900031},
volume = {2},
year = {1976}
}
@inproceedings{marschollek2008performance,
address = {Vancouver, Canada},
author = {Marschollek, M and Goevercin, M and Wolf, K.-H. and Song, B and Gietzelt, M and Haux, R and Steinhagen-Thiessen, E},
booktitle = {Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBS)},
pages = {1319--1322},
title = {{A performance comparison of accelerometry-based step detection algorithms on a large, non-laboratory sample of healthy and mobility-impaired persons}},
year = {2008}
}
@inproceedings{poultney2006efficient,
address = {Vancouver, Canada},
author = {Poultney, Christopher and Chopra, Sumit and Cun, Yann L and Others},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1137--1144},
title = {{Efficient learning of sparse representations with an energy-based model}},
year = {2006}
}
@article{McCulloch1943,
abstract = {Because of the 'all-or-none' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behaviour of every net can be described in these terms, with the addition of more complicated logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which nehaves under the other and gives the same results, algthough perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, W S and Pitts, W},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Work/phd/Papers/McCulloch, Pitts_1943_A Logical Calculus of the Idea Immanent in Nervous Activity.pdf:pdf},
isbn = {0007-4985},
issn = {0007-4985},
journal = {The Bulletin of Mathematical Biophysics},
keywords = {McCulloch and Pitts,neuron},
number = {4},
pages = {115--133},
pmid = {2185863},
title = {{A Logical Calculus of the Idea Immanent in Nervous Activity}},
url = {http://www.cse.chalmers.se/$\sim$coquand/AUTOMATA/mcp.pdf},
volume = {5},
year = {1943}
}
@article{Gribonval2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.3790v3},
author = {Gribonval, R{\'{e}}mi and Jenatton, Rodolphe and Bach, Francis and Kleinsteuber, Martin and Seibert, Matthias},
doi = {10.1109/TIT.2015.2424238},
eprint = {arXiv:1312.3790v3},
file = {:home/tom/Work/phd/Papers/Gribonval et al._2015_Sample complexity of dictionary learning and other matrix factorizations.pdf:pdf},
isbn = {2011277906},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Dictionary learning,K-means clustering,non-negative matrix factorization,principal component analysis,sample complexity,sparse coding,structured learning},
number = {6},
pages = {3469--3486},
title = {{Sample complexity of dictionary learning and other matrix factorizations}},
volume = {61},
year = {2015}
}
@misc{Mairal2010a,
author = {Mairal, Julien},
file = {:home/tom/Work/phd/Papers/Mairal_2010_Sparse Coding and Dictionary Learning for Image Analysis.pdf:pdf},
keywords = {()},
number = {September},
title = {{Sparse Coding and Dictionary Learning for Image Analysis}},
year = {2010}
}
@article{Chan2010,
author = {Chan, W. W P and Galiana, Henrietta L.},
doi = {10.1109/TBME.2009.2016112},
file = {:home/tom/Work/phd/Papers/Chan, Galiana_2010_A nonlinear model of the neural integrator improves detection of deficits in the human VOR.pdf:pdf},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Lesion detection,Modeling,Neural integrator (NI),Vestibulo-ocular reflex (VOR)},
number = {5},
pages = {1012--1023},
pmid = {19272974},
title = {{A nonlinear model of the neural integrator improves detection of deficits in the human VOR}},
volume = {57},
year = {2010}
}
@inproceedings{Zeiler2010,
address = {San Francisco, CA, USA},
author = {Zeiler, Matthew D. and Krishnan, Dilip and Taylor, Graham W. and Fergus, Rob},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Work/phd/Papers/Zeiler et al._2010_Deconvolutional Networks.pdf:pdf},
isbn = {9781424469857},
pages = {2528----2535},
publisher = {IEEE},
title = {{Deconvolutional Networks}},
year = {2010}
}
@article{Oculo8,
author = {Galvez-Ruiz, A and Roig, C and Mu{\~{n}}oz, S and Arruga, J},
journal = {Neuro-Opthalmology},
pages = {276--279},
title = {{Convergent-divergent nystagmus as a manifestation of oculopalatal tremor.}},
volume = {35},
year = {2011}
}
@inproceedings{cho2009kernel,
address = {Vancouver, Canada},
author = {Cho, Youngmin and Saul, Lawrence K},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/Cho, Saul_2009_Kernel Methods for Deep Learning.pdf:pdf},
pages = {342--350},
title = {{Kernel Methods for Deep Learning}},
year = {2009}
}
@inproceedings{Zhu2010,
abstract = {ABSTRACT A novel audio fingerprinting method that is highly robust to Time Scale Modification (TSM) and pitch shifting is pro- posed. Instead of simply employing spectral or tempo-related features, our system is based on computer-vision techniques. We transform each 1-D ...},
address = {Singapore, Singapore},
author = {Zhu, Bilei and Li, Wei and Wang, Zhurong and Xue, Xiangyang},
booktitle = {International Conference on Multimedia (MM)},
doi = {10.1145/1873951.1874130},
file = {:home/tom/Work/phd/Papers/Zhu et al._2010_A novel audio fingerprinting method robust to time scale modification and pitch shifting.pdf:pdf},
isbn = {9781605589336},
keywords = {audio fingerprinting,pitch shifting,robustness,time scale modification},
pages = {987},
title = {{A novel audio fingerprinting method robust to time scale modification and pitch shifting}},
url = {http://dl.acm.org/citation.cfm?doid=1873951.1874130},
year = {2010}
}
@article{Rockafellar1976,
abstract = {For the problem of minimizing a lower semicontinuous proper convex function f on a Hilbert space, the proximal point algorithm in exact form generates a sequence $\{ z^k \} $ by taking $z^{k + 1} $ to be the minimizes of $f(z) + ({1 / {2c_k }})\| {z - z^k } \|^2 $, where $c_k > 0$. This algorithm is of interest for several reasons, but especially because of its role in certain computational methods based on duality, such as the Hestenes-Powell method of multipliers in nonlinear programming. It is investigated here in a more general form where the requirement for exact minimization at each iteration is weakened, and the subdifferential $\partial f$ is replaced by an arbitrary maximal monotone operator T. Convergence is established under several criteria amenable to implementation. The rate of convergence is shown to be “typically” linear with an arbitrarily good modulus if $c_k $ stays large enough, in fact superlinear if $c_k \to \infty $. The case of $T = \partial f$ is treated in extra detail. Applicati...},
author = {Rockafellar, R. Tyrrell},
doi = {10.1137/0314056},
file = {:home/tom/Work/phd/Papers/Rockafellar_1976_Monotone Operators and the Proximal Point Algorithm.pdf:pdf},
isbn = {0233193960},
issn = {0363-0129},
journal = {SIAM Journal on Control and Optimization},
number = {5},
pages = {877--898},
title = {{Monotone Operators and the Proximal Point Algorithm}},
volume = {14},
year = {1976}
}
@inproceedings{Oculo14,
author = {Hoffmann, LC and Berry, SD},
booktitle = {National Academy of Sciences of the United States of America},
pages = {21371--21376},
title = {{Cerebellar theta oscillations are synchronized during hippocampal theta-contingent trace conditioning}},
year = {2009}
}
@article{Agarwal2013,
archivePrefix = {arXiv},
arxivId = {1309.1952},
author = {Agarwal, Alekh and Anandkumar, Animashree and Netrapalli, Praneeth},
eprint = {1309.1952},
file = {:home/tom/Work/phd/Papers/Agarwal, Anandkumar, Netrapalli_2013_A Clustering Approach to Learn Sparsely-Used Overcomplete Dictionaries.pdf:pdf},
journal = {arXiv preprint},
keywords = {dictionary learning,incoherence,lasso,overcomplete dictionaries,sparse coding},
number = {1952},
title = {{A Clustering Approach to Learn Sparsely-Used Overcomplete Dictionaries}},
url = {http://arxiv.org/abs/1309.1952},
volume = {arXiv:1309},
year = {2013}
}
@article{Oculo25,
author = {Odom, JV and Bach, M and Brigell, M and Holder, GE and McCulloch, DL and Tormene, AP and Et, Al.},
journal = {Documenta Ophthalmologica},
number = {120},
pages = {111--9},
title = {{ISCEV standard for clinical visual evoked potentials}},
volume = {Feb},
year = {2010}
}
@inproceedings{Sokolic2017,
abstract = {This paper studies the generalization error of invariant classifiers. In particular, we consider the common scenario where the classification task is invariant to certain transformations of the input, and that the classifier is constructed (or learned) to be invariant to these transformations. Our approach relies on factoring the input space into a product of a base space and a set of transformations. We show that whereas the generalization error of a non-invariant classifier is proportional to the complexity of the input space, the generalization error of an invariant classifier is proportional to the complexity of the base space. We also derive a set of sufficient conditions on the geometry of the base space and the set of transformations that ensure that the complexity of the base space is much smaller than the complexity of the input space. Our analysis applies to general classifiers such as convolutional neural networks. We demonstrate the implications of the developed theory for such classifiers with experiments on the MNIST and CIFAR-10 datasets.},
archivePrefix = {arXiv},
arxivId = {1610.04574},
author = {Sokolic, Jure and Giryes, Raja and Sapiro, Guillermo and Rodrigues, Miguel R. D.},
booktitle = {Artificial Intelligence and Statistics (AISTAT)},
eprint = {1610.04574},
file = {:home/tom/Work/phd/Papers/Sokolic et al._2017_Generalization Error of Invariant Classifiers.pdf:pdf},
pages = {1094--1103},
title = {{Generalization Error of Invariant Classifiers}},
url = {http://arxiv.org/abs/1610.04574},
year = {2017}
}
@article{Moreau2017a,
author = {Moreau, Thomas and Oudre, Laurent and Vayatis, Nicolas},
file = {:home/tom/Work/phd/Papers/Moreau, Oudre, Vayatis_2017_Distributed Convolutional Sparse Coding.pdf:pdf},
journal = {arXiv preprint},
number = {10087},
title = {{Distributed Convolutional Sparse Coding}},
volume = {arXiv:1705},
year = {2017}
}
@article{hinton2006reducing,
author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
journal = {Science},
number = {5786},
pages = {504--507},
publisher = {American Association for the Advancement of Science},
title = {{Reducing the dimensionality of data with neural networks}},
volume = {313},
year = {2006}
}
@inproceedings{Wohlberg2014,
address = {Florence, Italy},
author = {Wohlberg, Brendt},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2014.6854992},
file = {:home/tom/Work/phd/Papers/Wohlberg_2014_Efficient convolutional sparse coding.pdf:pdf},
isbn = {9781479928927},
issn = {15206149},
keywords = {ADMM,Convolutional Sparse Coding,Sparse Coding,Sparse Representation},
pages = {7173--7177},
title = {{Efficient convolutional sparse coding}},
year = {2014}
}
@inproceedings{Yu2012,
address = {Brussels, Belgium},
author = {Yu, Hsiang Fu and Hsieh, Cho Jui and Si, Si and Dhillon, Inderjit},
booktitle = {IEEE International Conference on Data Mining (ICDM)},
doi = {10.1109/ICDM.2012.168},
file = {:home/tom/Work/phd/Papers/Yu et al._2012_Scalable coordinate descent approaches to parallel matrix factorization for recommender systems.pdf:pdf},
isbn = {9780769549057},
issn = {15504786},
keywords = {Low rank approximation,Matrix factorization,Parallelization,Recommender systems},
pages = {765--774},
title = {{Scalable coordinate descent approaches to parallel matrix factorization for recommender systems}},
year = {2012}
}
@inproceedings{Bo2013,
address = {Portland, OR, USA},
author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2013.91},
file = {:home/tom/Work/phd/Papers/Bo, Ren, Fox_2013_Multipath Sparse Coding Using Hierarchical Matching Pursuit.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {1063-6919},
pages = {660--667},
title = {{Multipath Sparse Coding Using Hierarchical Matching Pursuit}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6618935},
year = {2013}
}
@article{Ferradans2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1307.5551v1},
author = {Ferradans, Sira and Papadakis, Nicolas and Rabin, Julien and Peyr{\'{e}}, Gabriel and Aujol, Jean Fran{\c{c}}ois},
doi = {10.1007/978-3-642-38267-3_36},
eprint = {arXiv:1307.5551v1},
file = {:home/tom/Work/phd/Papers/Ferradans et al._2011_Regularized discrete optimal transport.pdf:pdf},
isbn = {9783642382666},
issn = {03029743},
journal = {Informatica},
keywords = {Deconvolution,Optimal Transport,color transfer,convex optimization,manifold learning,proximal splitting,transport,variational regularization},
mendeley-tags = {Deconvolution,transport},
number = {1},
pages = {428--439},
title = {{Regularized discrete optimal transport}},
volume = {35},
year = {2011}
}
@inproceedings{brajdic2013walk,
address = {Zurich, Switzerland},
author = {Brajdic, A and Harle, R},
booktitle = {ACM international joint conference on Pervasive and ubiquitous computing},
organization = {ACM},
pages = {225--234},
title = {{Walk detection and step counting on unconstrained smartphones}},
year = {2013}
}
@book{Hertle2013,
author = {Hertle, Richard W and Dell'Osso, Louis F},
publisher = {Oxford University Press},
title = {{Nystagmus in infancy and childhood: current concepts in mechanisms, diagnoses, and management}},
year = {2013}
}
@article{Oculo26,
author = {Orekhova, EV and Stroganova, TA and Posikera, IN and Elam, M},
journal = {Clinical Neurophysiology},
number = {17},
pages = {1047--62},
title = {{EEG theta rhythm in infants and preschool children}},
volume = {MAy},
year = {2006}
}
@inproceedings{Strobl2013,
author = {Strobl, Eric V. and Visweswaran, Shyam},
booktitle = {International Conference on Machine Learning and Applications (ICMLA)},
doi = {10.1109/ICMLA.2013.84},
file = {:home/tom/Work/phd/Papers/Strobl, Visweswaran_2013_Deep Multiple Kernel Learning.pdf:pdf},
isbn = {978-0-7695-5144-9},
keywords = {deep learning,kernels,multiple,support vector machine},
pages = {414--417},
title = {{Deep Multiple Kernel Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6784654},
year = {2013}
}
@inproceedings{Cuturi2017,
address = {Sidney, Australia},
archivePrefix = {arXiv},
arxivId = {1703.01541},
author = {Cuturi, Marco and Blondel, Mathieu},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1703.01541},
file = {:home/tom/Work/phd/Papers/Cuturi, Blondel_2017_Soft-DTW a Differentiable Loss Function for Time-Series.pdf:pdf},
pages = {894--903},
title = {{Soft-DTW: a Differentiable Loss Function for Time-Series}},
url = {http://arxiv.org/abs/1703.01541},
year = {2017}
}
@inproceedings{Mackey2008,
abstract = {In analogy to the PCA setting, the sparse PCA problem is often solved by iter- atively alternating between two subtasks: cardinality-constrained rank-one vari- ance maximization and matrix deflation. While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justification from the PCA context. In this work, we demon- strate that the standard PCA deflation procedure is seldom appropriate for the sparse PCA setting. To rectify the situation, we first develop several deflation al- ternatives better suited to the cardinality-constrained context. We then reformulate the sparse PCAoptimization problemto explicitly reflect themaximumadditional variance objective on each round. The result is a generalized deflation procedure that typically outperforms more standard techniques on real-world datasets.},
address = {Vancouver, Canada},
annote = {* Regular deflation does not preserve orthogonality
* Propose new variants that make sure the orthogonality is preserved (schur complement and othonormal projections)},
author = {Mackey, Lester},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/Mackey_2008_Deflation Methods for Sparse PCA.pdf:pdf},
isbn = {9781605609492},
pages = {1017--1024},
title = {{Deflation Methods for Sparse PCA.}},
url = {https://papers.nips.cc/paper/3575-deflation-methods-for-sparse-pca.pdf},
year = {2008}
}
@article{Chaudhari2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.06485v5},
author = {Chaudhari, Pratik and Soatto, Stefano},
eprint = {arXiv:1511.06485v5},
file = {:home/tom/Work/phd/Papers/Chaudhari, Soatto_2015_On the energy landscape of deep networks.pdf:pdf},
journal = {arXiv preprint},
number = {06485},
title = {{On the energy landscape of deep networks}},
volume = {arXiv:1511},
year = {2015}
}
@article{kwapisz2011activity,
author = {Kwapisz, J and Weiss, G and Moore, S},
journal = {ACM SigKDD Explorations Newsletter},
number = {2},
pages = {74--82},
publisher = {ACM},
title = {{Activity recognition using cell phone accelerometers}},
volume = {12},
year = {2011}
}
@inproceedings{Choromanska2015,
address = {San Diego, CA, USA},
archivePrefix = {arXiv},
arxivId = {1412.0233},
author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'{e}}rard Ben and LeCun, Yann},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
eprint = {1412.0233},
file = {:home/tom/Work/phd/Papers/Choromanska et al._2015_The Loss Surfaces of Multilayer Networks.pdf:pdf},
isbn = {1412.0233},
issn = {15337928},
keywords = {Neural Network,Optimization},
pages = {192----204},
title = {{The Loss Surfaces of Multilayer Networks}},
url = {http://arxiv.org/abs/1412.0233%5Cnhttp://www.arxiv.org/pdf/1412.0233.pdf},
year = {2015}
}
@inproceedings{Lu2013,
address = {Portland, OR, USA},
author = {Lu, Cewu and Shi, Jiaping and Jia, Jiaya},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2013.60},
isbn = {978-0-7695-4989-7},
issn = {10636919},
keywords = {Dictionary Learning,Online Learning,Robust Statistics},
pages = {415--422},
title = {{Online robust dictionary learning}},
year = {2013}
}
@article{Cybenko1989,
author = {Cybenko, G.},
doi = {10.1007/BF02836480},
file = {:home/tom/Work/phd/Papers/Cybenko_1989_Approximation by Superpositions of a Sigmoidal Function.pdf:pdf},
isbn = {0780300564},
issn = {10009221},
journal = {Mathematics of Control, Signals, and Systems},
keywords = {approximation,completeness,neural networks},
number = {4},
pages = {303--314},
title = {{Approximation by Superpositions of a Sigmoidal Function}},
volume = {2},
year = {1989}
}
@misc{Moreau2017b,
address = {Paris, France},
author = {Moreau, Thomas and Grisel, Olivier},
booktitle = {PyParis},
howpublished = {Oral Presentation, PyParis},
title = {{Robustifying {\tt concurrent.futures}}},
year = {2017}
}
@inproceedings{Peyre2011,
abstract = {This paper introduces a novel approach to learn a dictionary in a sparsity-promoting analysis-type prior. The dictionary is opti- mized in order to optimally restore a set of exemplars from their degraded noisy versions. Towards this goal, we cast our prob- lem as a bilevel programming problem for which we propose a gradient descent algorithm to reach a stationary point that might be a local minimizer. When the dictionary analysis operator specializes to a convolution, our method turns out to be a way of learning generalized total variation-type prior. Applications to 1-D signal denoising are reported and potential applicability and extensions are discusses.},
annote = {Give an algorithm to solve the DL problem with a prior analysis approach.
Use a sparsity regularisation based on the Hubert norm (see note)},
author = {Peyr{\'{e}}, Gabriel and Fadili, Jalal},
booktitle = {International Conference on Sampling Theory and Applications (SampTA)},
file = {:home/tom/Work/phd/Papers/Peyr{\'{e}}, Fadili_2011_Learning analysis sparsity priors.pdf:pdf},
keywords = {analysis prior,dictionary learning,total varia-},
number = {1},
pages = {2--5},
title = {{Learning analysis sparsity priors}},
url = {http://hal.archives-ouvertes.fr/hal-00542016/},
volume = {2},
year = {2011}
}
@article{DAspremont2007,
archivePrefix = {arXiv},
arxivId = {cs.CE/0406021},
author = {D'Aspremont, Alexandre and {El Ghaoui}, Laurent and Jordan, Michael I. and Lanckriet, Gert R. G.},
doi = {10.1137/050645506},
eprint = {0406021},
file = {:home/tom/Work/phd/Papers/d'Aspremont et al._2007_A Direct Formulation for Sparse PCA Using Semidefinite Programming.pdf:pdf},
isbn = {0036-1445},
issn = {0036-1445},
journal = {SIAM Review},
keywords = {Karhunen–Lo{\`{e}}ve transform,Moreau–Yosida regularization,factor analysis,principal component analysis,semidefinite programming,semidefinite relaxation},
number = {3},
pages = {434--448},
primaryClass = {cs.CE},
title = {{A Direct Formulation for Sparse PCA Using Semidefinite Programming}},
url = {http://epubs.siam.org/doi/abs/10.1137/050645506},
volume = {49},
year = {2007}
}
@inproceedings{Gramfort2017,
address = {Long Beach, CA, USA},
archivePrefix = {arXiv},
arxivId = {1705.08006},
author = {Jas, Mainak and {Dupr{\'{e}} La Tour}, Tom and Şimşekli, Umut and Gramfort, Alexandre},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1705.08006},
file = {:home/tom/Work/phd/Papers/Jas et al._2017_Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding.pdf:pdf},
pages = {1--15},
title = {{Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding}},
url = {http://arxiv.org/abs/1705.08006},
year = {2017}
}
@inproceedings{JiaDeng2009,
address = {Miami Beach, FL, USA},
author = {{Jia Deng} and {Wei Dong} and Socher, R. and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPRW.2009.5206848},
file = {:home/tom/Work/phd/Papers/Jia Deng et al._2009_ImageNet A large-scale hierarchical image database.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
pages = {248--255},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206848},
year = {2009}
}
@article{Blumensath2009,
archivePrefix = {arXiv},
arxivId = {0805.0510},
author = {Blumensath, Thomas and Davies, Mike E.},
doi = {10.1016/j.acha.2009.04.002},
eprint = {0805.0510},
file = {:home/tom/Work/phd/Papers/Blumensath, Davies_2009_Iterative hard thresholding for compressed sensing.pdf:pdf},
isbn = {1063-5203},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
keywords = {Algorithms,Compressed sensing,Iterative hard thresholding,Signal recovery,Sparse inverse problem},
number = {3},
pages = {265--274},
publisher = {Elsevier Inc.},
title = {{Iterative hard thresholding for compressed sensing}},
url = {http://dx.doi.org/10.1016/j.acha.2009.04.002},
volume = {27},
year = {2009}
}
@article{Trench2003,
author = {Trench, William F},
file = {:home/tom/Work/phd/Papers/Trench_2003_Absolute equal distribution of families of finite sets.pdf:pdf},
journal = {Linear algebra and its applications},
keywords = {absolutely equally distributed,e-mail,edu,eigenvalues,equally distributed,singular values,trinity,wtrench},
pages = {131--146},
title = {{Absolute equal distribution of families of finite sets}},
volume = {367},
year = {2003}
}
@article{Cadieu2005,
author = {Cadieu, Charles F. and Olshausen, Bruno A.},
file = {:home/tom/Work/phd/Papers/Cadieu, Olshausen_2005_Learning Invariant and Variant Components of Time Varying Natural Images Using a Sparse , Multiplicative Model.pdf:pdf},
journal = {Journal of Vision},
number = {June},
pages = {1247--1283},
title = {{Learning Invariant and Variant Components of Time Varying Natural Images Using a Sparse , Multiplicative Model}},
volume = {12},
year = {2005}
}
@book{Bandeira2015,
author = {Bandeira, Afonso S},
file = {:home/tom/Work/phd/Papers/Bandeira_2015_Ten Lectures and Forty-Two Open Problems in the Mathematics of Data Science.pdf:pdf},
pages = {1--151},
title = {{Ten Lectures and Forty-Two Open Problems in the Mathematics of Data Science}},
year = {2015}
}
@article{Hesterberg2008,
archivePrefix = {arXiv},
arxivId = {0802.0964},
author = {Hesterberg, Tim and Choi, Nam Hee and Meier, Lukas and Fraley, Chris},
doi = {10.1214/08-SS035},
eprint = {0802.0964},
file = {:home/tom/Work/phd/Papers/Hesterberg et al._2008_Least angle and $ell_1$ penalized regression A review.pdf:pdf},
isbn = {1935-7516},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {Received,lasso,regression,regularization,vari- able selection.,ℓ1 penalty},
pages = {61--93},
title = {{Least angle and $\ell_1$ penalized regression: A review}},
volume = {2},
year = {2008}
}
@misc{Hinton2012,
author = {Hinton, Geoffrey E and Srivastava, Nitish and Swersky, Kevin},
booktitle = {COURSERA: Neural Networks for Machine Learning},
file = {:home/tom/Work/phd/Papers/Hinton, Srivastava, Swersky_2012_Lecture 6a- overview of mini-batch gradient descent.pdf:pdf},
howpublished = {Slide for online class COURSERA: Neural Networks for Machine Learning},
pages = {31},
title = {{Lecture 6a- overview of mini-batch gradient descent}},
url = {http://www.cs.toronto.edu/$\sim$tijmen/csc321/slides/lecture_slides_lec6.pdf},
year = {2012}
}
@misc{Sifre2014,
abstract = {Image classification is the problem of assigning a label that best describes the content of unknown images, given a set of training images with known labels. This thesis introduces image classification algorithms based on the scattering transform, studies their properties and describes extensive classification experiments on challenging texture and object image datasets. Images are high dimensional signals for which generic machine learning algorithms fail when applied directly on the raw pixel space. Therefore, most successful approaches involve building a specific low dimensional representation on which the classification is performed. Traditionally, the representation was engineered to reduce the dimensionality of images by building invariance to geometric transformations while retaining discriminative features. More recently, deep convolutional networks have achieved state-of-the-art results on most image classification tasks. Such networks progressively build more invariant representations through a hierarchy of convolutional layers where all the weights are learned. This thesis proposes several scattering representations. Those scattering representa-tions have a structure similar to convolutional networks, but the weights of scattering are designed to provide mathematical guaranty of invariance to geometric transformations, sta-bility to deformations and energy preservation. In this thesis, we focus on affine and more specifically on rigid-motion transformations, which consist in translations and rotations, and which are common in real world images. Translation scattering is a cascade of two dimensional wavelet modulus operators which builds translation invariance. We propose a first separable rigid-motion separable scatter-ing, which applies a first scattering along the position variable to build translation in-variance, followed by a second scattering transform along the rotational orbits of the first scattering, to build invariance to rotations. As any separable representation, separable scattering has the advantage of simplicity but also loses some information about the joint distribution of positions and orientations in the intermediate layers of the representation. We define a joint rigid-motion scattering which does retain this information. The joint scattering consists in a cascade of wavelet modulus applied directly on the joint rigid-motion group. We introduce convolutions, wavelets, a wavelet transform and scattering on the rigid-motion group and propose fast implementations. Both separable and joint scattering are applied to texture image classi-fication with state-of-the-art results on most available texture datasets. Finally, we demonstrate the applicability of joint scattering and group convolutions on generic object image datasets. It is shown that convolutional networks performances are enhanced through the use of separable convolutions, similar to the rigid-motion con-volutions. Also, a non-invariant version of the rigid-motion scattering is demonstrated to achieve results similar to those obtained by the first layers of convolutional networks.},
author = {Sifre, Laurent},
file = {:home/tom/Work/phd/Papers/Sifre_2014_Rigid-Motion Scattering For Image Classification.pdf:pdf},
howpublished = {PhD Thesis},
pages = {128},
title = {{Rigid-Motion Scattering For Image Classification}},
year = {2014}
}
@inproceedings{Haeffele2017,
address = {Honolulu, HI, USA},
author = {Haeffele, Benjamin D and Vidal, Rene},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.467},
file = {:home/tom/Work/phd/Papers/Haeffele, Vidal_2017_Global Optimality in Neural Network Training.pdf:pdf},
pages = {7331--7339},
title = {{Global Optimality in Neural Network Training}},
url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Haeffele_Global_Optimality_in_CVPR_2017_paper.pdf},
year = {2017}
}
@article{Bioucas-Dias2007,
author = {Bioucas-Dias, Jos?? M. and Figueiredo, M??rio A T},
doi = {10.1109/TIP.2007.909319},
file = {:home/tom/Work/phd/Papers/Bioucas-Dias, Figueiredo_2007_A new TwIST Two-step iterative shrinkagethresholding algorithms for image restoration.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Algorithm design and analysis,Convex analysis,Deconvolution,Image deconvolution,Image restoration,Non-smooth optimization,Optimization,Regularization,Total variation,Wavelets},
number = {12},
pages = {2992--3004},
pmid = {18092598},
title = {{A new TwIST: Two-step iterative shrinkage/thresholding algorithms for image restoration}},
volume = {16},
year = {2007}
}
@misc{libby2012simple,
author = {Libby, R},
howpublished = {Research report},
title = {{A Simple Method for Reliable Footstep Detection in Embedded Sensor Platforms}},
year = {2012}
}
@article{Wohlberg2017,
archivePrefix = {arXiv},
arxivId = {1705.04407},
author = {Wohlberg, Brendt},
eprint = {1705.04407},
file = {:home/tom/Work/phd/Papers/Wohlberg_2017_Convolutional Sparse Representations with Gradient Penalties.pdf:pdf},
journal = {arXiv preprint},
number = {04407},
title = {{Convolutional Sparse Representations with Gradient Penalties}},
url = {http://arxiv.org/abs/1705.04407},
volume = {arXiv:1705},
year = {2017}
}
@article{Oymak2015,
abstract = {In this paper we characterize sharp time-data tradeoffs for optimization problems used for solving linear inverse problems. We focus on the minimization of a least-squares objective subject to a constraint defined as the sub-level set of a penalty function. We present a unified convergence analysis of the gradient projection algorithm applied to such problems. We sharply characterize the convergence rate associated with a wide variety of random measurement ensembles in terms of the number of measurements and structural complexity of the signal with respect to the chosen penalty function. The results apply to both convex and nonconvex constraints, demonstrating that a linear convergence rate is attainable even though the least squares objective is not strongly convex in these settings. When specialized to Gaussian measurements our results show that such linear convergence occurs when the number of measurements is merely $4$ times the minimal number required to recover the desired signal at all (a.k.a. the phase transition). We also achieve a slower but geometric rate of convergence precisely above the phase transition point. Extensive numerical results suggest that the derived rates exactly match the empirical performance.},
archivePrefix = {arXiv},
arxivId = {1507.04793},
author = {Oymak, Samet and Recht, Benjamin and Soltanolkotabi, Mahdi},
eprint = {1507.04793},
file = {:home/tom/Work/phd/Papers/Oymak, Recht, Soltanolkotabi_2015_Sharp Time-Data Tradeoffs for Linear Inverse Problems.pdf:pdf},
journal = {arXiv preprint},
number = {04793},
title = {{Sharp Time-Data Tradeoffs for Linear Inverse Problems}},
url = {http://arxiv.org/abs/1507.04793},
volume = {arXiv:1507},
year = {2015}
}
@article{Combettes2008,
archivePrefix = {arXiv},
arxivId = {0807.2617},
author = {Combettes, Patrick L and Pesquet, Jean-Christophe},
doi = {10.1088/0266-5611/24/6/065014},
eprint = {0807.2617},
file = {:home/tom/Work/phd/Papers/Combettes, Pesquet_2008_A Proximal Decomposition Method for Solving Convex Variational Inverse Problems.pdf:pdf},
issn = {0266-5611, 1361-6420},
journal = {Inverse Problems},
keywords = {Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
number = {6},
pages = {65014},
title = {{A Proximal Decomposition Method for Solving Convex Variational Inverse Problems}},
url = {http://arxiv.org/abs/0807.2617},
volume = {24},
year = {2008}
}
@inproceedings{Krishnan2011,
address = {Colorado Spring, CO, USA},
author = {Krishnan, Dilip and Tay, Terence and Fergus, Rob},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Work/phd/Papers/Krishnan, Tay, Fergus_2011_Blind deconvolution using a normalized sparsity measure.pdf:pdf},
isbn = {1457703947},
keywords = {Deconvolution,transport},
mendeley-tags = {Deconvolution,transport},
pages = {233--240},
title = {{Blind deconvolution using a normalized sparsity measure}},
year = {2011}
}
@article{Overschee1994,
author = {{Van Overschee}, Peter and {De Moor}, Bart},
file = {:home/tom/Work/phd/Papers/Van Overschee, De Moor_1994_N4SID Subspace Algorithms for the Stochastic Systemst.pdf:pdf},
journal = {Automatica},
keywords = {--system identification,abstract--recently a great deal,decomposition,difference equations,given,kalman filters,multivariable systems,of attention has been,or and singular value,state space methods,subspace state space system,to numerical algorithms for},
number = {1},
pages = {75--93},
title = {{N4SID: Subspace Algorithms for the Stochastic Systemst}},
volume = {30},
year = {1994}
}
@inproceedings{DAspremont2007a,
address = {Corvallis, United States},
archivePrefix = {arXiv},
arxivId = {arXiv:0707.0705v3},
author = {D'Aspremont, Alexandre and Bach, Francis R and Ghaoui, Laurent El},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.1145/1273496.1273519},
eprint = {arXiv:0707.0705v3},
file = {:home/tom/Work/phd/Papers/d'Aspremont, Bach, Ghaoui_2007_Full regularization path for sparse principal component analysis.pdf:pdf},
isbn = {9781595937933},
number = {1},
pages = {177--184},
title = {{Full regularization path for sparse principal component analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273519},
year = {2007}
}
@inproceedings{fortunestep,
address = {Gainesville, FL, USA},
author = {Fortune, E and Lugade, V and Morrow, M and Kaufman, K},
booktitle = {American Society of Biomechanics Annual Meeting (ASB)},
title = {{Step counts using a tri-axial accelerometer during activity}},
year = {2012}
}
@inproceedings{Wang2012a,
address = {Beijing, China},
author = {Wang, Fei and Lee, Noah and Hu, Jianying and Sun, Jimeng and Ebadollahi, Shahram},
booktitle = {SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2339530.2339605},
file = {:home/tom/Work/phd/Papers/Wang et al._2012_Towards Heterogeneous Temporal Clinical Event Pattern Discovery A Convolutional Approach.pdf:pdf},
isbn = {9781450314626},
keywords = {convolution,dictionary learning,nmf,pattern discovery},
pages = {453--461},
publisher = {ACM},
title = {{Towards Heterogeneous Temporal Clinical Event Pattern Discovery : A Convolutional Approach}},
year = {2012}
}
@article{Dempster1977,
author = {Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
file = {:home/tom/Work/phd/Papers/Dempster, Laird, Rubin_1977_Dempster, Arthur P and Laird, Nan M and Rubin, Donald B.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (statistical methodology)},
number = {1},
pages = {1--38},
title = {{Dempster, Arthur P and Laird, Nan M and Rubin, Donald B}},
volume = {39},
year = {1977}
}
@article{Lin2015a,
abstract = {We consider the problem of minimizing the sum of two convex functions: one is smooth and given by a gradient oracle, and the other is separable over blocks of coordinates and has a simple known structure over each block. We develop an accelerated randomized proximal coordinate gradient (APCG) method for minimizing such convex composite functions. For strongly convex functions, our method achieves faster linear convergence rates than existing randomized proximal coordinate gradient methods. Without strong convexity, our method enjoys accelerated sublinear convergence rates. We show how to apply the APCG method to solve the regularized empirical risk minimization (ERM) problem and devise efficient implementations that avoid full-dimensional vector operations. For ill-conditioned ERM problems, our method obtains improved convergence rates than the state-of-the-art stochastic dual coordinate ascent method.},
archivePrefix = {arXiv},
arxivId = {1407.1296},
author = {Lin, Qihang and Lu, Zhaosong and Xiao, Lin},
doi = {10.1137/141000270},
eprint = {1407.1296},
file = {:home/tom/Work/phd/Papers/Lin, Lu, Xiao_2015_An Accelerated Randomized Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minim.pdf:pdf},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
keywords = {65Y20,68W20,90C06,90C25,acceler-ated proximal gradient method,convex optimization,coordinate descent method,randomized algorithm},
number = {4},
pages = {2244--2273},
title = {{An Accelerated Randomized Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minimization}},
url = {http://epubs.siam.org/doi/10.1137/141000270},
volume = {25},
year = {2015}
}
@article{renaudin2012step,
author = {Renaudin, V and Susi, M and Lachapelle, G},
journal = {Sensors},
number = {7},
pages = {8507--8525},
title = {{Step length estimation using handheld inertial sensors}},
volume = {12},
year = {2012}
}
@inproceedings{Jenatton2009,
address = {Clearwater Beach, FL, USA},
archivePrefix = {arXiv},
arxivId = {0909.1440},
author = {Jenatton, Rodolphe and Obozinski, Guillaume and Bach, Francis},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
doi = {1553374},
eprint = {0909.1440},
file = {:home/tom/Work/phd/Papers/Jenatton, Obozinski, Bach_2009_Structured Sparse Principal Component Analysis.pdf:pdf},
issn = {15324435},
pages = {366--373},
title = {{Structured Sparse Principal Component Analysis}},
url = {http://arxiv.org/abs/0909.1440},
year = {2009}
}
@inproceedings{Kavukcuoglu2013,
address = {Vancouver, Canada},
author = {Kavukcuoglu, Koray and Sermanet, Pierre and Boureau, Y-lan and Gregor, Karol and Lecun, Yann},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/Kavukcuoglu et al._2010_Learning Convolutional Feature Hierarchies for Visual Recognition.pdf:pdf},
keywords = {convolution,dictionary learning},
pages = {1090--1098},
title = {{Learning Convolutional Feature Hierarchies for Visual Recognition}},
year = {2010}
}
@article{Oculo19,
author = {Lavery, MA and O'Neill, JF and Chu, FC and Martyn, LJ},
journal = {Ophthalmology},
number = {91},
pages = {425--453},
title = {{Acquired nystagmus in early childhood: a presenting sign of intracranial tumor.}},
volume = {May},
year = {1984}
}
@article{Saigo2004,
abstract = {MOTIVATION: Remote homology detection between protein sequences is a central problem in computational biology. Discriminative methods involving support vector machines (SVMs) are currently the most effective methods for the problem of superfamily recognition in the Structural Classification Of Proteins (SCOP) database. The performance of SVMs depends critically on the kernel function used to quantify the similarity between sequences. RESULTS: We propose new kernels for strings adapted to biological sequences, which we call local alignment kernels. These kernels measure the similarity between two sequences by summing up scores obtained from local alignments with gaps of the sequences. When tested in combination with SVM on their ability to recognize SCOP superfamilies on a benchmark dataset, the new kernels outperform state-of-the-art methods for remote homology detection. AVAILABILITY: Software and data available upon request.},
author = {Saigo, Hiroto and Vert, Jean Philippe and Ueda, Nobuhisa and Akutsu, Tatsuya},
doi = {10.1093/bioinformatics/bth141},
file = {:home/tom/Work/phd/Papers/Saigo et al._2004_Protein homology detection using string alignment kernels.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {11},
pages = {1682--1689},
pmid = {14988126},
title = {{Protein homology detection using string alignment kernels}},
volume = {20},
year = {2004}
}
@inproceedings{Haeffele2017b,
address = {Quebec, Canada},
archivePrefix = {arXiv},
arxivId = {1706.01912},
author = {Haeffele, Benjamin D and Stahl, Richard and Vanmeerbeeck, Geert and Vidal, Ren{\'{e}}},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
doi = {10.1007/978-3-319-66185-8},
eprint = {1706.01912},
file = {:home/tom/Work/phd/Papers/Haeffele et al._2017_Efficient Reconstruction of Holographic Lens-Free Images by Sparse Phase Recovery.pdf:pdf},
isbn = {978-3-319-66184-1},
keywords = {holography,lens-free imaging,sparsity},
pages = {109--117},
title = {{Efficient Reconstruction of Holographic Lens-Free Images by Sparse Phase Recovery}},
url = {http://link.springer.com/10.1007/978-3-319-66185-8},
year = {2017}
}
@inproceedings{Zhang2010,
address = {San Francisco, CA, USA},
author = {Zhang, Qiang and Li, Baoxin},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Work/phd/Papers/Zhang, Li_2010_Discriminative K-SVD for Dictionary Learning in Face Recognition.pdf:pdf},
title = {{Discriminative K-SVD for Dictionary Learning in Face Recognition}},
year = {2010}
}
@article{Bozzo2010,
author = {Bozzo, Enrico and Carniel, Roberto and Fasino, Dario},
doi = {10.1016/j.camwa.2010.05.028},
file = {:home/tom/Work/phd/Papers/Bozzo, Carniel, Fasino_2010_Relationship between Singular Spectrum Analysis and Fourier analysis Theory and application to the monitorin.pdf:pdf},
issn = {08981221},
journal = {Computers and Mathematics with Applications},
keywords = {Fourier analysis,Singular Spectrum Analysis,Time series,Toeplitz matrices,Volcanic tremor},
number = {3},
pages = {812--820},
publisher = {Elsevier Ltd},
title = {{Relationship between Singular Spectrum Analysis and Fourier analysis: Theory and application to the monitoring of volcanic activity}},
url = {http://dx.doi.org/10.1016/j.camwa.2010.05.028},
volume = {60},
year = {2010}
}
@article{Schmidt2013,
abstract = {We propose the stochastic average gradient (SAG) method for optimizing the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method's iteration cost is independent of the number of terms in the sum. However, by incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate than black-box SG methods. The convergence rate is improved from O(1/k^{1/2}) to O(1/k) in general, and when the sum is strongly-convex the convergence rate is improved from the sub-linear O(1/k) to a linear convergence rate of the form O(p^k) for p < 1. Further, in many cases the convergence rate of the new method is also faster than black-box deterministic gradient methods, in terms of the number of gradient evaluations. Numerical experiments indicate that the new algorithm often dramatically outperforms existing SG and deterministic gradient methods, and that the performance may be further improved through the use of non-uniform sampling strategies.},
archivePrefix = {arXiv},
arxivId = {1309.2388},
author = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
eprint = {1309.2388},
file = {:home/tom/Work/phd/Papers/Schmidt, Roux, Bach_2013_Minimizing Finite Sums with the Stochastic Average Gradient.pdf:pdf},
journal = {arXiv preprint},
number = {2388},
title = {{Minimizing Finite Sums with the Stochastic Average Gradient}},
url = {http://arxiv.org/abs/1309.2388},
volume = {arXiv:1309},
year = {2013}
}
@article{Oculo15,
author = {Huang, MY and CC, Chen and Huber-Reggi, SP and Neuhauss, SC and Straumann, D.},
journal = {Annals of the New-York Academy of Science},
number = {1233},
pages = {285--291},
title = {{Comparison of infantile nystagmus syndrome in achiasmatic zebrafish and humans}},
volume = {Sep},
year = {2011}
}
@inproceedings{Dalal2005,
address = {San Diego, CA, USA},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Dalal, Navneet and Triggs, Bill},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2005.177},
eprint = {9411012},
file = {:home/tom/Work/phd/Papers/Dalal, Triggs_2005_Histograms of Oriented Gradients for Human Detection.pdf:pdf},
isbn = {0-7695-2372-2},
issn = {1063-6919},
pages = {886--893},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{Histograms of Oriented Gradients for Human Detection}},
year = {2005}
}
@article{Aharon2006,
author = {Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
file = {:home/tom/Work/phd/Papers/Aharon, Elad, Bruckstein_2006_K-SVD An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation.pdf:pdf},
journal = {IEEE Transaction on Signal Processing},
number = {11},
pages = {4311--4322},
title = {{K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation}},
volume = {54},
year = {2006}
}
@inproceedings{collobert2008unified,
address = {Helsinki, Finland},
author = {Collobert, Ronan and Weston, Jason},
booktitle = {International Conference on Machine Learning (ICML)},
organization = {ACM},
pages = {160--167},
title = {{A unified architecture for natural language processing: Deep neural networks with multitask learning}},
year = {2008}
}
@article{Tao2015,
archivePrefix = {arXiv},
arxivId = {1501.02888},
author = {Tao, Shaozhe and Boley, Daniel and Zhang, Shuzhong},
eprint = {1501.02888},
journal = {arXiv preprint},
number = {02888},
title = {{Local Linear Convergence of ISTA and FISTA on the LASSO Problem}},
volume = {arXiv:1501},
year = {2015}
}
@article{Oculo4,
author = {Cullen, Kathleen E. and {Van Horn}, Marion R.},
journal = {European Journal of Neuroscience},
number = {33},
pages = {2147--2154},
title = {{The neural control of fast vs. slow vergence eye movements}},
volume = {Jun},
year = {2011}
}
@inproceedings{Kim2010,
address = {Vancouver, Canada},
author = {Kim, Taehwan and Shakhnarovich, Gregory and Urtasun, Raquel},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/Kim, Shakhnarovich, Urtasun_2010_Sparse Coding for Learning Interpretable Spatio-Temporal Primitives.pdf:pdf},
isbn = {9781617823800},
pages = {1117--1125},
title = {{Sparse Coding for Learning Interpretable Spatio-Temporal Primitives}},
year = {2010}
}
@article{Schmidt2008,
author = {Schmidt, Mikkel N and Hansen, Lars Kai},
file = {:home/tom/Work/phd/Papers/Schmidt, Hansen_2008_Shift Invariant Sparse Coding of Image and Music Data.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
pages = {1--14},
title = {{Shift Invariant Sparse Coding of Image and Music Data}},
year = {2008}
}
@article{Aharon2008,
author = {Aharon, Michal and Elad, Michael},
file = {:home/tom/Work/phd/Papers/Aharon, Elad_2008_Sparse and Redundant Modeling of Image Content Using an Image-Signature-Dictionary.pdf:pdf},
journal = {SIAM Journal on Imaging Sciences},
keywords = {denoising,dictionary,image-signature,learning,matching pursuit,mod,sparse representation,subject classifications},
number = {3},
pages = {228--247},
title = {{Sparse and Redundant Modeling of Image Content Using an Image-Signature-Dictionary}},
volume = {1},
year = {2008}
}
@inproceedings{Berthet2013,
address = {Princeton, United States},
archivePrefix = {arXiv},
arxivId = {arXiv:1304.0828v2},
author = {Berthet, Quentin and Rigollet, Philippe},
booktitle = {Conference on Learning Theory (COLT)},
eprint = {arXiv:1304.0828v2},
file = {:home/tom/Work/phd/Papers/Berthet, Rigollet_2013_Computational Lower Bounds for Sparse PCA.pdf:pdf},
keywords = {and phrases,planted,polynomial-time reduction,sparse pca},
pages = {1046--1066},
title = {{Computational Lower Bounds for Sparse PCA}},
url = {http://arxiv.org/abs/1304.0828},
year = {2013}
}
@article{Hong2012,
archivePrefix = {arXiv},
arxivId = {1208.3922},
author = {Hong, Mingyi and Luo, Zhi-Quan},
eprint = {1208.3922},
file = {:home/tom/Work/phd/Papers/Hong, Luo_2012_On the Linear Convergence of the Alternating Direction Method of Multipliers.pdf:pdf},
journal = {arXiv preprint},
keywords = {49,90,alternating directions of multipliers,ams,by the national science,computer engineering,department of electrical and,dual ascent,error bound,foundation,grant number dms-1015346,linear convergence,minneapolis,mn 55455,mos,subject classifications,the research is supported,university of minnesota,usa},
number = {3922},
title = {{On the Linear Convergence of the Alternating Direction Method of Multipliers}},
url = {http://arxiv.org/abs/1208.3922},
volume = {arXiv:1208},
year = {2012}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
address = {San Diego, CA, USA},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
booktitle = {International Conference on Learning Representation (ICLR)},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
file = {:home/tom/Work/phd/Papers/Kingma, Ba_2015_Adam A Method for Stochastic Optimization.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
pages = {1--10},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2015}
}
@article{jerri1977shannon,
author = {Jerri, Abdul J},
journal = {Proceedings of the IEEE},
number = {11},
pages = {1565--1596},
publisher = {IEEE},
title = {{The Shannon sampling theorem, Its various extensions and applications: A tutorial review}},
volume = {65},
year = {1977}
}
@article{Naik2011,
abstract = {Independent Component Analysis (ICA), a computationally efficient blind source separation technique, has been an area of interest for researchers for many practical applications in various fields of science and engineering. This paper attempts to cover the fundamental concepts involved in ICA techniques and review its applications. A thorough discussion of the applications and ambiguities problems of ICA has been carried out.Different ICA methods and their applications in various disciplines of science and engineering have been reviewed. In this paper, we present ICA methods from the basics to their potential applications to serve as a comprehensive single source for an inquisitive researcher to carry out research in this field. Povzetek: Podan je pregled tehnike ICA (Independent Component Analysis).},
author = {Naik, Ganesh R and Kumar, Dinesh K},
file = {:home/tom/Work/phd/Papers/Naik, Kumar_2011_An Overview of Independent Component Analysis and Its Applications.pdf:pdf},
isbn = {9780123747266},
issn = {1854-3871},
journal = {Informatica},
keywords = {blind source separation,independent component analysis,multi run ICA,non-gaussianity,overcomplete ICA,undercomplete ICA},
pages = {63--81},
title = {{An Overview of Independent Component Analysis and Its Applications}},
volume = {35},
year = {2011}
}
@article{Luo1993,
author = {Luo, Zhi Quan and Tseng, Paul},
doi = {10.1007/BF02096261},
file = {:home/tom/Work/phd/Papers/Luo, Tseng_1993_Error bounds and convergence analysis of feasible descent methods a general approach.pdf:pdf},
issn = {02545330},
journal = {Annals of Operations Research},
keywords = {Error bound,feasible descent methods,linear convergence},
number = {1},
pages = {157--178},
title = {{Error bounds and convergence analysis of feasible descent methods: a general approach}},
volume = {46-47},
year = {1993}
}
@article{Wang2007,
abstract = {A nonparametric multiple regression method, based on the extended Gini that depends on one parameter, v, is investigated. The parameter v enables production of infinite alternative linear approximations to the regression curve which differ in the weighting schemes applied to the slopes of the curve. The method allows the investigator to stress different sections of one independent variable while keeping the treatment of the other independent variables intact. As an application we investigate nonresponse patterns in a survey of household expenditures to learn about the relationship between nonresponse and income. The empirical results show that the higher the income, the higher the response rate, and the larger the household, the higher the response rate. The Arab population tends to respond more than the Jewish one, whereas the ultrareligious group tends to respond less than the rest of the population. The implications on the bias in the estimates are discussed.},
author = {Wang, Hansheng and Guodong, Li and Guohua, Jiang},
doi = {10.1198/00},
file = {:home/tom/Work/phd/Papers/Wang, Guodong, Guohua_2007_Robust Regression Shrinkage and Consistent Variable Selection Through the LAD-Lasso.pdf:pdf},
isbn = {0040170040000},
issn = {0735-0015},
journal = {Journal of Business \& Economic Statistics},
keywords = {lad,lad-lasso,lasso,oracle property},
number = {3},
pages = {347--355},
pmid = {18392118},
title = {{Robust Regression Shrinkage and Consistent Variable Selection Through the LAD-Lasso}},
url = {http://www.tandfonline.com/doi/abs/10.1198/00},
volume = {25},
year = {2007}
}
@article{Song1997,
author = {Song, D and Gupta, Arjun K.},
file = {:home/tom/Work/phd/Papers/Song, Gupta_1997_$L_p$-norm Uniform Distribution.pdf:pdf},
journal = {The American Mathematical Society},
number = {2},
pages = {595--601},
title = {{$L_p$-norm Uniform Distribution}},
volume = {125},
year = {1997}
}
@article{Oculo13,
author = {Gresty, M. and Ell, J. and Findley, L.},
journal = {Journal of Neurology, Neurosurgery and Psychiatry},
number = {45},
pages = {431--439},
title = {{Acquired pendular nystagmus: its characteristics, localising value and pathophysiology}},
volume = {May},
year = {1982}
}
@inproceedings{larochelle2007empirical,
address = {Corvallis, United States},
author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
booktitle = {International Conference on Machine Learning (ICML)},
organization = {ACM},
pages = {473--480},
title = {{An empirical evaluation of deep architectures on problems with many factors of variation}},
year = {2007}
}
@article{yang2015randomized,
author = {Yang, Yun and Pilanci, Mert and Wainwright, Martin J.},
journal = {arXiv preprint},
number = {06195},
title = {{Randomized sketches for kernels: Fast and optimal non-parametric regression}},
volume = {arXiv:1501},
year = {2015}
}
@inproceedings{Listgarten2005,
address = {Vancouver, Canada},
author = {Listgarten, Jennifer and Neal, Radford M and Roweis, Sam T and Emili, Andrew},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/Listgarten et al._2004_Multiple alignment of continuous time series.pdf:pdf},
pages = {817--824},
title = {{Multiple alignment of continuous time series}},
year = {2004}
}
@inproceedings{Zheng2015,
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1503.05479v1},
author = {Zheng, Qinqing and Tomioka, Ryota},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1503.05479v1},
pages = {3106--3113},
title = {{Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm}},
year = {2015}
}
@inproceedings{Knopp2010,
abstract = {Most methods for the recognition of shape classes from 3D datasets focus on classifying clean, often manually generated models. However, 3D shapes obtained through acquisition techniques such as Structure-from-Motion or LIDAR scanning are noisy, clutter and holes. In that case global shape features-still dominating the 3D shape class recognition literature-are less appropriate. Inspired by 2D methods, recently researchers have started to work with local features. In keeping with this strand, we propose a new robust 3D shape classification method. It contains two main contributions. First, we extend a robust 2D feature descriptor, SURF, to be used in the context of 3D shapes. Second, we show how 3D shape class recognition can be improved by probabilistic Hough transform based methods, already popular in 2D. Through our experiments on partial shape retrieval, we show the power of the proposed 3D features. Their combination with the Hough transform yields superior results for class recognition on standard datasets. The potential for the applicability of such a method in classifying 3D obtained from Structure-from-Motion methods is promising, as we show in some initial experiments.},
address = {Heraklion, Crete, Greece},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Knopp, Jan and Prasad, Mukta and Willems, Geert and Timofte, Radu and {Van Gool}, Luc},
booktitle = {European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-642-15567-3_43},
eprint = {9780201398298},
file = {:home/tom/Work/phd/Papers/Knopp et al._2010_Hough transform and 3D SURF for robust three dimensional classification.pdf:pdf},
isbn = {3642155669},
issn = {03029743},
pages = {589--602},
pmid = {4520227},
title = {{Hough transform and 3D SURF for robust three dimensional classification}},
year = {2010}
}
@article{dijkstra2008detection,
author = {Dijkstra, B and Zijlstra, W and Scherder, E and Kamsma, Y},
journal = {Age and ageing},
number = {4},
pages = {436--441},
publisher = {Br Geriatrics Soc},
title = {{Detection of walking periods and number of steps in older adults and patients with Parkinson's disease: accuracy of a pedometer and an accelerometry-based method}},
volume = {37},
year = {2008}
}
@article{Scaman2017,
abstract = {In this paper, we determine the optimal convergence rates for strongly convex and smooth distributed optimization in two settings: centralized and decentralized communications over a network. For centralized (i.e. master/slave) algorithms, we show that distributing Nesterov's accelerated gradient descent is optimal and achieves a precision $\varepsilon \textgreater{} 0$ in time $O(\sqrt{\kappa\_g}(1+\Delta\tau)\ln(1/\varepsilon))$, where $\kappa\_g$ is the condition number of the (global) function to optimize, $\Delta$ is the diameter of the network, and $\tau$ (resp. $1$) is the time needed to communicate values between two neighbors (resp. perform local computations). For decentralized algorithms based on gossip, we provide the first optimal algorithm, called the multi-step dual accelerated (MSDA) method, that achieves a precision $\varepsilon \textgreater{} 0$ in time $O(\sqrt{\kappa\_l}(1+\frac{\tau}{\sqrt{\gamma}})\ln(1/\varepsilon))$, where $\kappa\_l$ is the condition number of the local functions and $\gamma$ is the (normalized) eigengap of the gossip matrix used for communication between nodes. We then verify the efficiency of MSDA against state-of-the-art methods for two problems: least-squares regression and classification by logistic regression.},
archivePrefix = {arXiv},
arxivId = {1702.08704},
author = {Scaman, Kevin and Bach, Francis and Bubeck, S{\'{e}}bastien and Lee, Yin Tat and Massouli{\'{e}}, Laurent},
eprint = {1702.08704},
file = {:home/tom/Work/phd/Papers/Scaman et al._2017_Optimal algorithms for smooth and strongly convex distributed optimization in networks.pdf:pdf},
journal = {arXiv preprint},
number = {08704},
title = {{Optimal algorithms for smooth and strongly convex distributed optimization in networks}},
url = {http://arxiv.org/abs/1702.08704},
volume = {arXiv:1702},
year = {2017}
}
@article{Mallat1993,
author = {Mallat, St\'ephane and Zhang, Zhifeng},
file = {:home/tom/Work/phd/Papers/Mallat, Zhang_1993_Matching Pursuits With Time-Frequency Dictionaries.pdf:pdf},
journal = {IEEE Transactions on Signal Processing},
number = {12},
pages = {3397--3415},
title = {{Matching Pursuits With Time-Frequency Dictionaries}},
volume = {41},
year = {1993}
}
@article{Oculo3,
author = {Brodsky, MC and Keating, GF},
journal = {Neuro-Opthalmology},
number = {34},
pages = {274--275},
title = {{Chiasmal glioma in spasmus nutans: a cautionary note}},
volume = {Sep},
year = {2014}
}
@inproceedings{Liu2010,
abstract = {Recently the low-rank representation (LRR) has been successfully used in exploring the multiple subspace structures of data. It assumes that the observed data is drawn from several low-rank subspaces and sometimes contaminated by outliers and occlusions. However, the noise (low-rank representation residual) is assumed to be sparse, which is generally characterized by minimizing the l1 -norm of the residual. This actually assumes that the residual follows the Laplacian distribution. The Laplacian assumption, however, may not be accurate enough to describe various noises in real scenarios. In this paper, we propose a new framework, termed robust low-rank representation, by considering the low-rank representation as a low-rank constrained estimation for the errors in the observed data. This framework aims to find the maximum likelihood estimation solution of the low-rank representation residuals. We present an efficient iteratively reweighted inexact augmented Lagrange multiplier algorithm to solve the new problem. Extensive experimental results show that our framework is more robust to various noises (illumination, occlusion, etc) than LRR, and also outperforms other state-of-the-art methods.},
address = {Haifa, Israel},
archivePrefix = {arXiv},
arxivId = {0810.3286v1},
author = {Liu, Guangcan and Lin, Zhouchen and Yu, Yong},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.1109/TCYB.2013.2286106},
eprint = {0810.3286v1},
file = {:home/tom/Work/phd/Papers/Liu, Lin, Yu_2010_Robust subspace segmentation via low-rank representation.pdf:pdf},
isbn = {9781605589077},
issn = {21682267},
keywords = {Low-rank representation,matrix recovery,robust regression,subspace segmentation},
pages = {663--670},
pmid = {24196982},
title = {{Robust subspace segmentation via low-rank representation}},
year = {2010}
}
@inproceedings{Papagerogakis2017,
author = {Papagerogakis, Christos and Hitziger, Sebastian and Papadopoulo, Th\'eodore},
booktitle = {Groupe de Recherche et d'Etudes en Traitement du Signal et des Images (GRETSI)},
file = {:home/tom/Work/phd/Papers/Papagerogakis, Hitziger, Papadopoulo_2017_Dictionary Learning for multidimensional data.pdf:pdf},
pages = {1--4},
title = {{Dictionary Learning for multidimensional data}},
year = {2017}
}
@inproceedings{mladenov2009step,
address = {Dublin, Ireland},
author = {Mladenov, M and Mock, M},
booktitle = {International Workshop on Context-Aware Middleware and Services (COMSWARE)},
organization = {ACM},
pages = {1--5},
title = {{A step counter service for Java-enabled devices using a built-in accelerometer}},
year = {2009}
}
@article{Rey1991,
abstract = {A new method for nystagmus classification, using system identification techniques, is presented. We formulate a system whose input is head position and whose output is eye position. We approximate this system with an autoregressive with exogenous input (ARX) model which relates the input and output (transfer function) regardless of the temporal profile for the sensory stimulation. The system is then identified using a least squares criteria and three indicators are produced. From these a flag is produced that marks slow and fast phases as well as blinks and bad data segments. Tests with simulated and real data are presented and indicate that the segment classification is remarkably insensitive to recording noise and that it is more robust than previous techniques. Operator intervention is minimal. We expect the method to be applicable for all types of ocular nystagmus. Here, however, we illustrate our results only in the context of the vestibuloocular reflex (VOR). A discussion explains how this method can also be applied for optokinetic (OKN) or pursuit nystagmus.},
author = {Rey, C. G. and Galiana, Henrietta L.},
doi = {10.1109/10.76379},
file = {:home/tom/Work/phd/Papers/Rey, Galiana_1991_Parametric classification of segments in ocular nystagmus.pdf:pdf},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
number = {2},
pages = {142--148},
pmid = {2066123},
title = {{Parametric classification of segments in ocular nystagmus}},
volume = {38},
year = {1991}
}
@phdthesis{Goroshin2015,
author = {Goroshin, Rostislav},
file = {:home/tom/Work/phd/Papers/Goroshin_2015_Unsupervised Feature Learning in Computer Vision.pdf:pdf},
number = {September},
pages = {1----100},
school = {New-York University},
title = {{Unsupervised Feature Learning in Computer Vision}},
year = {2015}
}
@inproceedings{Afsari2012,
address = {Providence, RI, USA},
author = {Afsari, B and Chaudhry, R and Ravichandran, A and Vidal, R},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2012.6247929},
file = {:home/tom/Work/phd/Papers/Afsari et al._2012_Group action induced distances for averaging and clustering Linear Dynamical Systems with applications to the analysi.pdf:pdf},
isbn = {978-1-4673-1228-8},
issn = {1063-6919},
keywords = {LDS averaging algorithm,LDS statespace realization},
pages = {2208--2215},
title = {{Group action induced distances for averaging and clustering Linear Dynamical Systems with applications to the analysis of dynamic scenes}},
year = {2012}
}
@article{Oculo20,
author = {Lee, A.},
journal = {Journal of Pediatric Ophthalmology and Strabismus},
number = {33},
pages = {68--69},
title = {{Neuroimaging in all cases of spasmus nutans}},
volume = {Jan-Feb},
year = {1996}
}
@article{Gramfort2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08596v1},
author = {Gramfort, Alexandre and Cuturi, Marco},
eprint = {arXiv:1503.08596v1},
file = {:home/tom/Work/phd/Papers/Gramfort, Cuturi_2015_Fast Optimal Transport Averaging of Neuroimaging Data.pdf:pdf},
journal = {arXiv preprint},
keywords = {Deconvolution,transport},
mendeley-tags = {Deconvolution,transport},
number = {08596},
title = {{Fast Optimal Transport Averaging of Neuroimaging Data}},
volume = {arXiv:1503},
year = {2015}
}
@book{morgan1989generalization,
address = {Denver, United States},
author = {Morgan, Nelson and Bourlard, Herv{\'{e}}},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {630--637},
publisher = {International Computer Science Institute},
title = {{Generalization and parameter estimation in feedforward nets: Some experiments}},
year = {1990}
}
@inproceedings{Schmidt2011,
abstract = {We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the non-smooth term. We show that both the basic proximal-gradient method and the accelerated proximal-gradient method achieve the same convergence rate as in the error-free case, provided that the errors decrease at appropriate rates.Using these rates, we perform as well as or better than a carefully chosen fixed error level on a set of structured sparsity problems.},
address = {Grenada, Spain},
archivePrefix = {arXiv},
arxivId = {1109.2415},
author = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1109.2415},
file = {:home/tom/Work/phd/Papers/Schmidt, Roux, Bach_2011_Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization.pdf:pdf},
isbn = {9781618395993},
pages = {1458--1466},
title = {{Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization}},
year = {2011}
}
@article{Friedman1937,
author = {Friedman, Milton},
doi = {10.2307/2279372},
file = {:home/tom/Work/phd/Papers/Friedman_1937_The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance.pdf:pdf},
isbn = {01621459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
number = {200},
pages = {675--701},
pmid = {5377},
title = {{The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance}},
url = {http://www.jstor.org/stable/2279372?origin=crossref},
volume = {32},
year = {1937}
}
@article{Giryes2016a,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.08291v5},
author = {Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M and Carolina, North},
eprint = {arXiv:1504.08291v5},
file = {:home/tom/Work/phd/Papers/Giryes et al._2016_Deep Neural Networks with Random Gaussian Weights A Universal Classification Strategy.pdf:pdf},
journal = {IEEE Transaction on Signal Processing},
number = {13},
pages = {3444--3457},
title = {{Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?}},
volume = {64},
year = {2016}
}
@inproceedings{Moreau2015a,
author = {Moreau, Thomas and Oudre, Laurent and Vayatis, Nicolas},
booktitle = {Groupe de Recherche et d'Etudes en Traitement du Signal et des Images (GRETSI)},
file = {:home/tom/Work/phd/Papers/Moreau, Oudre, Vayatis_2015_Groupement automatique pour l ' analyse du spectre singulier.pdf:pdf},
title = {{Groupement automatique pour l ' analyse du spectre singulier}},
year = {2015}
}
@article{Oculo6,
author = {Escudero, M and Vidal, Pierre-Paul},
journal = {European Journal of Neuroscience},
number = {8},
pages = {572--580},
title = {{A quantitative study of electroencephalography, eye movements and neck electromyography characterizing the sleep-wake cycle of the guinea-pig}},
volume = {Mar},
year = {1996}
}
@article{Richtarik2012,
abstract = {In this work we show that randomized (block) coordinate descent methods can be accelerated by parallelization when applied to the problem of minimizing the sum of a partially separable smooth convex function and a simple separable convex function. The theoretical speedup, as compared to the serial method, and referring to the number of iterations needed to approximately solve the problem with high probability, is a simple expression depending on the number of parallel processors and a natural and easily computable measure of separability of the smooth component of the objective function. In the worst case, when no degree of separability is present, there may be no speedup; in the best case, when the problem is separable, the speedup is equal to the number of processors. Our analysis also works in the mode when the number of blocks being updated at each iteration is random, which allows for modeling situations with busy or unreliable processors. We show that our algorithm is able to solve a LASSO problem involving a matrix with 20 billion nonzeros in 2 hours on a large memory node with 24 cores.},
archivePrefix = {arXiv},
arxivId = {1212.0873},
author = {Richt{\'{a}}rik, Peter and Tak{\'{a}}{\v{c}}, Martin},
doi = {10.1007/s10107-015-0901-6},
eprint = {1212.0873},
file = {:home/tom/Work/phd/Papers/Richt{\'{a}}rik, Tak{\'{a}}{\v{c}}_2012_Parallel coordinate descent methods for big data optimization.pdf:pdf},
isbn = {1010701509},
issn = {14364646},
journal = {arXiv preprint},
keywords = {big data optimization,composite ob-,convex optimization,expected separable over-approximation,huge-,iteration complexity,jective,lasso,parallel coordinate descent,partial separability,scale optimization},
number = {0873},
title = {{Parallel coordinate descent methods for big data optimization}},
url = {http://arxiv.org/abs/1212.0873},
volume = {arXiv:1212},
year = {2012}
}
@article{Wiatowski2015,
abstract = {Deep convolutional neural networks have led to breakthrough results in numerous practical machine learning tasks such as classification of images in the ImageNet data set, control-policy-learning to play Atari games or the board game Go, and image captioning. Many of these applications first perform feature extraction and then feed the results thereof into a trainable classifier. The mathematical analysis of deep convolutional neural networks for feature extraction was initiated by Mallat, 2012. Specifically, Mallat considered so-called scattering networks based on a wavelet transform followed by the modulus non-linearity in each network layer, and proved translation invariance (asymptotically in the wavelet scale parameter) and deformation stability of the corresponding feature extractor. This paper complements Mallat's results by developing a theory of deep convolutional neural networks for feature extraction encompassing general convolutional transforms, or in more technical parlance, general semi-discrete frames (including Weyl-Heisenberg, curvelet, shearlet, ridgelet, and wavelet frames), general Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions), and general Lipschitz-continuous pooling operators emulating sub-sampling and averaging. In addition, all of these elements can be different in different network layers. For the resulting feature extractor we prove a translation invariance result which is of vertical nature in the sense of the network depth determining the amount of invariance, and we establish deformation sensitivity bounds that apply to signal classes with inherent deformation insensitivity such as, e.g., band-limited functions.},
archivePrefix = {arXiv},
arxivId = {1512.06293},
author = {Wiatowski, Thomas and B{\"{o}}lcskei, Helmut},
eprint = {1512.06293},
file = {:home/tom/Work/phd/Papers/Wiatowski, B{\"{o}}lcskei_2018_A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
title = {{A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction}},
url = {http://arxiv.org/abs/1512.06293},
volume = {to appear},
year = {2018}
}
@article{Bay2008,
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
file = {:home/tom/Work/phd/Papers/Bay et al._2008_Speeded-Up Robust Features.pdf:pdf},
isbn = {9783540338321},
issn = {10773142},
journal = {Computer Vision and Image Understanding (CVIU)},
keywords = {camera calibration,feature description,interest points,local features,object recognition},
number = {September},
pages = {346--359},
pmid = {16081019},
title = {{Speeded-Up Robust Features}},
url = {papers2://publication/uuid/54DC4FA5-76E1-4FF2-A32D-C2900405827E},
volume = {110},
year = {2008}
}
@phdthesis{agarwal2012computational,
author = {Agarwal, Alekh},
school = {University of California, Berkeley},
title = {{Computational Trade-offs in Statistical Learning}},
year = {2012}
}
@article{Coles2013,
author = {Coles, Lisa D and Patterson, Edward E and Sheffield, W Douglas and Mavoori, Jaideep and Higgins, Jason and Michael, Bland and Leyde, Kent and Cloyd, James C and Litt, Brian and Vite, Charles},
journal = {Epilepsy research},
number = {3},
pages = {456--460},
title = {{Feasibility study of a caregiver seizure alert system in canine epilepsy}},
volume = {106},
year = {2013}
}
@article{Krishnaprasad1983,
author = {Krishnaprasad, P. S.},
journal = {International Journal of Control,},
number = {5},
pages = {1055--1079},
title = {{On families of systems and deformations}},
volume = {38},
year = {1983}
}
@article{Dalalyan2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1700v1},
author = {Dalalyan, Arnak S and Hebiri, Mohamed and Lederer, Johannes},
eprint = {arXiv:1402.1700v1},
file = {:home/tom/Work/phd/Papers/Dalalyan, Hebiri, Lederer_2014_On the prediction performance of the Lasso.pdf:pdf},
journal = {arXiv preprint},
keywords = {62C20,62G05,62G08,62G20,and phrases,multiple linear regres,multiple linear regression,oracle inequalities,sparse recovery,total variation penalty},
number = {1700},
title = {{On the prediction performance of the Lasso}},
volume = {arXiv:1402},
year = {2014}
}
@inproceedings{Lee2001,
author = {Lee, Dd and Seung, Hs},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
number = {1},
pages = {556--562},
title = {{Algorithms for non-negative matrix factorization}},
year = {2001}
}
@article{williamson2000gait,
author = {Williamson, R and Andrews, B},
journal = {IEEE Transactions on Rehabilitation Engineering},
number = {3},
pages = {312--319},
publisher = {IEEE},
title = {{Gait event detection for FES using accelerometers and supervised machine learning}},
volume = {8},
year = {2000}
}
@article{Karimi2014,
archivePrefix = {arXiv},
arxivId = {1401.4220},
author = {Karimi, Sahar and Vavasis, Stephen},
eprint = {1401.4220},
journal = {arXiv preprint},
number = {4220},
title = {{IMRO: a proximal quasi-Newton method for solving {\$}l{\_}1{\$}-regularized least square problem}},
volume = {arXiv:1401},
year = {2014}
}
@article{Olah2017,
annote = {https://distill.pub/2017/feature-visualization},
author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
doi = {10.23915/distill.00007},
journal = {Distill},
title = {{Feature Visualization}},
year = {2017}
}
@techreport{Tseng1988,
author = {Tseng, Paul},
booktitle = {Technical Report LIDS-P-1840, Massachusetts Institute of Technology, Laboratory for Information and Decision Systems},
file = {:home/tom/Work/phd/Papers/Tseng_1988_Coordinate Ascent for Maximizing Nondifferentiable Concave Functions(2).pdf:pdf;:home/tom/Work/phd/Papers/Tseng_1988_Coordinate Ascent for Maximizing Nondifferentiable Concave Functions.pdf:pdf},
institution = {Laboratory for Information and Decision Systems (LIDS), MIT},
keywords = {coordinate ascent,method of multipliers,separable convex programming},
title = {{Coordinate Ascent for Maximizing Nondifferentiable Concave Functions}},
volume = {0171},
year = {1988}
}
@article{Mairal2009,
abstract = {Sparse coding�that is, modelling data vectors as\n\nsparse linear combinations of basis elements�is\n\nwidely used in machine learning, neuroscience,\n\nsignal processing, and statistics. This paper focuses\n\non learning the basis set, also called dictionary,\n\nto adapt it to specific data, an approach\n\nthat has recently proven to be very effective for\n\nsignal reconstruction and classification in the audio\n\nand image processing domains. This paper\n\nproposes a new online optimization algorithm\n\nfor dictionary learning, based on stochastic approximations,\n\nwhich scales up gracefully to large\n\ndatasets with millions of training samples. A\n\nproof of convergence is presented, along with\n\nexperiments with natural images demonstrating\n\nthat it leads to faster performance and better dictionaries\n\nthan classical batch algorithms for both\n\nsmall and large datasets.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {0908.0050},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
doi = {http://doi.acm.org/10.1145/1553374.1553463},
eprint = {0908.0050},
file = {:home/tom/Work/phd/Papers/Mairal et al._2009_Online dictionary learning for sparse coding.pdf:pdf},
isbn = {978-1-60558-516-1},
issn = {0016450X},
journal = {International Conference on Machine Learning (ICML)},
keywords = {Learning,Machine Learning,Optimization and Control},
number = {September},
pages = {689--696},
pmid = {710806},
title = {{Online dictionary learning for sparse coding}},
url = {http://arxiv.org/abs/0908.0050},
year = {2009}
}
@article{Tibshirani2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.2234v2},
author = {Tibshirani, Robert and Bien, Jacob and Friedman, Jerome and Hastie, Trevor and Simon, Noah and Taylor, Jonathan and Tibshirani, Ryan},
doi = {10.2307/41430939},
eprint = {arXiv:1011.2234v2},
file = {:home/tom/Work/phd/Papers/Tibshirani et al._2012_Strong rules for discarding predictors in Lasso- type problems.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (statistical methodology)},
number = {2},
pages = {245--266},
title = {{Strong rules for discarding predictors in Lasso- type problems}},
volume = {74},
year = {2012}
}
@inproceedings{Neyshabur2015,
abstract = {We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1506.02617},
author = {Neyshabur, Behnam and Salakhutdinov, Ruslan and Srebro, Nathan},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1506.02617},
file = {:home/tom/Work/phd/Papers/Neyshabur, Salakhutdinov, Srebro_2015_Path-SGD Path-Normalized Optimization in Deep Neural Networks.pdf:pdf},
issn = {10495258},
pages = {2422--2430},
title = {{Path-SGD: Path-Normalized Optimization in Deep Neural Networks}},
url = {http://arxiv.org/abs/1506.02617},
year = {2015}
}
@article{Cohen2017,
archivePrefix = {arXiv},
arxivId = {1705.02302},
author = {Cohen, Nadav and Sharir, Or and Levine, Yoav and Tamari, Ronen and Yakira, David and Shashua, Amnon},
eprint = {1705.02302},
file = {:home/tom/Work/phd/Papers/Cohen et al._2017_Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions.pdf:pdf},
journal = {arXiv preprint},
keywords = {convolutional networks,expressiveness,hierarchical tensor decompositions},
number = {02302},
title = {{Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions}},
url = {http://arxiv.org/abs/1705.02302},
volume = {arXiv:1705},
year = {2017}
}
@inproceedings{Lee2007,
abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that cap- ture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimiza- tion problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field sur- round suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons. 1},
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03733v1},
author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1.1.69.2112},
eprint = {arXiv:1506.03733v1},
file = {:home/tom/Work/phd/Papers/Lee et al._2007_Efficient Sparse coding algorithms.pdf:pdf},
isbn = {0262195682},
issn = {10495258},
keywords = {Stanford University},
pages = {801--808},
pmid = {17051527},
title = {{Efficient Sparse coding algorithms}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.2112&rep=rep1&type=pdf%5Cnhttp://books.nips.cc/papers/txt/nips19/NIPS2006_0878.txt},
year = {2007}
}
@book{Brockwell2009,
author = {Brockwell, Peter J. and Davis, Richard A.},
doi = {10.1007/978-0-387-98135-2},
file = {:home/tom/Work/phd/Papers/Brockwell, Davis_2009_Time Series Theory and Methods.pdf:pdf},
isbn = {1441903208},
issn = {01727397},
pages = {579},
pmid = {15772297},
title = {{Time Series: Theory and Methods}},
url = {https://books.google.com/books?id=TVIpBgAAQBAJ&pgis=1},
year = {2009}
}
@inproceedings{veltkamp2001shape,
address = {Genova, Italy},
author = {Veltkamp, R C},
booktitle = {International Conference on Shape Modeling and Applications (SMI)},
pages = {188--197},
title = {{Shape matching: Similarity measures and algorithms}},
year = {2001}
}
@inproceedings{Lee2001,
abstract = {Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multi- plicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary func- tion analogous to that used for proving convergence of the Expectation- Maximization algorithm. The algorithms can also be interpreted as diag- onally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence. Introduction},
address = {Vancouver, Canada},
author = {Lee, Dd and Seung, Hs},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1109/IJCNN.2008.4634046},
file = {:home/tom/Work/phd/Papers/Lee, Seung_2001_Algorithms for non-negative matrix factorization.pdf:pdf},
isbn = {9781424418206},
issn = {10987576},
pages = {556--562},
title = {{Algorithms for non-negative matrix factorization}},
url = {http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization},
year = {2001}
}
@inproceedings{Eigen2014,
archivePrefix = {arXiv},
arxivId = {1312.1847},
author = {Eigen, David and Rolfe, Jason and Fergus, Rob and LeCun, Yann},
booktitle = {International Conference on Learning Representation (ICLR)},
eprint = {1312.1847},
file = {:home/tom/Work/phd/Papers/Eigen et al._2014_Understanding Deep Architectures using a Recursive Convolutional Network.pdf:pdf},
pages = {1--8},
title = {{Understanding Deep Architectures using a Recursive Convolutional Network}},
url = {http://arxiv.org/abs/1312.1847},
year = {2014}
}
@inproceedings{Chalasani2013,
address = {Dallas, TX, USA},
annote = {Proximal method with line search and momentum acceleration (FISTA)
They add a predictive sparse decomposition paprt that learn jointly the sparrse code and a fast encoder.

The algorithm is the same as the one in beck_09_Fista.
The results with a variational rate are not so good n the implementation. might have some errors.},
author = {Chalasani, Rakesh and Principe, Jose C. and Ramakrishnan, Naveen},
booktitle = {International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2013.6706854},
file = {:home/tom/Work/phd/Papers/Chalasani, Principe, Ramakrishnan_2013_A fast proximal method for convolutional sparse coding.pdf:pdf},
isbn = {9781467361293},
issn = {2161-4393},
keywords = {Convolution,Feature Extraction,Sparse Coding,Unsupervised Learning},
pages = {1--5},
title = {{A fast proximal method for convolutional sparse coding}},
year = {2013}
}
@inproceedings{Kalchbrenner2014,
abstract = {The ability to accurately represent sen- tences is central to language understand- ing. We describe a convolutional architec- ture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pool- ing, a global pooling operation over lin- ear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily ap- plicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment predic- tion, six-way question classification and Twitter sentiment prediction by distant su- pervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.},
address = {Baltimore, USA},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.2188v1},
author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
booktitle = {Annual meeting of the Association of Computational Linguistics (ACL)},
eprint = {arXiv:1404.2188v1},
file = {:home/tom/Work/phd/Papers/Kalchbrenner, Grefenstette, Blunsom_2014_A Convolutional Neural Network for Modelling Sentences.pdf:pdf},
keywords = {Deconvolution,Neural net,convolution,transport},
mendeley-tags = {Deconvolution,transport},
pages = {212----217},
publisher = {Association of Computational Linguistics (ACL)},
title = {{A Convolutional Neural Network for Modelling Sentences}},
year = {2014}
}
@inproceedings{coates2011importance,
address = {Bellevue, WA, USA},
author = {Coates, Adam and Ng, Andrew Y},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Work/phd/Papers/Coates, Ng_2011_The importance of encoding versus training with sparse coding and vector quantization.pdf:pdf},
pages = {921--928},
title = {{The importance of encoding versus training with sparse coding and vector quantization}},
year = {2011}
}
@article{Friedlander2016,
archivePrefix = {arXiv},
arxivId = {1603.05719},
author = {Friedlander, Michael P and Goh, Gabriel},
eprint = {1603.05719},
journal = {arXiv preprint},
number = {05719},
title = {{Efficient evaluation of scaled proximal operators}},
volume = {arXiv:1603},
year = {2016}
}
@article{Jafari2011,
author = {Jafari, Maria G. and Plumbley, Mark D.},
doi = {10.1109/JSTSP.2011.2157892},
file = {:home/tom/Work/phd/Papers/Jafari, Plumbley_2011_Fast Dictionary Learning for Sparse Representations of Speech Signals.pdf:pdf},
isbn = {1932-4553},
issn = {1932-4553},
journal = {IEEE Journal of Selected Topics in Signal Processing},
keywords = {Adaptive dictionary,dictionary learning,sparse decomposition,sparse dictionary,speech analysis,speech denoising},
number = {5},
pages = {1025--1031},
title = {{Fast Dictionary Learning for Sparse Representations of Speech Signals}},
volume = {5},
year = {2011}
}
@article{Fercoq2013,
archivePrefix = {arXiv},
arxivId = {1312.5799},
author = {Fercoq, Olivier and Richt{\'{a}}rik, Peter},
doi = {10.1137/16M1085905},
eprint = {1312.5799},
file = {:home/tom/Work/phd/Papers/Fercoq, Richt{\'{a}}rik_2015_Accelerated, Parallel and Proximal Coordinate Descent.pdf:pdf},
isbn = {0036-1445},
issn = {0036-1445},
journal = {SIAM Journal on Optimization},
keywords = {10,1137,130949993,49m27,65k05,65y20,68q25,68w10,68w20,90c25,acceleration,ams subject classifications,big data,complexity,convex optimization,doi,parallel methods,partial separability,proximal methods,randomized coordinate descent},
number = {4},
pages = {1997--2023},
title = {{Accelerated, Parallel and Proximal Coordinate Descent}},
url = {http://arxiv.org/abs/1312.5799},
volume = {25},
year = {2015}
}
@techreport{Scherrer2012a,
abstract = {We present a generic framework for par- allel coordinate descent (CD) algorithms that includes, as special cases, the orig- inal sequential algorithms Cyclic CD and Stochastic CD, as well as the recent paral- lel Shotgun algorithm. We introduce two novel parallel algorithms that are also spe- cial cases—Thread-Greedy CD and Coloring- Based CD—and give performance measure- ments for an OpenMP implementation of these. 1.},
author = {Scherrer, Chad and Halappanavar, Mahantesh and Tewari, Ambuj and Haglin, David},
booktitle = {Pacific Northwest National Laboratory (PNNL)},
file = {:home/tom/Work/phd/Papers/Scherrer et al._2012_Scaling Up Coordinate Descent Algorithms for Large $ell_1$ Regularization Problems.pdf:pdf},
institution = {Pacific Northwest National Laboratory (PNNL)},
isbn = {978-1-4503-1285-1},
title = {{Scaling Up Coordinate Descent Algorithms for Large $\ell_1$ Regularization Problems}},
volume = {2},
year = {2012}
}
@article{Deng2015,
archivePrefix = {arXiv},
arxivId = {1208.3922},
author = {Deng, Wei and Yin, Wotao},
doi = {10.1007/s10915-015-0048-x},
eprint = {1208.3922},
file = {:home/tom/Work/phd/Papers/Deng, Yin_2015_On the Global and Linear Convergence of the Generalized Alternating Direction Method of Multipliers.pdf:pdf},
issn = {1573-7691},
journal = {Journal of Scientific Computing},
keywords = {alternating directions of multipliers,dual ascent,error bound,linear convergence,subject classifications},
number = {3},
pages = {889--916},
publisher = {Springer US},
title = {{On the Global and Linear Convergence of the Generalized Alternating Direction Method of Multipliers}},
url = {http://arxiv.org/abs/1208.3922},
volume = {77005},
year = {2015}
}
@inproceedings{Le2013,
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {1112.6209},
author = {Le, Quoc V and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S and Dean, Jeff and Ng, Andrew Y},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/MSP.2011.940881},
eprint = {1112.6209},
file = {:home/tom/Work/phd/Papers/Le et al._2013_Building high-level features using large scale unsupervised learning.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {10535888},
keywords = {deep learning,unsupervised learning},
pages = {8595--8598},
pmid = {20957573},
title = {{Building high-level features using large scale unsupervised learning}},
url = {http://arxiv.org/abs/1112.6209},
year = {2013}
}
@article{Bartlett2003,
author = {Bartlett, Peter L and Maass, Wolfgang},
file = {:home/tom/Work/phd/Papers/Bartlett, Maass_2003_Vapnik-Chervonenkis Dimension of Neural Nets.pdf:pdf},
journal = {The handbook of brain theory and neural networks},
pages = {1188--1192},
title = {{Vapnik-Chervonenkis Dimension of Neural Nets}},
year = {2003}
}
@inproceedings{Richard2012,
abstract = {The paper introduces a penalized matrix estimation procedure aiming at solutions which are sparse and low-rank at the same time. Such structures arise in the context of social networks or protein interactions where underlying graphs have adjacency matrices which are block-diagonal in the appropriate basis. We introduce a convex mixed penalty which involves $\ell_1$-norm and trace norm simultaneously. We obtain an oracle inequality which indicates how the two effects interact according to the nature of the target matrix. We bound generalization error in the link prediction problem. We also develop proximal descent strategies to solve the optimization problem efficiently and evaluate performance on synthetic and real data sets.},
address = {Edinburgh, Great Britain},
archivePrefix = {arXiv},
arxivId = {1206.6474},
author = {Richard, Emile and Paris, Ecole Centrale and Vayatis, Nicolas and Savalle, Pierre-Andre},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1206.6474},
file = {:home/tom/Work/phd/Papers/Richard et al._2012_Estimation of Simultaneously Sparse and Low Rank Matrices.pdf:pdf},
isbn = {978-1-4503-1285-1},
pages = {1351--1358},
title = {{Estimation of Simultaneously Sparse and Low Rank Matrices}},
year = {2012}
}
@article{Zhang2012,
abstract = {Given a sample covariance matrix, we examine the problem of maxi- mizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in ma- chine learning and engineering. Unfortunately, this problem is also combinatorially hard and we discuss convex relaxation techniques that efficiently produce good ap- proximate solutions. We then describe several algorithms solving these relaxations as well as greedy algorithms that iteratively improve the solution quality. Finally, we illustrate sparse PCA in several applications, ranging from senate voting and finance to news data.},
archivePrefix = {arXiv},
arxivId = {1011.3781},
author = {Zhang, Youwei and D'Aspremont, Alexandre and {El Ghaoui}, Laurent},
doi = {10.1007/978-1-4614-0769-0_31},
eprint = {1011.3781},
file = {:home/tom/Work/phd/Papers/Zhang, d'Aspremont, El Ghaoui_2012_Sparse PCA Convex relaxations, algorithms and applications.pdf:pdf},
isbn = {978-1-4614-0769-0},
issn = {08848289},
journal = {International Series in Operations Research and Management Science},
pages = {915--940},
title = {{Sparse PCA: Convex relaxations, algorithms and applications}},
volume = {166},
year = {2012}
}
@article{Paatero1994,
abstract = {A new variant 'PMF' of factor analysis is described. It is assumed that X is a matrix of observed data and sigma is the known matrix of standard deviations of elements of X. Both X and sigma are of dimensions n x m. The method solves the bilinear matrix problem X = GF + E where G is the unknown left hand factor matrix (scores) of dimensions n x p, F is the unknown right hand factor matrix (loadings) of dimensions p x m, and E is the matrix of residuals. The problem is solved in the weighted least squares sense: G and F are determined so that the Frobenius norm of E divided (element-by-element) by sigma is minimized. Furthermore, the solution is constrained so that all the elements of G and F are required to be non-negative. It is shown that the solutions by PMF are usually different from any solutions produced by the customary factor analysis (FA, i.e. principal component analysis (PCA) followed by rotations). Usually PMF produces a better fit to the data than FA. Also, the result of PF is guaranteed to be non-negative, while the result of FA often cannot be rotated so that all negative entries would be eliminated. Different possible application areas of the new method are briefly discussed. In environmental data, the error estimates of data can be widely varying and non-negativity is often an essential feature of the underlying models. Thus it is concluded that PMF is better suited than FA or PCA in many environmental applications. Examples of successful applications of PMF are shown in companion papers.},
author = {Paatero, Pentti and Tapper, Unto},
doi = {10.1002/env.3170050203},
file = {:home/tom/Work/phd/Papers/Paatero, Tapper_1994_Positive matrix factorization A non-negative factor model with optimal utilization of error estimates of data value.pdf:pdf},
isbn = {1180-4009},
issn = {1099095X},
journal = {Environmetrics},
keywords = {Alternating regression,Error estimates,Factor analysis,Principal component analysis,Repetitive measurements,Scaling,Weighted least squares},
number = {2},
pages = {111--126},
pmid = {39},
title = {{Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values}},
volume = {5},
year = {1994}
}
@article{Wang2015a,
abstract = {Despite its nonconvex nature, $\ell_0$ sparse approximation is desirable in many theoretical and application cases. We study the $\ell_0$ sparse approximation problem with the tool of deep learning, by proposing Deep $\ell_0$ Encoders. Two typical forms, the $\ell_0$ regularized problem and the $M$-sparse problem, are investigated. Based on solid iterative algorithms, we model them as feed-forward neural networks, through introducing novel neurons and pooling functions. Enforcing such structural priors acts as an effective network regularization. The deep encoders also enjoy faster inference, larger learning capacity, and better scalability compared to conventional sparse coding solutions. Furthermore, under task-driven losses, the models can be conveniently optimized from end to end. Numerical results demonstrate the impressive performances of the proposed encoders.},
archivePrefix = {arXiv},
arxivId = {1509.00153},
author = {Wang, Zhangyang and Ling, Qing and Huang, Thomas S.},
eprint = {1509.00153},
file = {:home/tom/Work/phd/Papers/Wang, Ling, Huang_2015_Learning Deep $ell_0$ Encoders.pdf:pdf},
journal = {arXiv preprint},
number = {00153},
title = {{Learning Deep $\ell_0$ Encoders}},
url = {http://arxiv.org/abs/1509.00153},
volume = {arXiv:1509},
year = {2015}
}
@article{Gribonval2015a,
archivePrefix = {arXiv},
arxivId = {1407.5155},
author = {Gribonval, Remi and Jenatton, Rodolphe and Bach, Francis},
doi = {10.1109/TIT.2015.2472522},
eprint = {1407.5155},
file = {:home/tom/Work/phd/Papers/Gribonval, Jenatton, Bach_2015_Sparse and spurious dictionary learning with noise and outliers.pdf:pdf},
isbn = {2011277906},
journal = {IEEE Transactions on Information Theory},
number = {11},
pages = {6298--6319},
title = {{Sparse and spurious: dictionary learning with noise and outliers}},
url = {http://arxiv.org/abs/1407.5155},
volume = {61},
year = {2015}
}
@article{Lee2014,
archivePrefix = {arXiv},
arxivId = {1206.1623v1},
author = {Lee, Jason D and Sun, Yeukai and Saunders, Michael A},
eprint = {1206.1623v1},
file = {:home/tom/Work/phd/Papers/Lee, Sun, Saunders_2014_Proximal Newton-type Methods for Minimizing Convex Objective Functions in Composite Form.pdf:pdf},
journal = {SIAM Journal on Optimization},
keywords = {1,65k05,90c25,90c53,a composite function,ams subject classifications,and statistical learning can,be formulated as minimizing,convex optimization,in bioinformatics,ing,introduction,many problems of relevance,nonsmooth optimization,proximal mapping,signal process-},
number = {3},
pages = {1420----1443},
title = {{Proximal Newton-type Methods for Minimizing Convex Objective Functions in Composite Form}},
volume = {24},
year = {2014}
}
@article{Neyshabur2017,
abstract = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
archivePrefix = {arXiv},
arxivId = {1706.08947},
author = {Kawaguchi, Kenji and {Pack Kaelbling}, Leslie and Bengio, Yoshua},
eprint = {1706.08947},
file = {:home/tom/Work/phd/Papers/Kawaguchi, Pack Kaelbling, Bengio_2017_Generalization in Deep Learning.pdf:pdf},
journal = {preprintq},
number = {05468},
title = {{Generalization in Deep Learning}},
url = {http://arxiv.org/abs/1706.08947},
volume = {arXiv:1710},
year = {2017}
}
@article{Friedman2007,
abstract = {We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the $L_1$-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the ``fused lasso,'' however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems.},
archivePrefix = {arXiv},
arxivId = {0708.1485},
author = {Friedman, Jerome and Hastie, Trevor and H{\"{o}}fling, Holger and Tibshirani, Robert},
doi = {10.1214/07-AOAS131},
eprint = {0708.1485},
file = {:home/tom/Work/phd/Papers/Friedman et al._2007_Pathwise coordinate optimization.pdf:pdf},
issn = {1932-6157},
journal = {The Annals of Applied Statistics},
keywords = {convex optimization,coordinate descent,lasso},
number = {2},
pages = {302--332},
pmid = {261057600003},
title = {{Pathwise coordinate optimization}},
url = {http://arxiv.org/abs/0708.1485},
volume = {1},
year = {2007}
}
@article{VanHorn2008,
author = {{Van Horn}, Marion R. and Cullen, Kathleen E.},
journal = {Journal of Neurophysiology},
keywords = {Oculo},
mendeley-tags = {Oculo},
number = {4},
pages = {1967----1982},
title = {{Dynamic Coding of Vertical Facilitated Vergence by Premotor Saccadic Brust Neurons}},
volume = {100},
year = {2008}
}
@article{Hiriart-Urruty1991,
author = {Hiriart-Urruty, J. B.},
doi = {10.1016/0022-247X(91)90187-5},
file = {:home/tom/Work/phd/Papers/Hiriart-Urruty_1991_How to regularize a difference of convex functions.pdf:pdf},
issn = {10960813},
journal = {Journal of Mathematical Analysis and Applications},
number = {1},
pages = {196--209},
title = {{How to regularize a difference of convex functions}},
volume = {162},
year = {1991}
}
@article{tsanas2010accurate,
author = {Tsanas, Athanasios and Little, Max A and McSharry, Patrick E and Ramig, Lorraine O},
journal = {IEEE Transactions on Biomedical Engineering},
number = {4},
pages = {884--893},
publisher = {IEEE},
title = {{Accurate telemonitoring of Parkinson's disease progression by noninvasive speech tests}},
volume = {57},
year = {2010}
}
@inproceedings{yu2009deep,
address = {Vancouver, Canada},
author = {Yu, Kai and Xu, Wei and Gong, Yihong},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1889--1896},
title = {{Deep learning with kernel regularization for visual recognition}},
year = {2009}
}
@article{Oculo7,
author = {Farmer, J and Hoyt, CS},
journal = {American Journal of Ophthalmology},
number = {98},
pages = {504--509},
title = {{Monocular nystagmus in infancy and early childhood}},
volume = {Oct},
year = {1984}
}
@inproceedings{Sermanet2014,
abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
booktitle = {International Conference on Learning Representation (ICLR)},
doi = {10.1109/CVPR.2015.7299176},
eprint = {1312.6229},
file = {:home/tom/Work/phd/Papers/Sermanet et al._2014_OverFeat Integrated Recognition, Localization and Detection using Convolutional Networks.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
pmid = {1000200972},
title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2014}
}
@article{Oculo28,
author = {Schwartz, MA and Selhorst, JB and Ochs, AL and Beck, RW and Campbell, WW and HArris, JK and Et, Al.},
journal = {Annals of Neurology},
number = {20},
pages = {677--83},
title = {{Oculomasticatory myorhythmia: a unique movement disorder occurring in Whipple's disease}},
volume = {Dec},
year = {1986}
}
@article{Golyandina2014,
archivePrefix = {arXiv},
arxivId = {1206.6910},
author = {Golyandina, Nina and Korobeynikov, Anton},
doi = {10.1016/j.csda.2013.04.009},
eprint = {1206.6910},
file = {:home/tom/Work/phd/Papers/Golyandina, Korobeynikov_2014_Basic Singular Spectrum Analysis and Forecasting with R.pdf:pdf},
journal = {Computational Statistics \& Data Analysis},
keywords = {2000 msc,62m10,62m20,65c60,forecasting,frequency estimation,r package,singular spectrum analysis,time series,time series analysis},
pages = {934----954},
title = {{Basic Singular Spectrum Analysis and Forecasting with R}},
url = {http://arxiv.org/abs/1206.6910%0Ahttp://dx.doi.org/10.1016/j.csda.2013.04.009},
volume = {71},
year = {2014}
}
@article{Frossard2011,
author = {Frossard, Pascal and Tosic, Ivana},
file = {:home/tom/Work/phd/Papers/Frossard, Tosic_2011_Dictionary Learning.pdf:pdf},
journal = {IEEE Signal Processing Magazine},
keywords = {dictionary learning,sparse coding},
number = {2},
pages = {27----38},
title = {{Dictionary Learning}},
volume = {28},
year = {2011}
}
@article{Liu2017a,
archivePrefix = {arXiv},
arxivId = {1706.09563},
author = {Liu, Jialin and Garcia-Cardona, Cristina and Wohlberg, Brendt and Yin, Wotao},
eprint = {1706.09563},
file = {:home/tom/Work/phd/Papers/Liu et al._2017_Online Convolutional Dictionary Learning(2).pdf:pdf},
journal = {arXiv preprint},
keywords = {convolutional dictionary learning,convolutional sparse coding,descent,recursive least squares,stochastic gradient},
number = {00106},
title = {{Online Convolutional Dictionary Learning}},
url = {http://arxiv.org/abs/1706.09563},
volume = {arXiv:1709},
year = {2017}
}
@article{Oculo11,
author = {Good, WV and Koch, TS and Jan, JE},
journal = {Developmental Medicine and Child Neurology},
number = {35},
pages = {1106--1110},
title = {{Monocular nystagmus caused by unilateral anterior visual- pathway disease}},
volume = {Dec},
year = {1993}
}
@inproceedings{oner2012towards,
address = {San Diego, CA, USA},
author = {Oner, M and Pulcifer-Stump, J and Seeling, P and Kaya, T},
booktitle = {International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
pages = {1980--1983},
title = {{Towards the run and walk activity classification through Step detection-An Android application}},
year = {2012}
}
@article{Haeffele2015,
archivePrefix = {arXiv},
arxivId = {1506.07540},
author = {Haeffele, Benjamin D. and Vidal, Rene},
doi = {10.1088/0953-8984/28/3/035305},
eprint = {1506.07540},
file = {:home/tom/Work/phd/Papers/Haeffele, Vidal_2015_Global Optimality in Tensor Factorization, Deep Learning, and Beyond.pdf:pdf},
isbn = {1506.07540},
issn = {1361648X},
journal = {arXiv preprint},
number = {07540},
pmid = {473150},
title = {{Global Optimality in Tensor Factorization, Deep Learning, and Beyond}},
url = {http://arxiv.org/abs/1506.07540},
volume = {arXiv:1506},
year = {2015}
}
@article{Cho2015,
archivePrefix = {arXiv},
arxivId = {1507.01053},
author = {Cho, Kyunghyun and Courville, Aaron and Bengio, Yoshua},
doi = {10.1109/TMM.2015.2477044},
eprint = {1507.01053},
file = {:home/tom/Work/phd/Papers/Cho, Courville, Bengio_2015_Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks.pdf:pdf},
isbn = {978-1-60558-907-7},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Attention mechanism,deep learning,recurrent neural networks},
number = {11},
pages = {1875--1886},
title = {{Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks}},
volume = {17},
year = {2015}
}
@inproceedings{He2016,
address = {Las Vegas, NV, USA},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.03385v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {arXiv:1512.03385v1},
file = {:home/tom/Work/phd/Papers/He et al._2016_Deep Residual Learning for Image Recognition.pdf:pdf},
pages = {770--778},
title = {{Deep Residual Learning for Image Recognition}},
year = {2016}
}
@inproceedings{han2006gait,
address = {Ioannina, Greece},
author = {Han, J and Jeon, H S and Jeon, B S and Park, K S},
booktitle = {International Special Topic Conference on Information Technology in Biomedicine (ITAB)},
title = {{Gait detection from three dimensional acceleration signals of ankles for the patients with Parkinson's disease}},
year = {2006}
}
@book{VanHandel2014,
abstract = {These lecture notes were written for the course ORF 570: Probability in High Dimension that I taught at Princeton in the Spring 2014 semester. The aim was to introduce in as cohesive a manner as I could manage a set of methods, many of which have their origin in probability in Banach spaces, that arise across a broad range of contemporary problems in di↵erent areas. The notes are necessarily incomplete. The ambitious syllabus for the course was laughably beyond the scope of Princeton's 12-week semester. As a result, there are regrettable omissions, as well as many fascinating topics that I would have liked to but could not cover in the context of this course. These include: a. Bernstein's inequality does not appear anywhere in these notes (disgrace-ful!), nor do any Bernstein-type concentration inequalities (such as con-centration of the exponential distribution and Talagrand's concentration inequality for empirical processes) and the notion of modified log-Sobolev inequalities. These should be included at the end of Part I. b. Chaining with adaptive truncation and entropy with brackets. Beyond be-ing a classical topic in empirical process theory, the power of the idea of adaptive truncation has again proven its value in the recent solution of the long-standing Bernoulli problem due to Bednorz and Lata la.},
author = {{Van Handel}, Ramon},
booktitle = {Princeton University},
file = {:home/tom/Work/phd/Papers/Van Handel_2014_Probability in High Dimension.pdf:pdf},
title = {{Probability in High Dimension}},
url = {http://www.princeton.edu/$\sim$rvan/ORF570.pdf},
year = {2014}
}
@article{Tibshirani1996,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tibshirani, Robert},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Work/phd/Papers/Tibshirani_1996_Regression Shrinkage and Selection via the Lasso.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of the Royal Statistical Society: Series B (statistical methodology)},
keywords = {icle},
number = {1},
pages = {267----288},
pmid = {25246403},
title = {{Regression Shrinkage and Selection via the Lasso}},
volume = {58},
year = {1996}
}
@inproceedings{Johnson2015,
abstract = {By reducing optimization to a sequence of small subproblems, working set methods achieve fast convergence times for many challenging problems. Despite excellent performance, theoretical understanding of working sets is limited, and implementations often resort to heuristics to determine subproblem size, makeup, and stopping criteria. We propose Blitz, a fast working set algorithm accompanied by useful guarantees. Making no assumptions on data, our theory relates subproblem size to progress toward convergence. This result motivates methods for optimizing algorithmic parameters and discarding irrelevant variables as iterations progress. Applied to L1-regularized learning, Blitz convincingly outperforms existing solvers in sequential, limited-memory, and distributed settings. Blitz is not specific to L1-regularized learning, making the algorithm relevant to many applications involving sparsity or constraints.},
address = {Lille, France},
author = {Johnson, Tyler and Guestrin, Carlos},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Work/phd/Papers/Johnson, Guestrin_2015_Blitz A Principled Meta-Algorithm for Scaling Sparse Optimization.pdf:pdf},
isbn = {9781510810587},
pages = {1171--1179},
title = {{Blitz: A Principled Meta-Algorithm for Scaling Sparse Optimization}},
url = {http://proceedings.mlr.press/v37/johnson15.html},
year = {2015}
}
@book{Hastie2015,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin J.},
file = {:home/tom/Work/phd/Papers/Hastie, Tibshirani, Wainwright_2015_Statistical Learning with Sparsity.pdf:pdf},
isbn = {9781498712170},
pages = {1--351},
publisher = {CRC Press},
title = {{Statistical Learning with Sparsity}},
year = {2015}
}
@inproceedings{Mairal2008,
address = {Anchorage, AK, USA},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo and Zisserman, Andrew},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Work/phd/Papers/Mairal et al._2008_Discriminative Learned Dictionaries for Local Image Analysis.pdf:pdf},
pages = {1--8},
title = {{Discriminative Learned Dictionaries for Local Image Analysis}},
year = {2008}
}
@article{Oculo34,
author = {Yang, S and Jeong, J and Kim, JG and Yoon, YH},
journal = {Ophthalmic surgery Lasers Imaging},
number = {37},
pages = {230--3},
title = {{Progressive venous stasis retinopathy and open-angle glaucoma associated with primary pulmonary hypertension}},
volume = {May-Jun},
year = {2006}
}
@article{Bottou2008,
author = {Bottou, L{\'{e}}on and Bousquet, Olivier},
file = {:home/tom/Work/phd/Papers/Bottou, Bousquet_2008_Learning using large datasets.pdf:pdf},
journal = {Mining Massive DataSets for Security},
keywords = {large-scale learning,optimization,statistics},
title = {{Learning using large datasets}},
volume = {3},
year = {2008}
}
@article{Zou2006,
abstract = {Principal component analysis (PCA) is widely used in data processing and dimension- ality reduction.However,PCAsuffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results.We introduce a newmethod called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings.We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results. Key},
archivePrefix = {arXiv},
arxivId = {1205.0121v2},
author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1198/106186006X113430},
eprint = {1205.0121v2},
file = {:home/tom/Work/phd/Papers/Zou, Hastie, Tibshirani_2006_Sparse Principal Component Analysis.pdf:pdf},
isbn = {106186006X},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {arrays,elastic net,gene expression,lasso,multivariate analysis,singular,thresholding,value decomposition},
number = {2},
pages = {265--286},
pmid = {21811560},
title = {{Sparse Principal Component Analysis}},
volume = {15},
year = {2006}
}
@inproceedings{Pati1993,
abstract = {We describe a recursive algorithm to compute representations of functions with respect to nonorthogonal and possibly overcomplete dictionaries of elementary building blocks e.g. affine (wavelet) frames. We propose a modification to the matching pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual (error) at every step and thereby leads to improved convergence. We refer to this modified algorithm as orthogonal matching pursuit (OMP). It is shown that all additional computation required for the OMP algorithm may be performed recursively},
address = {Pacific Grove, CA, USA},
author = {Pati, Y.C. and Rezaiifar, R. and Krishnaprasad, P.S.},
booktitle = {Asilomar Conference on Signals, Systems and Computers},
doi = {10.1109/ACSSC.1993.342465},
file = {:home/tom/Work/phd/Papers/Pati, Rezaiifar, Krishnaprasad_1993_Orthogonal matching pursuit recursive function approximation with applications to wavelet decomposit.pdf:pdf},
isbn = {0-8186-4120-7},
issn = {1058-6393},
pages = {40--44},
title = {{Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition}},
url = {http://ieeexplore.ieee.org/document/342465/},
year = {1993}
}
@incollection{sohn2006mobility,
author = {Sohn, T and Varshavsky, A and LaMarca, A and Chen, M and Choudhury, T and Smith, I and Consolvo, S and Hightower, J and Griswold, W and {De Lara}, E},
booktitle = {UbiComp 2006: Ubiquitous Computing},
pages = {212--224},
publisher = {Springer},
title = {{Mobility detection using everyday gsm traces}},
year = {2006}
}
@inproceedings{Zhou2014,
address = {Colombus, OH, USA},
author = {Zhou, Yin and Chang, Hang and Barner, Kenneth and Spellman, Paul and Parvin, Bahram and Division, Life Sciences and Berkeley, Lawrence},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Work/phd/Papers/Zhou et al._2014_Classification of Histology Sections via Multispectral Convolutional Sparse Coding.pdf:pdf},
keywords = {Convolution,dictionary learning,sparse coding},
pages = {3081----3088},
title = {{Classification of Histology Sections via Multispectral Convolutional Sparse Coding}},
year = {2014}
}
@techreport{July2013,
author = {Wotao, Yin},
file = {:home/tom/Work/phd/Papers/Wotao_2013_Sparse Optimization Lecture Parallel and Distributed Sparse Optimization.pdf:pdf},
number = {July},
title = {{Sparse Optimization Lecture : Parallel and Distributed Sparse Optimization}},
year = {2013}
}
@article{Bristow2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2407v1},
author = {Bristow, Hilton and Lucey, Simon},
eprint = {arXiv:1406.2407v1},
file = {:home/tom/Work/phd/Papers/Bristow, Lucey_2014_Optimization Methods for Convolutional Sparse Coding.pdf:pdf},
journal = {arXiv preprint},
keywords = {admm,convolution,fista,fourier,l 1,learning,machine,sisc,sparse coding},
number = {2407},
title = {{Optimization Methods for Convolutional Sparse Coding}},
volume = {arXiv:1406},
year = {2014}
}
@article{Wohlberg2016,
author = {Wohlberg, Brendt},
doi = {10.1109/TIP.2015.2495260},
file = {:home/tom/Work/phd/Papers/Wohlberg_2016_Efficient Algorithms for Convolutional Sparse Representations.pdf:pdf},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
keywords = {ADMM,Index Terms— Sparse representation,convolutional sparse representation,dictionary learning,sparse coding},
number = {1},
title = {{Efficient Algorithms for Convolutional Sparse Representations}},
volume = {25},
year = {2016}
}
@inproceedings{Lee2016,
abstract = {We show that gradient descent converges to a local minimizer, almost surely with random initialization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.},
address = {New-York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1602.04915},
author = {Lee, Jason D. and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
booktitle = {Conference on Learning Theory (COLT)},
eprint = {1602.04915},
file = {:home/tom/Work/phd/Papers/Lee et al._2016_Gradient Descent Converges to Minimizers.pdf:pdf},
keywords = {dynamical systems,gradient descent,local minimum,saddle points,smooth optimization},
pages = {1246--1257},
title = {{Gradient Descent Converges to Minimizers}},
url = {http://arxiv.org/abs/1602.04915},
year = {2016}
}
@article{Barron1993,
author = {Barron, Andrew},
file = {:home/tom/Work/phd/Papers/Barron_1993_Universal Approximation Bounds for Superpositions of a Sigmoidal Function.pdf:pdf},
journal = {IEEE Transaction on Information Theory},
number = {3},
pages = {930--945},
title = {{Universal Approximation Bounds for Superpositions of a Sigmoidal Function}},
volume = {39},
year = {1993}
}
@article{Bruna2012,
archivePrefix = {arXiv},
arxivId = {1203.1513},
author = {Bruna, Joan and Mallat, St\'ephane},
doi = {E618F3E2-0F49-4C93-89B8-CDCE0EADE5D9},
eprint = {1203.1513},
file = {:home/tom/Work/phd/Papers/Bruna, Mallat_2013_Invariant Scattering Networks.pdf:pdf},
isbn = {0162-8828},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {1872--1886},
pmid = {23070037},
title = {{Invariant Scattering Networks}},
url = {http://people.ee.duke.edu/$\sim$lcarin/Bo8.6.2012.pdf},
volume = {35},
year = {2013}
}
@article{Chen1994,
author = {Chen, Gong and Teboulle, Marc},
doi = {10.1007/BF01582566},
file = {:home/tom/Work/phd/Papers/Chen, Teboulle_1994_A proximal-based decomposition method for convex minimization problems.pdf:pdf},
issn = {00255610},
journal = {Mathematical Programming},
keywords = {AMS Subject Classification: 90C25,Augmented Lagrangian,Convex programming,Decomposition-splitting methods,Proximal methods},
number = {1-3},
pages = {81--101},
title = {{A proximal-based decomposition method for convex minimization problems}},
volume = {64},
year = {1994}
}
@inproceedings{Becker2012,
address = {South Lake Tahoe, United States},
archivePrefix = {arXiv},
arxivId = {1206.1156},
author = {Becker, Stephen and {Jalal Fadili}, M.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1206.1156},
file = {:home/tom/Work/phd/Papers/Becker, Jalal Fadili_2012_A quasi-Newton proximal splitting method.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
pages = {2618--2626},
title = {{A quasi-Newton proximal splitting method}},
url = {http://arxiv.org/abs/1206.1156},
year = {2012}
}
@inproceedings{Keskar2017,
abstract = {This book is aimed to provide an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria: 1) expertise or knowledge of the authors; 2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and 3) the application areas that have the potential to be impacted significantly by deep learning and that have gained concentrated research efforts, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning. In Chapter 1, we provide the background of deep learning, as intrinsically connected to the use of multiple layers of nonlinear transformations to derive features from the sensory signals such as speech and visual images. In the most recent literature, deep learning is embodied also as representation learning, which involves a hierarchy of features or concepts where higher-level representations of them are defined from lower-level ones and where the same lower-level representations help to define higher-level ones. In Chapter 2, a brief historical account of deep learning is presented. In particular, selected chronological development of speech recognition is used to illustrate the recent impact of deep learning that has become a dominant technology in speech recognition industry within only a few years since the start of a collaboration between academic and industrial researchers in applying deep learning to speech recognition. In Chapter 3, a three-way classification scheme for a large body of work in deep learning is developed. We classify a growing number of deep learning techniques into unsupervised, supervised, and hybrid categories, and present qualitative descriptions and a literature survey for each category. From Chapter 4 to Chapter 6, we discuss in detail three popular deep networks and related learning methods, one in each category. Chapter 4 is devoted to deep autoencoders as a prominent example of the unsupervised deep learning techniques. Chapter 5 gives a major example in the hybrid deep network category, which is the discriminative feed-forward neural network for supervised learning with many layers initialized using layer-by-layer generative, unsupervised pre-training. In Chapter 6, deep stacking networks and several of the variants are discussed in detail, which exemplify the discriminative or supervised deep learning techniques in the three-way categorization scheme. In Chapters 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing and of applied artificial intelligence. In Chapter 7, we review the applications of deep learning to speech and audio processing, with emphasis on speech recognition organized according to several prominent themes. In Chapters 8, we present recent results of applying deep learning to language modeling and natural language processing. Chapter 9 is devoted to selected applications of deep learning to information retrieval including Web search. In Chapter 10, we cover selected applications of deep learning to image object recognition in computer vision. Selected applications of deep learning to multi-modal processing and multi-task learning are reviewed in Chapter 11. Finally, an epilogue is given in Chapter 12 to summarize what we presented in earlier chapters and to discuss future challenges and directions.},
address = {Toulon, France},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.7522v4},
author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
booktitle = {International Conference on Learning Representation (ICLR)},
doi = {10.1227/01.NEU.0000255452.20602.C9},
eprint = {arXiv:1412.7522v4},
file = {:home/tom/Work/phd/Papers/Keskar et al._2017_On Large-Batch Training for Deep Learning Generalization Gap and Sharp Minima.pdf:pdf},
isbn = {9781405161251},
issn = {1607-551X},
pmid = {17460516},
title = {{On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}},
year = {2017}
}
@article{Tao2015,
abstract = {We establish local linear convergence bounds for the ISTA and FISTA iterations on the model LASSO problem. We show that FISTA can be viewed as an accelerated ISTA process. Using a spectral analysis, we show that, when close enough to the solution, both iterations converge linearly, but FISTA slows down compared to ISTA, making it advantageous to switch to ISTA toward the end of the iteration processs. We illustrate the results with some synthetic numerical examples.},
archivePrefix = {arXiv},
arxivId = {1501.02888},
author = {Tao, Shaozhe and Boley, Daniel and Zhang, Shuzhong},
doi = {10.1137/151004549},
eprint = {1501.02888},
file = {:home/tom/Work/phd/Papers/Tao, Boley, Zhang_2015_Local Linear Convergence of ISTA and FISTA on the LASSO Problem.pdf:pdf},
issn = {10526234},
journal = {arXiv preprint},
number = {02888},
title = {{Local Linear Convergence of ISTA and FISTA on the LASSO Problem}},
url = {http://arxiv.org/abs/1501.02888},
volume = {arXiv:1501},
year = {2015}
}
@article{Janzamin2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.08473v3},
author = {Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
eprint = {arXiv:1506.08473v3},
file = {:home/tom/Work/phd/Papers/Janzamin, Sedghi, Anandkumar_2015_Beating the Perils of Non-Convexity Guaranteed Training of Neural Networks using Tensor Methods.pdf:pdf},
journal = {arXiv preprint},
keywords = {method-of-moments,neural networks,risk bound,tensor decomposition},
number = {08473},
title = {{Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods}},
volume = {arXiv:1506},
year = {2015}
}
@article{Oculo2,
author = {Averbuch-Heller, L and Zivotofsky, AZ and Remler, BF and Das, VE and Dell'Osso, Louis F and Leigh, RJ},
journal = {Neurology},
number = {45},
pages = {509--515},
title = {{Convergent- divergent pendular nystagmus: possible role of the vergence system}},
volume = {Mar},
year = {1995}
}
@phdthesis{mariani,
author = {Mariani, B},
school = {EPFL},
title = {{Assessment of Foot Signature Using Wearable Sensors for Clinical Gait Analysis and Real-Time Activity Recognition}},
year = {2012}
}
@article{Berthet2013a,
archivePrefix = {arXiv},
arxivId = {arXiv:1202.5070v3},
author = {Berthet, Quentin and Rigollet, Philippe},
doi = {10.1214/13-AOS1127},
eprint = {arXiv:1202.5070v3},
file = {:home/tom/Work/phd/Papers/Berthet, Rigollet_2013_Optimal detection of sparse principal components in high dimension.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {High-dimensional detection,Minimax lower bounds,Planted clique,Semidefinite relaxation,Sparse principal component analysis,Spiked covariance model},
number = {4},
pages = {1780--1815},
title = {{Optimal detection of sparse principal components in high dimension}},
volume = {41},
year = {2013}
}
@article{Bubeck2015,
archivePrefix = {arXiv},
arxivId = {1405.4980},
author = {Bubeck, S{\'{e}}bastien},
doi = {10.1561/2200000050},
eprint = {1405.4980},
file = {:home/tom/Work/phd/Papers/Bubeck_2015_Convex Optimization Algorithms and Complexity.pdf:pdf},
isbn = {2200000049},
issn = {1935-8237},
journal = {Foundations and Trends in Machine Learning},
number = {3-4},
pages = {231--357},
title = {{Convex Optimization: Algorithms and Complexity}},
url = {http://www.nowpublishers.com/article/Details/MAL-050},
volume = {8},
year = {2015}
}
@article{Karimi2014,
archivePrefix = {arXiv},
arxivId = {1401.4220},
author = {Karimi, Sahar and Vavasis, Stephen},
eprint = {1401.4220},
file = {:home/tom/Work/phd/Papers/Karimi, Vavasis_2014_IMRO a proximal quasi-Newton method for solving $l_1$-regularized least square problem.pdf:pdf},
journal = {arXiv preprint},
keywords = {basis pursuit de-,convex optimization,l 1 -regularized least,minimization of com-,noising problem,proximal methods,quasi-newton methods,sparse recovery,square problem},
number = {4220},
title = {{IMRO: a proximal quasi-Newton method for solving $l_1$-regularized least square problem}},
url = {http://arxiv.org/abs/1401.4220},
volume = {arXiv:1401},
year = {2014}
}
@article{Bobin2008,
author = {Bobin, J{\'{e}}r{\^{o}}me and Moudden, Y and Fadili, J and Starck, J L},
doi = {10.1007/s10851-008-0065-6},
file = {:home/tom/Work/phd/Papers/Bobin et al._2008_Morphological Diversity and Sparsity for Multichannel Data Restoration.pdf:pdf},
issn = {0924-9907},
journal = {Journal of Mathematical Imaging and Vision},
keywords = {multichannel data,overcomplete representations,restoration,sparsity},
number = {2},
pages = {149--168},
title = {{Morphological Diversity and Sparsity for Multichannel Data Restoration}},
url = {http://www.springerlink.com/index/10.1007/s10851-008-0065-6},
volume = {33},
year = {2008}
}
@article{Oculo22,
author = {McIlwaine, GG and Carrim, ZI and Lueck, CJ and Chrisp, TM},
journal = {Journal of Neuro-Ophthalmology},
number = {25},
pages = {40--43},
title = {{A mechanical theory to account for bitemporal hemianopia from chiasmal compression}},
volume = {Mar},
year = {2005}
}
@inproceedings{You2016a,
abstract = {In this paper, we propose and study an Asynchronous parallel Greedy Coordinate Descent (Asy-GCD) algorithm for minimizing a smooth function with bounded constraints. At each iteration, workers asynchronously conduct greedy coordinate descent updates on a block of variables. In the first part of the paper, we analyze the theoretical behavior of Asy-GCD and prove a linear convergence rate. In the second part, we develop an efficient kernel SVM solver based on Asy-GCD in the shared memory multi-core setting. Since our algorithm is fully asynchronous—each core does not need to idle and wait for the other cores—the resulting algorithm enjoys good speedup and outperforms existing multi-core kernel SVM solvers including asynchronous stochastic coordinate descent and multi-core LIBSVM.},
address = {Barcelona, Spain},
author = {You, Yang and Lian, Xiangru and Liu, Ji and Yu, Hsiang-Fu and Dhillon, Inderjit S. and Demmel, James and Hsieh, Cho-Jui},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/You et al._2016_Asynchronous Parallel Greedy Coordinate Descent.pdf:pdf},
issn = {10495258},
pages = {4682--4690},
title = {{Asynchronous Parallel Greedy Coordinate Descent}},
year = {2016}
}
@article{weinberg2002using,
author = {Weinberg, H},
journal = {Analog Devices AN-602 application note},
title = {{Using the ADXL202 in pedometer and personal navigation applications}},
year = {2002}
}
@article{Bruckstein2009,
author = {Bruckstein, Alfred M and Donoho, David L and Elad, Michael},
file = {:home/tom/Work/phd/Papers/Bruckstein, Donoho, Elad_2009_From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images.pdf:pdf},
journal = {Society for Industrial and Applied Mathematics},
keywords = {Sparse-Land,basis pursuit,compression,denoising,dictionary learning,inverse problems,linear system of equations,matching pursuit,mutual coherence,overcomplete,redundant,sparse coding,sparse represen- tation,underdetermined},
number = {1},
pages = {34--81},
title = {{From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images}},
volume = {51},
year = {2009}
}
@article{Srivastava2014,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:home/tom/Work/phd/Papers/Srivastava et al._2014_Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@inproceedings{Zhou2009,
abstract = {Alignment of time series is an important problem to solve in many scientific dis- ciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale differ- ence between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical cor- relation analysis (CCA) for spatio-temporal alignment of human motion between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing local spatial deformations. We show CTW's effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects per- forming similar actions, and alignment of similar facial expressions made by two people. Our results demonstrate thatCTWprovides both visually and qualitatively better alignment than state-of-the-art techniques based on DTW.},
address = {Vancouver, Canada},
author = {Zhou, Feng and de la Torre, Fernando},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1103/PhysRevB.72.205311},
file = {:home/tom/Work/phd/Papers/Zhou, de la Torre_2009_Canonical time warping for alignment of human behavior.pdf:pdf},
isbn = {9781615679119},
issn = {10980121},
keywords = {Canonical Correlation Analysis,Dynamic Time Warping},
pages = {2286--2294},
title = {{Canonical time warping for alignment of human behavior}},
url = {http://f-zhou.com/pdf/2009_nips_ctw.pdf},
year = {2009}
}
@article{Agarwal2014,
archivePrefix = {arXiv},
arxivId = {1310.7991},
author = {Agarwal, Alekh and Anandkumar, Animashree and Jain, Prateek},
eprint = {1310.7991},
file = {:home/tom/Work/phd/Papers/Agarwal, Anandkumar, Jain_2014_Learning sparsely used overcomplete dictionaries via alternating minimization.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {alternating minimization,dictionary learning,incoherence,lasso,sparse coding},
pages = {1--15},
title = {{Learning sparsely used overcomplete dictionaries via alternating minimization}},
url = {http://arxiv.org/abs/1310.7991},
volume = {35},
year = {2014}
}
@article{Frasconi1997,
archivePrefix = {arXiv},
arxivId = {arXiv:hep-lat/0007035},
author = {Frasconi, Paolo and Gori, Marco and Tesi, Alberto},
doi = {10.1002/jssc.200600256},
eprint = {0007035},
file = {:home/tom/Work/phd/Papers/Frasconi, Gori, Tesi_1993_Backpropagation for Lineraly-Separable Patterns a Detailed Analysis.pdf:pdf},
isbn = {9780203209493},
issn = {0065-230X},
journal = {Progress in Neural Networks: Architecture},
keywords = {1-butyl-3-methylimidazolium hexafluorophosphate,1002,2006,200600256,accepted,august 23,august 25,doi 10,headspace liquid-phase microex-,high-performance liquid chromatography,jssc,july 4,phenols,received,revised,traction},
number = {265},
pages = {42 -- 47},
pmid = {19878769},
primaryClass = {arXiv:hep-lat},
title = {{Successes and failures of backpropagation: A theoretical investigation}},
volume = {5},
year = {1997}
}
@article{Osher2009,
abstract = {We propose a fast algorithm for solving the Basis Pursuit problem, minu{|u|1 : Au = f}, which has application to compressed sensing. We design an efficient method for solving the related unconstrained problem minu E(u) = |u|1+$\lambda$?Au−f?2 2 based on a greedy coordinate descent method. We claim that in combination with a Bregman iterative method, our algorithm will achieve a solution with speed and accuracy competitive with some of the leading methods for the basis pursuit problem. 1.},
author = {Osher, Stanley and Li, Yingying},
doi = {10.3934/ipi.2009.3.487},
file = {:home/tom/Work/phd/Papers/Osher, Li_2009_Coordinate descent optimization for $ell_1$ minimization with application to compressed sensing a greedy algorithm.pdf:pdf},
issn = {1930-8337},
journal = {Inverse Problems and Imaging},
keywords = {and phrases,basis pursuit,bregman iteration,constrained,greedy sweep,shrinkage},
number = {3},
pages = {487--503},
title = {{Coordinate descent optimization for $\ell_1$ minimization with application to compressed sensing; a greedy algorithm}},
volume = {3},
year = {2009}
}
@article{Liu2017,
abstract = {While a number of different algorithms have recently been proposed for convolutional dictionary learning, this remains an expensive problem. The single biggest impediment to learning from large training sets is the memory requirements, which grow at least linearly with the size of the training set since all existing methods are batch algorithms. The work reported here addresses this limitation by extending online dictionary learning ideas to the convolutional context.},
archivePrefix = {arXiv},
arxivId = {1706.09563},
author = {Liu, Jialin and Garcia-Cardona, Cristina and Wohlberg, Brendt and Yin, Wotao},
eprint = {1706.09563},
file = {:home/tom/Work/phd/Papers/Liu et al._2017_Online Convolutional Dictionary Learning.pdf:pdf},
journal = {arXiv preprint},
number = {09563},
title = {{Online Convolutional Dictionary Learning}},
url = {http://arxiv.org/abs/1706.09563},
volume = {arXiv:1706},
year = {2017}
}
@article{Su2016,
abstract = {We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.},
archivePrefix = {arXiv},
arxivId = {1503.01243},
author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel J.},
doi = {10.1017/S1446788715000774},
eprint = {1503.01243},
file = {:home/tom/Work/phd/Papers/Su, Boyd, Candes_2016_A Differential Equation for Modeling Nesterov's Accelerated Gradient Method Theory and Insights.pdf:pdf},
issn = {10495258},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {convex optimization,differential equation,first-order methods,nesterov,restarting,s accelerated scheme},
number = {153},
pages = {1--43},
title = {{A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights}},
url = {http://arxiv.org/abs/1503.01243},
volume = {17},
year = {2016}
}
@inproceedings{Hu2010,
address = {Hong Kong, China},
author = {Hu, Rui and Barnard, Mark and Collomosse, John},
booktitle = {IEEE International Conference on Image Processing (ICIP)},
file = {:home/tom/Work/phd/Papers/Hu, Barnard, Collomosse_2010_Gradient Field Descriptor for Sketch Based Retrieval and Localization.pdf:pdf},
isbn = {9781424479948},
keywords = {able attention in recent,call for the retrieval,databases by visual example,has received consider-,of imagery based on,qve,the task of querying,visual appearance,with bag-of-visual-words ap-,years,yet many creative applications},
pages = {1025--1028},
title = {{Gradient Field Descriptor for Sketch Based Retrieval and Localization}},
year = {2010}
}
@article{Andoni2014,
author = {Andoni, Alexandr and Panigrahy, Rina and Valiant, Gregory and Zhang, Li},
file = {:home/tom/Work/phd/Papers/Andoni et al._2014_Learning Polynomials with Neural Networks.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
number = {2},
pages = {1908--1916},
title = {{Learning Polynomials with Neural Networks}},
volume = {32},
year = {2014}
}
@article{Chandrasekaran2012,
archivePrefix = {arXiv},
arxivId = {1012.0621},
author = {Chandrasekaran, Venkat and Recht, Benjamin and Parrilo, Pablo A. and Willsky, Alan S.},
doi = {10.1007/s10208-012-9135-7},
eprint = {1012.0621},
file = {:home/tom/Work/phd/Papers/Chandrasekaran et al._2012_The Convex Geometry of Linear Inverse Problems.pdf:pdf},
isbn = {978-1-4244-8215-3},
issn = {16153375},
journal = {Foundations of Computational Mathematics},
keywords = {Atomic norms,Convex optimization,Gaussian width,Real algebraic geometry,Semidefinite programming,Symmetry},
number = {6},
pages = {805--849},
title = {{The Convex Geometry of Linear Inverse Problems}},
volume = {12},
year = {2012}
}
@inproceedings{Bo2012,
author = {Bo, Liefeng and Sminchisescu, Cristian},
booktitle = {arXiv preprint},
file = {:home/tom/Work/phd/Papers/Bo, Sminchisescu_2012_Greedy Block Coordinate Descent for Large Scale Gaussian Process Regression.pdf:pdf},
isbn = {0-9749039-4-9},
number = {3238},
title = {{Greedy Block Coordinate Descent for Large Scale Gaussian Process Regression}},
volume = {arXiv:1206},
year = {2012}
}
@article{Oculo12,
author = {Gottlob, I. and Wizov, S. and Reinecke, R.},
journal = {Investigative Ophthalmology and Visual Science},
number = {36},
pages = {2768--2771},
title = {{Spasmus nutans. A long-term follow-up}},
volume = {Dec},
year = {1995}
}
@article{kim2004step,
author = {Kim, J and Jang, H and Hwang, D.-H. and Park, C},
journal = {Journal of Global Positioning Systems},
number = {1-2},
pages = {273--289},
publisher = {Scientific Research Publishing},
title = {{A step, stride and heading determination for the pedestrian navigation system}},
volume = {3},
year = {2004}
}
@article{pratt1969hadamard,
author = {Pratt, William K. and Kane, Julius and Andrews, Harry C.},
journal = {Proceedings of the IEEE},
number = {1},
pages = {58--68},
publisher = {IEEE},
title = {{Hadamard transform image coding}},
volume = {57},
year = {1969}
}
@article{salakhutdinov2009semantic,
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
journal = {International Journal of Approximate Reasoning},
number = {7},
pages = {969--978},
publisher = {Elsevier},
title = {{Semantic hashing}},
volume = {50},
year = {2009}
}
@inproceedings{ying2007automatic,
address = {Aachen, Germany},
author = {Ying, H and Silex, C and Schnitzer, A and Leonhardt, S and Schiek, M},
booktitle = {International Workshop on Wearable and Implantable Body Sensor Networks (BSN)},
pages = {80--85},
title = {{Automatic step detection in the accelerometer signal}},
year = {2007}
}
@article{Barber2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1404.5609v1},
author = {Barber, Rina F. and Cand{\`{e}}s, Emmanuel J.},
eprint = {arXiv:1404.5609v1},
file = {:home/tom/Work/phd/Papers/Barber, Cand{\`{e}}s_2014_Controlling the false discovery rate via knockoffs.pdf:pdf},
journal = {arXiv preprint},
keywords = {false discovery rate,fdr,lasso,martingale theory,mutation methods,per-,sequential hypothesis testing,variable selection},
number = {5609},
title = {{Controlling the false discovery rate via knockoffs}},
url = {http://arxiv.org/abs/1404.5609},
volume = {arXiv:1404},
year = {2014}
}
@article{Balakrishnan2014,
archivePrefix = {arXiv},
arxivId = {1408.2156},
author = {Balakrishnan, Sivaraman and Wainwright, Martin J. and Yu, Bin},
eprint = {1408.2156},
file = {:home/tom/Work/phd/Papers/Balakrishnan, Wainwright, Yu_2014_Statistical guarantees for the EM algorithm From population to sample-based analysis.pdf:pdf},
journal = {arXiv preprint},
number = {2156},
title = {{Statistical guarantees for the EM algorithm: From population to sample-based analysis}},
url = {http://arxiv.org/abs/1408.2156},
volume = {arXiv:1408},
year = {2014}
}
@inproceedings{jimenez2009comparison,
author = {Jimenez, A R and Seco, F and Prieto, C and Guevara, J},
booktitle = {International Symposium on Intelligent Signal Processing (WISP)},
organization = {IEEE},
pages = {37--42},
title = {{A comparison of pedestrian dead-reckoning algorithms using a low-cost MEMS IMU}},
year = {2009}
}
@inproceedings{Dauphin2014,
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1406.2572},
author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1406.2572},
file = {:home/tom/Work/phd/Papers/Dauphin et al._2014_Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.pdf:pdf},
isbn = {1406.2572},
issn = {10495258},
pages = {2933--2941},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
url = {http://arxiv.org/abs/1406.2572},
year = {2014}
}
@techreport{Camero1966,
author = {Camero, Scott H.},
file = {:home/tom/Work/phd/Papers/Camero_1966_Piece-wise Linear Approximations.pdf:pdf},
institution = {Computer Science Division, IIT Research Institute, Chicago, IL, USA},
title = {{Piece-wise Linear Approximations}},
year = {1966}
}
@article{Bahl1975,
author = {Bahl, L and Jelinek, Frederick},
journal = {IEEE Transactions on Information Theory},
number = {4},
pages = {404--411},
publisher = {IEEE},
title = {{Decoding for channels with insertions, deletions, and substitutions with applications to speech recognition}},
volume = {21},
year = {1975}
}
@inproceedings{Nutini,
address = {Lille, France},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.00552v1},
author = {Nutini, Julie and Schmidt, Mark and Laradji, Issam H and Friedlander, Michael P. and Koepke, Hoyt},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {arXiv:1506.00552v1},
file = {:home/tom/Work/phd/Papers/Nutini et al._2015_Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection.pdf:pdf},
pages = {1632--1641},
title = {{Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection}},
year = {2015}
}
@inproceedings{le1990handwritten,
address = {Denver, United States},
author = {{Le Cun}, B Boser and Denker, J S and Henderson, D and Howard, Richard E and Hubbard, W and Jackel, Lawrence D},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
organization = {Citeseer},
pages = {396--404},
title = {{Handwritten digit recognition with a back-propagation network}},
year = {1990}
}
@article{Sokolic2016,
abstract = {The generalization error of deep neural networks via their classification margin is studied in this work. Our approach is based on the Jacobian matrix of a deep neural network and can be applied to networks with arbitrary non-linearities and pooling layers, and to networks with different architectures such as feed forward networks and residual networks. Our analysis leads to the conclusion that a bounded spectral norm of the network's Jacobian matrix in the neighbourhood of the training samples is crucial for a deep neural network of arbitrary depth and width to generalize well. This is a significant improvement over the current bounds in the literature, which imply that the generalization error grows with either the width or the depth of the network. Moreover, it shows that the recently proposed batch normalization and weight normalization re-parametrizations enjoy good generalization properties, and leads to a novel network regularizer based on the network's Jacobian matrix. The analysis is supported with experimental results on the MNIST, CIFAR-10, LaRED and ImageNet datasets.},
archivePrefix = {arXiv},
arxivId = {1605.08254},
author = {Sokolic, Jure and Giryes, Raja and Sapiro, Guillermo and Rodrigues, Miguel R. D.},
doi = {10.1109/TSP.2017.2708039},
eprint = {1605.08254},
file = {:home/tom/Work/phd/Papers/Sokolic et al._2016_Robust Large Margin Deep Neural Networks.pdf:pdf},
issn = {1053-587X},
journal = {arXiv preprint},
number = {08254},
title = {{Robust Large Margin Deep Neural Networks}},
url = {http://arxiv.org/abs/1605.08254%0Ahttp://dx.doi.org/10.1109/TSP.2017.2708039},
volume = {arXiv:1605},
year = {2016}
}
@article{Hinton2006,
author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
file = {:home/tom/Work/phd/Papers/Hinton, Osindero, Teh_2006_A fast learning algorithm for deep belief nets.pdf:pdf},
journal = {Neural computation},
number = {7},
pages = {1527--1554},
publisher = {MIT Press},
title = {{A fast learning algorithm for deep belief nets}},
volume = {18},
year = {2006}
}
@article{nemirovski2009robust,
author = {Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
journal = {SIAM Journal on optimization},
number = {4},
pages = {1574--1609},
publisher = {SIAM},
title = {{Robust stochastic approximation approach to stochastic programming}},
volume = {19},
year = {2009}
}
@article{Ayachi2016,
author = {Ayachi, F and Nguyen, H and Goubault, E and Boissy, P and Duval, C},
file = {:home/tom/Work/phd/Papers/Ayachi et al._2016_The Use of Empirical Mode Decomposition-Based Algorithm and Inertial Measurement Units to Auto-Detect Daily Living Ac.pdf:pdf},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
number = {10},
pages = {1060--1070},
title = {{The Use of Empirical Mode Decomposition-Based Algorithm and Inertial Measurement Units to Auto-Detect Daily Living Activities of Healthy Adults}},
volume = {24},
year = {2016}
}
@article{karantonis2006implementation,
author = {Karantonis, D M and Narayanan, M R and Mathie, M and Lovell, N H and Celler, B G},
journal = {IEEE Transactions on Information Technology in Biomedicine},
number = {1},
pages = {156--167},
publisher = {IEEE},
title = {{Implementation of a real-time human movement classifier using a triaxial accelerometer for ambulatory monitoring}},
volume = {10},
year = {2006}
}
@inproceedings{krizhevsky2012imagenet,
address = {South Lake Tahoe, United States},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in neural information processing systems (NIPS)},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
year = {2012}
}
@inproceedings{Barak2013,
address = {Stanford University, USA},
archivePrefix = {arXiv},
arxivId = {1312.6652},
author = {Barak, Boaz and Kelner, Jonathan and Steurer, David},
booktitle = {Annual ACM Symposium on Theory of Computing},
eprint = {1312.6652},
file = {:home/tom/Work/phd/Papers/Barak, Kelner, Steurer_2013_Rounding Sum-of-Squares Relaxations.pdf:pdf},
pages = {1----45},
title = {{Rounding Sum-of-Squares Relaxations}},
url = {http://arxiv.org/abs/1312.6652},
year = {2013}
}
@article{Hornik1991,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt},
doi = {10.1016/0893-6080(91)90009-T},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Work/phd/Papers/Hornik_1991_Approximation capabilities of multilayer feedforward networks.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Activation function,Input environment measure,Lp(??) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
number = {2},
pages = {251--257},
pmid = {25246403},
title = {{Approximation capabilities of multilayer feedforward networks}},
volume = {4},
year = {1991}
}
@article{Scheinberg2013,
archivePrefix = {arXiv},
arxivId = {1311.6547},
author = {Scheinberg, Katya and Tang, Xiaocheng},
eprint = {1311.6547},
journal = {arXiv preprint},
number = {6547},
title = {{Practical Inexact Proximal Quasi-Newton Method with Global Complexity Analysis}},
volume = {arXiv:1311},
year = {2013}
}
@article{chandrasekaran2013computational,
author = {Chandrasekaran, Venkat and Jordan, Michael I},
journal = {Proceedings of the National Academy of Sciences},
number = {13},
pages = {E1181----E1190},
publisher = {National Acad Sciences},
title = {{Computational and statistical tradeoffs via convex relaxation}},
volume = {110},
year = {2013}
}
@article{DeCock2002,
author = {{De Cock}, Katrien and {De Moor}, Bart},
file = {:home/tom/Work/phd/Papers/De Cock, De Moor_2002_Subspace angles and distances between ARMA models.pdf:pdf},
journal = {System and Control Letter},
keywords = {arma models,distance measure,linear sys-,principal angles,stochastic realization,tems},
number = {4},
pages = {265--270},
title = {{Subspace angles and distances between ARMA models}},
volume = {46},
year = {2002}
}
@inproceedings{Chan2005,
address = {San Diego, CA, USA},
author = {Chan, Antoni B. and Vasconcelos, Nuno},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2005.279},
file = {:home/tom/Work/phd/Papers/Chan, Vasconcelos_2005_Probabilistic kernels for the classification of auto-regressive visual processes.pdf:pdf},
isbn = {0769523722},
issn = {1063-6919},
pages = {846--851},
title = {{Probabilistic kernels for the classification of auto-regressive visual processes}},
year = {2005}
}
@article{Douglas2000,
author = {Douglas, Scott C and Amari, Shun-ichi and Kung, S},
file = {:home/tom/Work/phd/Papers/Douglas, Amari, Kung_2000_On Gradient Adaptation with Unit-Norm Constraints.pdf:pdf},
journal = {IEEE Transactions on Signal Processing},
number = {6},
pages = {640--643},
title = {{On Gradient Adaptation with Unit-Norm Constraints}},
volume = {48},
year = {2000}
}
@article{Golyandina2015,
archivePrefix = {arXiv},
arxivId = {1309.5050},
author = {Golyandina, Nina and Korobeynikov, Anton and Shlemov, Alex and Usevich, Konstantin},
doi = {10.18637/jss.v067.i02},
eprint = {1309.5050},
file = {:home/tom/Work/phd/Papers/Golyandina et al._2015_Multivariate and 2D Extensions of Singular Spectrum Analysis with the Rssa Package.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {analysis,decomposition,forecasting,image processing,r package,singular spectrum analysis,time series},
number = {1},
pages = {1--78},
title = {{Multivariate and 2D Extensions of Singular Spectrum Analysis with the Rssa Package}},
url = {http://arxiv.org/abs/1309.5050%0Ahttp://dx.doi.org/10.18637/jss.v067.i02},
volume = {67},
year = {2015}
}
@article{Oculo24,
author = {Newman, SA and Hedges, TR and Wall, M and Sedwick, LA},
journal = {Survey of Ophthalmology},
number = {34},
pages = {453--456},
title = {{Spasmus nutans-- or is it?}},
volume = {May-Jun},
year = {1990}
}
@inproceedings{Adler2013,
address = {Southampton, United Kingdom},
author = {Adler, A. and Elad, Michael and Hel-Or, Y. and Rivlin, E.},
booktitle = {IEEE International Workshop on Machine Learning for Signal Processing (MLSP)},
pages = {22 -- 25},
title = {{Sparse Coding with Anomaly Detection}},
year = {2013}
}
@article{naqvi2012step,
author = {Naqvi, N Z and Kumar, A and Chauhan, A and Sahni, K},
journal = {International Journal on Computer Science and Engineering (IJCSE)},
number = {5},
pages = {675--682},
title = {{Step Counting Using Smartphone-Based Accelerometer}},
volume = {4},
year = {2012}
}
@article{Jolliffe2003,
abstract = {In many multivariate statistical techniques, a set of linear functions of the original p variables is produced. One of the more dif cult aspects of these techniques is the inter-pretation of the linear functions, as these functions usually have nonzero coeff cients on all p variables. A common approach is to effectively ignore (treat as zero) any coef cients less than some threshold value, so that the function becomes simple and the interpretation becomes easier for the users. Such a procedure can be misleading. There are alternatives to principal component analysis which restrict the coef cients to a smaller number of possible values in the derivationof the linear functions,or replace the principalcomponentsby " prin-cipal variables. " This article introduces a new technique, borrowing an idea proposed by Tibshirani in the context of multiple regression where similar problems arise in interpreting regression equations. This approach is the so-called LASSO, the " least absolute shrinkage and selection operator, " in which a bound is introduced on the sum of the absolute values of the coef cients, and in which some coef cients consequently become zero. We explore some of the propertiesof the new technique,both theoreticallyand using simulation studies, and apply it to an example.},
annote = {Founding paper for SPCA using LASSO

* True sparse PCA - with orthogonality constraints},
author = {Jolliffe, Ian T and Trendafilov, Nickolay T and Uddin, Mudassir},
doi = {10.1198/1061860032148},
file = {:home/tom/Work/phd/Papers/Jolliffe, Trendafilov, Uddin_2003_A Modified Principal Component Technique Based on the LASSO.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Interpretation,Principal component analysis,Simplii cation},
number = {3},
pages = {531--547},
title = {{A Modified Principal Component Technique Based on the LASSO}},
url = {http://amstat.tandfonline.com/loi/ucgs20%5Cnhttp://dx.doi.org/10.1198/1061860032148%5Cnhttp://amstat.tandfonline.com/page/terms-and-conditions},
volume = {12},
year = {2003}
}
@inproceedings{Vu2013,
address = {South Lake Tahoe, United States},
author = {Vu, Vincent Q and Lei, Jing and Cho, Juhee and Rohe, Karl},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/Vu et al._2013_Fantope Projection and Selection A near-optimal convex relaxation of sparse PCA.pdf:pdf},
pages = {2670--2678},
title = {{Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA}},
year = {2013}
}
@inproceedings{Bengio2006,
address = {Vancouver, Canada},
author = {Bengio, Yoshua and Roux, Nicolas Le and Vincent, Pascal and Delalleau, Olivier and Marcotte, Patrice and Branch, Downtown},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/Bengio et al._2006_Convex Neural Networks.pdf:pdf},
pages = {123--130},
title = {{Convex Neural Networks}},
year = {2006}
}
@inproceedings{Elhamifar2012,
author = {Elhamifar, Ehsan and Sapiro, Guillermo and Vidal, Rene},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2012.6247852},
file = {:home/tom/Work/phd/Papers/Elhamifar, Sapiro, Vidal_2012_See all by looking at a few Sparse modeling for finding representative objects.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
pages = {1600--1607},
pmid = {1006110},
title = {{See all by looking at a few: Sparse modeling for finding representative objects}},
year = {2012}
}
@article{Cavazza2017,
archivePrefix = {arXiv},
arxivId = {1710.05092},
author = {Cavazza, Jacopo and Morerio, Pietro and Haeffele, Benjamin and Lane, Connor and Murino, Vittorio and Vidal, Rene},
eprint = {1710.05092},
file = {:home/tom/Work/phd/Papers/Cavazza et al._2017_Dropout as a Low-Rank Regularizer for Matrix Factorization.pdf:pdf},
number = {2},
title = {{Dropout as a Low-Rank Regularizer for Matrix Factorization}},
url = {http://arxiv.org/abs/1710.05092},
year = {2017}
}
@inproceedings{Moghadam2005,
abstract = {Sparse PCA seeks approximate sparse “eigenvectors” whose projections capture the maximal variance of data. As a cardinality-constrained and non-convex optimization problem, it is NP-hard and is encountered in a wide range of applied fields, from bio-informatics to finance. Recent progress has focused mainly on continuous approximation and convex relaxation of the hard cardinality constraint. In contrast, we consider an alternative discrete spectral formulation based on variational eigenvalue bounds and provide an effective greedy strategy as well as provably optimal solutions using branch-and-bound search. Moreover, the exact methodology used reveals a simple renormalization step that improves approximate solutions obtained by any continuous method. The resulting performance gain of discrete algorithms is demonstrated on real-world benchmark data and in extensive Monte Carlo evaluation trials.},
address = {Vancouver, Canada},
annote = {From Duplicate 1 (Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms - Moghadam, Baback; Weiss, Yair; Avidan, Shai)

* Propose a 2-step procedure that evaluate the support of the sparse-eigenvector and then solve a reduced kxk problem to get exact value.
* Describe a combinatorial algorithm to compute the SPCA using growing matrices},
author = {Moghadam, Baback and Weiss, Yair and Avidan, Shai},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1145/1143844.1143925},
file = {:home/tom/Work/phd/Papers/Moghadam, Weiss, Avidan_2006_Spectral Bounds for Sparse PCA Exact and Greedy Algorithms.pdf:pdf;:home/tom/Work/phd/Papers/Moghadam, Weiss, Avidan_2006_Spectral Bounds for Sparse PCA Exact and Greedy Algorithms.pdf:pdf},
isbn = {9780262232531},
issn = {1049-5258},
pages = {915--922},
title = {{Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms}},
year = {2006}
}
@article{Dinh2017,
archivePrefix = {arXiv},
arxivId = {1703.04933},
author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
eprint = {1703.04933},
file = {:home/tom/Work/phd/Papers/Dinh et al._2017_Sharp Minima Can Generalize For Deep Nets.pdf:pdf},
issn = {1938-7228},
journal = {arXiv preprint},
number = {04933},
title = {{Sharp Minima Can Generalize For Deep Nets}},
url = {http://arxiv.org/abs/1703.04933},
volume = {arXiv:1703},
year = {2017}
}
@article{Zok2004,
abstract = {A double integration technique is presented that estimates whole body centre of mass (CoM) displacement from signals of a single force platform, compensating for the drift and low frequency noise inherent in the signals. The technique is composed of two different integration techniques, which may also be used separately, and is applied to transitory motor tasks with known initial and final conditions such as step ascent and descent, single step, etc. First, the lowest frequencies within the force platform signals and considered not to be associated with actual movement are filtered out. Second, a regular and a time-reversed double integration are performed and weighted against each other. The technique's accuracy was assessed using computer generated force platform signals that were artificially perturbed. Experimental data were used to compare the estimated CoM displacement to that obtained from a regular double integration and from segmental analysis performed on stereophotogrammetric data. It was shown that the proposed technique's CoM displacement estimates were more repeatable and up to 50% more accurate than those of a regular double integration. Moreover, the CoM displacement estimated using a single force platform and the proposed technique was found to be not statistically different from that obtained with more demanding measurement and processing techniques such as stereophotogrammetry and segmental analysis. ?? 2004 IPEM. Published by Elsevier Ltd. All rights reserved.},
author = {Zok, Mounir and Mazz{\`{a}}, Claudia and {Della Croce}, Ugo},
doi = {10.1016/j.medengphy.2004.07.005},
file = {:home/tom/Work/phd/Papers/Zok, Mazz{\`{a}}, Della Croce_2004_Total body centre of mass displacement estimated using ground reactions during transitory motor tasks Appl.pdf:pdf},
isbn = {1350-4533 (Print)\r1350-4533 (Linking)},
issn = {13504533},
journal = {Medical Engineering and Physics},
keywords = {Centre of mass,Force platform,Integration,Movement analysis},
number = {9 SPEC.ISS.},
pages = {791--798},
pmid = {15564116},
title = {{Total body centre of mass displacement estimated using ground reactions during transitory motor tasks: Application to step ascent}},
volume = {26},
year = {2004}
}
@inproceedings{Yang2010,
address = {San Francisco, CA, USA},
author = {Yang, Jianchao and Yu, Kai and Huang, Thomas},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Work/phd/Papers/Yang, Yu, Huang_2010_Supervised Translation-Invariant Sparse Coding.pdf:pdf},
isbn = {9781424469857},
keywords = {Convolution,Neural net,dictionary learning,sparse coding},
pages = {3517--3524},
publisher = {IEEE},
title = {{Supervised Translation-Invariant Sparse Coding}},
year = {2010}
}
@article{Oculo29,
author = {Sharpe, JA and Hoyt, WF and Rosenberg, MA},
journal = {Archive of Neurology},
number = {32},
pages = {191--4},
title = {{Convergence-evoked nystagmus. Congenital and acquired forms}},
volume = {Mar},
year = {1975}
}
@article{zaremba2014recurrent,
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
journal = {arXiv preprint},
number = {2329},
title = {{Recurrent neural network regularization}},
volume = {arXiv:1409},
year = {2014}
}
@inproceedings{Caruana2001,
address = {Vancouver, Canada},
author = {Caruana, Rich and Lawrence, Steve and Giles, Lee},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/Caruana, Lawrence, Giles_2001_Overfitting in Neural Nets Backpropagation, Conjugate Gradient, and Early Stopping.pdf:pdf},
pages = {402--408},
title = {{Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping}},
year = {2001}
}
@inproceedings{Yu2015,
address = {Lille, France},
author = {Yu, Rose and Cheng, Dehua and Liu, Yan},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Work/phd/Papers/Yu, Cheng, Liu_2015_Accelerated Online Low-Rank Tensor Learning for Multivariate spatio-temporal Streams.pdf:pdf},
pages = {1--10},
title = {{Accelerated Online Low-Rank Tensor Learning for Multivariate spatio-temporal Streams}},
volume = {37},
year = {2015}
}
@phdthesis{Gillis2011,
address = {Louvain-La-Neuve, Belgium},
author = {Gillis, Nicolas},
file = {:home/tom/Work/phd/Papers/Gillis_2011_Nonnegative matrix factorization Complexity, algorithms and applications.pdf:pdf},
school = {Universit\'e catholique de Louvain},
title = {{Nonnegative matrix factorization: Complexity, algorithms and applications}},
year = {2011}
}
@article{Rao2008,
abstract = {We consider settings where the observations are drawn from a zero-mean multivariate (real or complex) normal distribution with the population covariance matrix having eigenvalues of arbitrary multiplicity. We assume that the eigenvectors of the population covariance matrix are unknown and focus on inferential procedures that are based on the sample eigenvalues alone (i.e., "eigen-inference"). Results found in the literature establish the asymptotic normality of the fluctuation in the trace of powers of the sample covariance matrix. We develop concrete algorithms for analytically computing the limiting quantities and the covariance of the fluctuations. We exploit the asymptotic normality of the trace of powers of the sample covariance matrix to develop eigenvalue-based procedures for testing and estimation. Specifically, we formulate a simple test of hypotheses for the population eigenvalues and a technique for estimating the population eigenvalues in settings where the cumulative distribution function of the (nonrandom) population eigenvalues has a staircase structure. Monte Carlo simulations are used to demonstrate the superiority of the proposed methodologies over classical techniques and the robustness of the proposed techniques in high-dimensional, (relatively) small sample size settings. The improved performance results from the fact that the proposed inference procedures are "global" (in a sense that we describe) and exploit "global" information thereby overcoming the inherent biases that cripple classical inference procedures which are "local" and rely on "local" information.},
archivePrefix = {arXiv},
arxivId = {math/0701314},
author = {Rao, N. Raj and Mingo, James A. and Speicher, Roland and Edelman, Alan},
doi = {10.1214/07-AOS583},
eprint = {0701314},
file = {:home/tom/Work/phd/Papers/Rao et al._2008_Statistical Eigen-Inference from large wishart matrices.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Eigen-inference,Free probability,Linear statistics.,Random matrix theory,Sample covariance matrices,Second order freeness,Wishart matrices},
number = {6},
pages = {2850--2885},
primaryClass = {math},
title = {{Statistical Eigen-Inference from large wishart matrices}},
volume = {36},
year = {2008}
}
@article{Wang2014,
abstract = {Two types of low cost-per-iteration gradient descent methods have been extensively studied in parallel. One is online or stochastic gradient descent (OGD/SGD), and the other is randomzied coordinate descent (RBCD). In this paper, we combine the two types of methods together and propose online randomized block coordinate descent (ORBCD). At each iteration, ORBCD only computes the partial gradient of one block coordinate of one mini-batch samples. ORBCD is well suited for the composite minimization problem where one function is the average of the losses of a large number of samples and the other is a simple regularizer defined on high dimensional variables. We show that the iteration complexity of ORBCD has the same order as OGD or SGD. For strongly convex functions, by reducing the variance of stochastic gradients, we show that ORBCD can converge at a geometric rate in expectation, matching the convergence rate of SGD with variance reduction and RBCD.},
archivePrefix = {arXiv},
arxivId = {1407.0107},
author = {Wang, Huahua and Banerjee, Arindam},
eprint = {1407.0107},
file = {:home/tom/Work/phd/Papers/Wang, Banerjee_2014_Randomized Block Coordinate Descent for Online and Stochastic Optimization.pdf:pdf},
isbn = {1407.0107},
journal = {arXiv preprint},
number = {0107},
title = {{Randomized Block Coordinate Descent for Online and Stochastic Optimization}},
url = {http://arxiv.org/abs/1407.0107},
volume = {arXiv:1407},
year = {2014}
}
@misc{Grisel2016,
author = {Moreau, Thomas and Grisel, Olivier},
howpublished = {https://github.com/tomMoral/loky},
title = {{Loky}},
url = {https://github.com/tomMoral/loky}
}
@article{bubeck2014theory,
author = {Bubeck, S{\'{e}}bastien},
journal = {arXiv preprint},
number = {4980},
title = {{Theory of convex optimization for machine learning}},
volume = {arXiv:1405},
year = {2014}
}
@article{Edelman1998,
archivePrefix = {arXiv},
arxivId = {physics/9806030},
author = {Edelman, Alan and Arias, Tom{\'{a}}s A. and Smith, Steven T.},
doi = {10.1137/S0895479895290954},
eprint = {9806030},
file = {:home/tom/Work/phd/Papers/Edelman, Arias, Smith_1998_The Geometry of Algorithms with Orthogonality Constraints.pdf:pdf},
isbn = {08954798},
issn = {0895-4798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {conjugate gradient,eigenvalue optimization,eigenvalues and eigenvectors,electronic,grassmann man-,ifold,invariant subspace,newton,orthogonality constraints,rayleigh quotient iteration,reduced gradient method,s method,sequential quadratic programming,stiefel manifold,structures computation,subspace tracking},
number = {2},
pages = {303--353},
primaryClass = {physics},
title = {{The Geometry of Algorithms with Orthogonality Constraints}},
url = {http://epubs.siam.org/doi/abs/10.1137/S0895479895290954},
volume = {20},
year = {1998}
}
@article{Harris1954,
author = {Harris, Zellig S.},
doi = {10.1080/00437956.1954.11659520},
file = {:home/tom/Work/phd/Papers/Harris_1954_Distributional Structure.pdf:pdf},
isbn = {978-90-277-1267-7},
issn = {0043-7956},
journal = {WORD},
number = {2-3},
pages = {146--162},
title = {{Distributional Structure}},
url = {http://www.tandfonline.com/doi/full/10.1080/00437956.1954.11659520},
volume = {10},
year = {1954}
}
@inproceedings{Jiang2011,
address = {Colorado Spring, CO, USA},
author = {Jiang, Zhuolin and Lin, Zhe and Davis, Larry S and Incorporated, Adobe Systems and Jose, San},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Work/phd/Papers/Jiang et al._2011_Learning A Discriminative Dictionary for Sparse Coding via Label Consistent K-SVD.pdf:pdf},
pages = {1697--1704},
title = {{Learning A Discriminative Dictionary for Sparse Coding via Label Consistent K-SVD}},
year = {2011}
}
@article{Mairal2010,
archivePrefix = {arXiv},
arxivId = {0908.0050},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
eprint = {0908.0050},
file = {:home/tom/Work/phd/Papers/Mairal et al._2010_Online Learning for Matrix Factorization and Sparse Coding.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Deconvolution,basis pursuit,dictionary learning,factorization,ing,matrix factorization,negative matrix factorization,non,online,online learning,sparse cod,sparse coding,sparse principal component analysis,stochastic approximations,stochastic optimization,transport},
mendeley-tags = {Deconvolution,transport},
number = {1},
pages = {19--60},
title = {{Online Learning for Matrix Factorization and Sparse Coding}},
url = {http://arxiv.org/abs/0908.0050},
volume = {11},
year = {2010}
}
@inproceedings{Zinkevich2010,
abstract = {With the increase in available data parallel machine learning has become an in- creasingly pressing problem. In this paper we present the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evi- dence. Unlike prior work on parallel optimization algorithms [5, 7] our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analy- sis introduces a novel proof technique — contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime [1, 8]. 1},
address = {Vancouver, Canada},
author = {Zinkevich, Martin a and Smola, Alex and Weimer, Markus},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1088/0741-3335/38/11/011},
file = {:home/tom/Work/phd/Papers/Zinkevich, Smola, Weimer_2010_Parallelized Stochastic Gradient Descent.pdf:pdf},
issn = {07413335},
keywords = {Distributed,Optimization,Parallel algorithm,Stochastic Gradient Descent},
mendeley-tags = {Distributed,Optimization},
pages = {2595--2603},
pmid = {12484348},
title = {{Parallelized Stochastic Gradient Descent}},
url = {http://martin.zinkevich.org/publications/nips2010.pdf},
year = {2010}
}
@article{Grosse2007,
author = {Grosse, Roger and Raina, Rajat and Kwong, Helen and Ng, Andrew Y},
file = {:home/tom/Work/phd/Papers/Grosse et al._2007_Shift-Invariant Sparse Coding for Audio Classification.pdf:pdf},
journal = {Cortex},
keywords = {Convolution,dictionary learning,sparse coding},
pages = {9},
title = {{Shift-Invariant Sparse Coding for Audio Classification}},
volume = {8},
year = {2007}
}
@article{Oculo17,
author = {Kelly, TW},
journal = {Pediatrics},
number = {45},
pages = {295--296},
title = {{Optic glioma presenting as spasmus nutans}},
volume = {Feb},
year = {1970}
}
@article{Shalev-Shwartz2017,
abstract = {In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four families of problems for which some of the commonly used existing algorithms fail or suffer significant difficulty. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.},
archivePrefix = {arXiv},
arxivId = {1703.07950},
author = {Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
eprint = {1703.07950},
file = {:home/tom/Work/phd/Papers/Shalev-Shwartz, Shamir, Shammah_2017_Failures of Deep Learning.pdf:pdf},
journal = {arXiv preprint},
number = {07950},
title = {{Failures of Deep Learning}},
url = {http://arxiv.org/abs/1703.07950},
volume = {arXiv:1703},
year = {2017}
}
@article{hinton2012deep,
author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and Others},
journal = {IEEE Signal Processing Magazine},
number = {6},
pages = {82--97},
publisher = {IEEE},
title = {{Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups}},
volume = {29},
year = {2012}
}
@inproceedings{Wan2012,
address = {Atlanta, GA, USA},
author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Lecun, Yann and Fergus, Rob},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Work/phd/Papers/Wan et al._2012_Regularization of Neural Networks using DropConnect.pdf:pdf},
pages = {1058--1066},
title = {{Regularization of Neural Networks using DropConnect}},
year = {2012}
}
@article{Ghil2002,
author = {Ghil, M and Allen, M R and Dettinger, M D and Ide, K and Kondrashov, D and Mann, M E and Robertson, a W and Saunders, A and Tian, Y and Varadi, F and Yiou, P},
doi = {10.1029/2001RG000092},
file = {:home/tom/Work/phd/Papers/Ghil et al._2002_Advanced spectral methods for climate time series.pdf:pdf},
isbn = {8755-1209},
issn = {8755-1209},
journal = {Reviews of Geophysics},
number = {1},
pages = {3.1--3.41},
title = {{Advanced spectral methods for climate time series}},
volume = {40},
year = {2002}
}
@inproceedings{alaoui15fast,
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1411.0306},
author = {{El Alaoui}, Ahmed and Mahoney, Michael W.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1411.0306},
file = {:home/tom/Work/phd/Papers/El Alaoui, Mahoney_2015_Fast Randomized Kernel Ridge Regression with Statistical Guarantees.pdf:pdf},
issn = {10495258},
pages = {775--783},
title = {{Fast Randomized Kernel Ridge Regression with Statistical Guarantees}},
year = {2015}
}
@article{Parikh2014,
abstract = {Thismonograph is about a class of optimization algorithms called prox- imal algorithms.Much like Newton's method is a standard tool for solv- ing unconstrained smooth optimization problems of modest size, proxi- mal algorithms can be viewed as an analogous tool for nonsmooth, con- strained, large-scale, or distributed versions of these problems. They are very generally applicable, but are especially well-suited to problems of substantial recent interest involving large or high-dimensional datasets. Proximal methods sit at a higher level of abstraction than classical al- gorithms like Newton's method: the base operation is evaluating the proximal operator of a function, which itself involves solving a small convex optimization problem. These subproblems, which generalize the problem of projecting a point onto a convex set, often admit closed- form solutions or can be solved very quickly with standard or simple specialized methods. Here, we discuss the many different interpreta- tions of proximal operators and algorithms, describe their connections to many other topics in optimization and applied mathematics, survey some popular algorithms, and provide a large number of examples of proximal operators that commonly arise in practice.},
author = {Parikh, Neal and Boyd, Stephen},
doi = {10.1561/2400000003},
file = {:home/tom/Work/phd/Papers/Parikh, Boyd_2014_Proximal Algorithms.pdf:pdf},
isbn = {9781601987167},
issn = {2167-3888},
journal = {Foundations and Trends in Optimization},
number = {3},
pages = {123--231},
title = {{Proximal Algorithms}},
volume = {1},
year = {2014}
}
@inproceedings{rahimi2007random,
address = {Vancouver, Canada},
author = {Rahimi, Ali and Recht, Benjamin},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1177--1184},
title = {{Random features for large-scale kernel machines}},
year = {2007}
}
@inproceedings{hinton2008using,
address = {Vancouver, Canada},
author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1249--1256},
title = {{Using deep belief nets to learn covariance kernels for Gaussian processes}},
year = {2008}
}
@article{Giryes2016,
annote = {* Use projected gradient descent as a base for there work.
* Use an approximation of the solution set to do a 2 stage, innacurate projection on the solution set and give theoretical analysis

* Small link to LISTA, not so clear.},
archivePrefix = {arXiv},
arxivId = {1605.09232},
author = {Giryes, Raja and Eldar, Yonina C. and Bronstein, Alex M. and Sapiro, Guillermo},
eprint = {1605.09232},
file = {:home/tom/Work/phd/Papers/Giryes et al._2016_Tradeoffs between Convergence Speed and Reconstruction Accuracy in Inverse Problems.pdf:pdf},
journal = {arXiv preprint},
number = {09232},
title = {{Tradeoffs between Convergence Speed and Reconstruction Accuracy in Inverse Problems}},
url = {http://arxiv.org/abs/1605.09232},
volume = {arXiv:1605},
year = {2016}
}
@inproceedings{Megalooikonomou2005,
abstract = {Efficiently and accurately searching for similarities among time series and discovering interesting patterns is an important and non-trivial problem. In this paper, we introduce a new representation of time series, the Multiresolution Vector Quantized (MVQ) approximation, along with a new distance function. The novelty of MVQ is that it keeps both local and global information about the original time series in a hierarchical mechanism, processing the original time series at multiple resolutions. Moreover, the proposed representation is symbolic employing key subsequences and potentially allows the application of text-based retrieval techniques into the similarity analysis of time series. The proposed method is fast and scales linearly with the size of database and the dimensionality. Contrary to the vast majority in the literature that uses the Euclidean distance, MVQ uses a multi-resolution/hierarchical distance function. We performed experiments with real and synthetic data. The proposed distance function consistently outperforms all the major competitors (Euclidean, Dynamic Time Warping, Piecewise Aggregate Approximation) achieving up to 20% better precision/recall and clustering accuracy on the tested datasets.},
author = {Megalooikonomou, V. and Faloutsos, C.},
booktitle = {International Conference on Data Engineering (ICDE)},
doi = {10.1109/ICDE.2005.10},
file = {:home/tom/Work/phd/Papers/Megalooikonomou, Faloutsos_2005_A Multiresolution Symbolic Representation of Time Series.pdf:pdf},
isbn = {0-7695-2285-8},
issn = {10844627},
pages = {668--679},
title = {{A Multiresolution Symbolic Representation of Time Series}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1410183},
year = {2005}
}
@inproceedings{Sprechmann2013,
address = {Vancouver, Canada},
author = {Sprechmann, Pablo and Bronstein, Alex M. and Bronstein, Michael and Sapiro, Guillermo},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:home/tom/Work/phd/Papers/Sprechmann et al._2013_Learnable Low Rank Sparse Models for Speech Denoising.pdf:pdf},
pages = {136--140},
title = {{Learnable Low Rank Sparse Models for Speech Denoising}},
year = {2013}
}
@inproceedings{Fercoq2015,
address = {Lille, France},
archivePrefix = {arXiv},
arxivId = {1505.03410},
author = {Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1505.03410},
file = {:home/tom/Work/phd/Papers/Fercoq, Gramfort, Salmon_2015_Mind the duality gap safer rules for the Lasso.pdf:pdf},
pages = {333--342},
title = {{Mind the duality gap : safer rules for the Lasso}},
year = {2015}
}
@article{He2000,
author = {He, B. S. and Yang, Hai and Wang, S. L.},
doi = {10.1023/A:1004603514434},
file = {:home/tom/Work/phd/Papers/He, Yang, Wang_2000_Alternating Direction Method with Self-Adaptive Penalty Parameters for Monotone Variational Inequalities.pdf:pdf},
issn = {0022-3239},
journal = {Journal of Optimization Theory and Applications},
keywords = {alternating direction,method,monotone variational inequalities,variable penalty parameters},
number = {2},
pages = {337--356},
title = {{Alternating Direction Method with Self-Adaptive Penalty Parameters for Monotone Variational Inequalities}},
url = {http://link.springer.com/10.1023/A:1004603514434},
volume = {106},
year = {2000}
}
@inproceedings{Lewicki1999,
address = {Denver, CO, USA},
author = {Lewicki, Michael S and Sejnowski, Terrence J and Jolla, La},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/Lewicki, Sejnowski, Jolla_1999_Coding time-varying signals using sparse, shift-invariant representations.pdf:pdf},
pages = {730--736},
title = {{Coding time-varying signals using sparse, shift-invariant representations}},
year = {1999}
}
@inproceedings{alaoui2015fast,
author = {Alaoui, Ahmed and Mahoney, Michael W},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {775--783},
title = {{Fast randomized kernel ridge regression with statistical guarantees}},
year = {2015}
}
@article{Martin2000,
author = {Martin, Richard J.},
doi = {10.1109/78.827549},
file = {:home/tom/Work/phd/Papers/Martin_2000_Metric for ARMA processes.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {autoregression,classification,linear systems},
number = {4},
pages = {1164--1170},
title = {{Metric for ARMA processes}},
volume = {48},
year = {2000}
}
@article{Papyan2017a,
archivePrefix = {arXiv},
arxivId = {1607.02009},
author = {Papyan, Vardan and Sulam, Jeremias and Elad, Michael},
doi = {10.1109/TSP.2017.2733447},
eprint = {1607.02009},
file = {:home/tom/Work/phd/Papers/Papyan, Sulam, Elad_2017_Working Locally Thinking Globally Theoretical Guarantees for Convolutional Sparse Coding.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Basis Pursuit,Computational modeling,Convolution,Convolutional Sparse Coding,Convolutional codes,Dictionaries,Global modeling,Local Processing,Matching pursuit algorithms,Mathematical model,Orthogonal Matching Pursuit,Sparse Representations,Sparse matrices,Stability Guarantees,Uniqueness Guarantees},
number = {21},
pages = {5687--5701},
title = {{Working Locally Thinking Globally: Theoretical Guarantees for Convolutional Sparse Coding}},
volume = {65},
year = {2017}
}
@article{Dalalyan2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1700v1},
author = {Dalalyan, Arnak S and Hebiri, Mohamed and Lederer, Johannes},
eprint = {arXiv:1402.1700v1},
journal = {arXiv preprint},
number = {1700},
title = {{On the prediction performance of the Lasso}},
volume = {arXiv:1402},
year = {2014}
}
@article{montavon2011kernel,
author = {Montavon, Gr{\'{e}}goire and Braun, Mikio L and M{\"{u}}ller, Klaus-Robert},
file = {:home/tom/Work/phd/Papers/Montavon, Braun, M{\"{u}}ller_2011_Kernel analysis of deep networks.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep networks,kernel principal component analysis,representations},
number = {Sep},
pages = {2563--2581},
title = {{Kernel analysis of deep networks}},
volume = {12},
year = {2011}
}
@inproceedings{Bradley2011,
address = {Bellevue, WA, USA},
archivePrefix = {arXiv},
arxivId = {1105.5379},
author = {Bradley, Joseph K. and Kyrola, Aapo and Bickson, Danny and Guestrin, Carlos},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1105.5379},
file = {:home/tom/Work/phd/Papers/Bradley et al._2011_Parallel Coordinate Descent for $ell_1$-Regularized Loss Minimization.pdf:pdf},
isbn = {9781450306195},
pages = {321--328},
title = {{Parallel Coordinate Descent for $\ell_1$-Regularized Loss Minimization}},
url = {http://arxiv.org/abs/1105.5379},
year = {2011}
}
@inproceedings{Heide2015,
address = {Boston, MA, USA},
author = {Heide, Felix and Heidrich, Wolfgang and Wetzstein, Gordon},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7299149},
file = {:home/tom/Work/phd/Papers/Heide, Heidrich, Wetzstein_2015_Fast and flexible convolutional sparse coding.pdf:pdf},
isbn = {978-1-4673-6964-0},
pages = {5135--5143},
title = {{Fast and flexible convolutional sparse coding}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7299149},
year = {2015}
}
@article{Yue2016,
abstract = {We propose a second-order method, the inexact regularized proximal Newton (IRPN) method, to minimize a sum of smooth and non-smooth convex functions. We prove that the IRPN method converges globally to the set of optimal solutions and the asymptotic rate of convergence is superlinear, even when the objective function is not strongly convex. Key to our analysis is a novel usage of a certain error bound condition. We compare two empirically efficient algorithms---the newGLMNET and adaptively restarted FISTA---with our proposed IRPN by applying them to the $\ell_1$-regularized logistic regression problem. Experiment results show the superiority of our proposed algorithm.},
archivePrefix = {arXiv},
arxivId = {1605.07522},
author = {Yue, Man-Chung and Zhou, Zirui and So, Anthony Man-Cho},
eprint = {1605.07522},
file = {:home/tom/Work/phd/Papers/Yue, Zhou, So_2016_Inexact Regularized Proximal Newton Method Provable Convergence Guarantees for Non-Smooth Convex Minimization without.pdf:pdf},
journal = {arXiv preprint},
number = {07522},
title = {{Inexact Regularized Proximal Newton Method: Provable Convergence Guarantees for Non-Smooth Convex Minimization without Strong Convexity}},
url = {http://arxiv.org/abs/1605.07522},
volume = {arXiv:1605},
year = {2016}
}
@inproceedings{Barchiesi2011,
address = {Prague, Czech Republic},
author = {Barchiesi, Daniele and Plumbley, Mark D.},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2011.5947682},
file = {:home/tom/Work/phd/Papers/Barchiesi, Plumbley_2011_Dictionary Learning of Convolved Signals.pdf:pdf},
keywords = {Convolution,dictionary learning},
pages = {5812--5815},
title = {{Dictionary Learning of Convolved Signals}},
year = {2011}
}
@article{hotelling1933analysis,
author = {Hotelling, Harold},
journal = {Journal of educational psychology},
number = {6},
pages = {417},
publisher = {Warwick \& York},
title = {{Analysis of a complex of statistical variables into principal components.}},
volume = {24},
year = {1933}
}
@inproceedings{Bruna2015,
address = {South Brisbane, Australia},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bruna, Joan and Sprechmann, Pablo and LeCun, Yan},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Work/phd/Papers/Bruna, Sprechmann, LeCun_2015_Source separation with scattering non-negative matrix factorization.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {1876----1880},
pmid = {25246403},
title = {{Source separation with scattering non-negative matrix factorization}},
year = {2015}
}
@inproceedings{Bagnall2006,
address = {Philadelphia, United States},
author = {Bagnall, Anthony and Ratanamahatana, Chotirat “Ann” and Keogh, Eamonn and Lonardi, Stefano and Janacek, Gareth},
booktitle = {SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1007/s10618-005-0028-0},
file = {:home/tom/Work/phd/Papers/Bagnall et al._2006_A Bit Level Representation for Time Series Data Mining with Shape Based Similarity.pdf:pdf},
isbn = {0000000000000},
issn = {1384-5810},
number = {1},
pages = {11--40},
publisher = {ACM},
title = {{A Bit Level Representation for Time Series Data Mining with Shape Based Similarity}},
url = {http://link.springer.com/10.1007/s10618-005-0028-0},
volume = {13},
year = {2006}
}
@article{Low2012,
abstract = {While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.},
archivePrefix = {arXiv},
arxivId = {1204.6078},
author = {Low, Yucheng and Bickson, Danny and Gonzalez, Joseph and Guestrin, Carlos and Kyrola, Aapo and Hellerstein, Joseph M},
doi = {10.14778/2212351.2212354},
eprint = {1204.6078},
file = {:home/tom/Work/phd/Papers/Low et al._2012_Distributed GraphLab a framework for machine learning and data mining in the cloud.pdf:pdf},
issn = {2150-8097},
journal = {VLDB Endowment},
keywords = {a framework for machine,and data mining in,learning,the cloud,tributed graphlab},
number = {8},
pages = {716--727},
title = {{Distributed GraphLab: a framework for machine learning and data mining in the cloud}},
url = {http://dl.acm.org/citation.cfm?id=2212354},
volume = {5},
year = {2012}
}
@inproceedings{Oudre2015,
address = {Lyon, France},
author = {Oudre, Laurent and Moreau, Thomas and Truong, Charles and Barrois-M{\"{u}}ller, R{\'{e}}mi and Dadashi, Robert and Gr{\'{e}}gory, Thomas},
booktitle = {Groupe de Recherche et d'Etudes en Traitement du Signal et des Images (GRETSI)},
file = {:home/tom/Work/phd/Papers/Oudre et al._2015_D{\'{e}}tection de pas {\`{a}} partir de donn{\'{e}}es d'acc{\'{e}}l{\'{e}}rom{\'{e}}trie.pdf:pdf},
title = {{D{\'{e}}tection de pas {\`{a}} partir de donn{\'{e}}es d'acc{\'{e}}l{\'{e}}rom{\'{e}}trie}},
year = {2015}
}
@article{DAspremont2008,
annote = {* True sparse PCA - with orthogonality constraints},
author = {D'Aspremont, Alexandre and Bach, Francis and Ghaoui, L.E.},
file = {:home/tom/Work/phd/Papers/d'Aspremont, Bach, Ghaoui_2008_Optimal solutions for sparse principal component analysis.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {lasso,pca,sparse eigenvalues,sparse recovery,subset selection},
pages = {1269--1294},
title = {{Optimal solutions for sparse principal component analysis}},
url = {http://dl.acm.org/citation.cfm?id=1442775},
volume = {9},
year = {2008}
}
@article{Scheinberg2013,
abstract = {Recently several methods were proposed for sparse optimization which make careful use of second-order information [10, 28, 16, 3] to improve local convergence rates. These methods construct a composite quadratic approximation using Hessian information, optimize this approximation using a first-order method, such as coordinate descent and employ a line search to ensure sufficient descent. Here we propose a general framework, which includes slightly modified versions of existing algorithms and also a new algorithm, which uses limited memory BFGS Hessian approximations, and provide a novel global convergence rate analysis, which covers methods that solve subproblems via coordinate descent.},
archivePrefix = {arXiv},
arxivId = {1311.6547},
author = {Scheinberg, Katya and Tang, Xiaocheng},
eprint = {1311.6547},
file = {:home/tom/Work/phd/Papers/Scheinberg, Tang_2013_Practical Inexact Proximal Quasi-Newton Method with Global Complexity Analysis.pdf:pdf},
journal = {arXiv preprint},
number = {6547},
title = {{Practical Inexact Proximal Quasi-Newton Method with Global Complexity Analysis}},
url = {http://arxiv.org/abs/1311.6547},
volume = {arXiv:1311},
year = {2013}
}
@article{Richtarik2011,
abstract = {In this paper we develop a randomized block-coordinate descent method for minimizing the sum of a smooth and a simple nonsmooth block-separable convex function and prove that it obtains an $\epsilon$-accurate solution with probability at least $1-\rho$ in at most $O(\tfrac{n}{\epsilon} \log \tfrac{1}{\rho})$ iterations, where $n$ is the number of blocks. For strongly convex functions the method converges linearly. This extends recent results of Nesterov [Efficiency of coordinate descent methods on huge-scale optimization problems, CORE Discussion Paper #2010/2], which cover the smooth case, to composite minimization, while at the same time improving the complexity by the factor of 4 and removing $\epsilon$ from the logarithmic term. More importantly, in contrast with the aforementioned work in which the author achieves the results by applying the method to a regularized version of the objective function with an unknown scaling factor, we show that this is not necessary, thus achieving true iteration complexity bounds. In the smooth case we also allow for arbitrary probability vectors and non-Euclidean norms. Finally, we demonstrate numerically that the algorithm is able to solve huge-scale $\ell_1$-regularized least squares and support vector machine problems with a billion variables.},
archivePrefix = {arXiv},
arxivId = {1107.2848},
author = {Richt{\'{a}}rik, Peter and Tak{\'{a}}{\v{c}}, Martin},
eprint = {1107.2848},
file = {:home/tom/Work/phd/Papers/Richt{\'{a}}rik, Tak{\'{a}}{\v{c}}_2014_Iteration Complexity of Randomized Block-Coordinate Descent Methods for Minimizing a Composite Function(2).pdf:pdf},
journal = {Mathematical Programming},
keywords = {alternating minimization,block coordinate descent,composite minimization,convex optimization,coor-,dinate relaxation,iteration complexity,l1-regularization,large scale,support vector machines},
number = {1-2},
pages = {1--38},
title = {{Iteration Complexity of Randomized Block-Coordinate Descent Methods for Minimizing a Composite Function}},
url = {http://arxiv.org/abs/1107.2848},
volume = {144},
year = {2014}
}
@article{Chen1994,
author = {Chen, Gong and Teboulle, Marc},
journal = {Mathematical Programming},
number = {1-3},
pages = {81--101},
title = {{A proximal-based decomposition method for convex minimization problems}},
volume = {64},
year = {1994}
}
@article{Li2016,
abstract = {We develop a fast and robust algorithm for solving large-scale convex composite optimization models with an emphasis on the ℓ 1 -regularized least square regression (the Lasso) problems. Although there exist a large amount of solvers in the literature for Lasso problems, so far no solver can handle difficult real large scale regression problems. By relying on the piecewise linear-quadratic structure of the problems to realize the remarkable fast linear convergence property of the augmented Lagrangian algorithm, and by exploiting the superlinear convergence of the semismooth Newton-CG method, we are able to design a new algorithm, called Ssnal, to efficiently solve the aforementioned difficult problems. Global convergence and local linear convergence results for Ssnal are established. Numerical results, including the comparison between our approach and several state-of-the-art solvers, on real data sets, are presented to demonstrate the high efficiency and robustness of our proposed algorithm in solving large-scale difficult problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1607.05428v1},
author = {Li, Xudong and Sun, Defeng and Toh, Kim-Chuan},
eprint = {arXiv:1607.05428v1},
file = {:home/tom/Work/phd/Papers/Li, Sun, Toh_2016_An efficient linearly convergent semismooth Netwon-CG augmented Lagrangian method for Lasso problems.pdf:pdf},
journal = {arXiv preprint},
keywords = {()},
number = {05428},
title = {{An efficient linearly convergent semismooth Netwon-CG augmented Lagrangian method for Lasso problems}},
volume = {arXiv:1607},
year = {2016}
}
@article{DAspremont2014,
archivePrefix = {arXiv},
arxivId = {1205.0121},
author = {D'Aspremont, Alexandre and Bach, Francis and Ghaoui, Laurent El},
doi = {10.1007/s10107-014-0751-7},
eprint = {1205.0121},
file = {:home/tom/Work/phd/Papers/d'Aspremont, Bach, Ghaoui_2014_Approximation bounds for sparse principal component analysis.pdf:pdf},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {62H25,90C22,90C27},
number = {1-2},
pages = {89--110},
title = {{Approximation bounds for sparse principal component analysis}},
volume = {148},
year = {2014}
}
@article{Jain2000,
author = {Jain, A.K. and Duin, R. P W and Mao, Jianchang},
doi = {10.1109/34.824819},
file = {:home/tom/Work/phd/Papers/Jain, Duin, Mao_2000_Statistical pattern recognition a review.pdf:pdf},
isbn = {0162-8828 VO - 22},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {1},
pages = {4--37},
pmid = {17542025},
title = {{Statistical pattern recognition: a review}},
url = {http://ieeexplore.ieee.org/ielx5/34/17859/00824819.pdf?tp=&arnumber=824819&isnumber=17859%5Cnhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824819},
volume = {22},
year = {2000}
}
@article{Oculo18,
author = {Kushnner, BJ},
journal = {Archive Ophthalmology},
number = {113},
pages = {1298--1300},
title = {{Infantile uniocular blindness with bilateral nystagmus. A syndrome}},
volume = {Oct},
year = {1995}
}
@article{Hillar2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1106.3616v3},
author = {Hillar, Christopher and Sommer, Ft},
eprint = {arXiv:1106.3616v3},
file = {:home/tom/Work/phd/Papers/Hillar, Sommer_2011_When can dictionary learning uniquely recover sparse data from subsamples.pdf:pdf},
journal = {arXiv preprint},
number = {3616},
title = {{When can dictionary learning uniquely recover sparse data from subsamples?}},
url = {http://arxiv.org/abs/1106.3616},
volume = {arXiv:1106},
year = {2011}
}
@article{Liang2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1711.01530v1},
author = {Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
eprint = {arXiv:1711.01530v1},
file = {:home/tom/Work/phd/Papers/Liang et al._2017_Fisher-Rao Metric, Geometry, and Complexity of Neural Networks.pdf:pdf},
journal = {arXiv preprint},
keywords = {and phrases,capacity control,deep learning,fisher-rao metric,generalization error,gradient,infor-,invariance,mation geometry,natural,relu activation,statistical learning theory},
number = {01530},
title = {{Fisher-Rao Metric, Geometry, and Complexity of Neural Networks}},
volume = {arXiv:1711},
year = {2017}
}
@inproceedings{Schuldt2004,
abstract = {Local space-time features capture local events in video and can be adapted to the size, the frequency and the veloc-ity of moving patterns. In this paper we demonstrate how such features can be used for recognizing complex motion patterns. We construct video representations in terms of lo-cal space-time features and integrate such representations with SVM classification schemes for recognition. For the purpose of evaluation we introduce a new video database containing 2391 sequences of six human actions performed by 25 people in four different scenarios. The presented re-sults of action recognition justify the proposed method and demonstrate its advantage compared to other relative ap-proaches for action recognition.},
address = {Cambridge, UK},
archivePrefix = {arXiv},
arxivId = {1505.04868},
author = {Sch{\"{u}}ldt, Christian and Laptev, Ivan and Caputo, Barbara},
booktitle = {International Conference on Pattern Recognition (ICPR)},
doi = {10.1109/ICPR.2004.1334462},
eprint = {1505.04868},
file = {:home/tom/Work/phd/Papers/Sch{\"{u}}ldt, Laptev, Caputo_2004_Recognizing human actions A local SVM approach.pdf:pdf},
isbn = {0769521282},
issn = {10514651},
pages = {32--36},
pmid = {12171414},
title = {{Recognizing human actions: A local SVM approach}},
year = {2004}
}
@article{marcus1993building,
author = {Marcus, Mitchell P and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
journal = {Computational linguistics},
number = {2},
pages = {313--330},
publisher = {MIT Press},
title = {{Building a large annotated corpus of English: The Penn Treebank}},
volume = {19},
year = {1993}
}
@article{Oculo33,
author = {Weissman, BM and Dell'Osso, LF and Abel, LA and Leigh, RJ},
journal = {Archive of Ophthalmology},
number = {105},
pages = {525--8},
title = {{Spasmus nutans. A quantitative prospective study}},
volume = {Apr},
year = {1987}
}
@inproceedings{Radinsky2004,
abstract = {An improved algorithm for classification of nystagmus was designed allowing the sorting of response segments even in severely non-linear patients and subjects with abnormally large phase shifts. The algorithm employs a model-based approach that was developed by Rey and Galiana [1]. The improved classification algorithm consists of two essential stages. In the first stage the eye velocity response is classified to obtain initial estimates of the slow phase eye velocity intervals. In the second stage, the slow phase estimates are used to identify a response phase shift and non- linearity, and compensate for their effects. Multiple tests on simulated data and experimental data obtained from clinical subjects are presented. The results of the tests demonstrate that the algorithm is able to analyze the patient data with a high accuracy even in the presence of noise, eye- blinks and other artifacts. Keywords—Nystagmus},
address = {San Francisco, CA, USA},
author = {Radinsky, Iliya and Galiana, Henrietta L.},
booktitle = {IEEE Annual International Conference Engineering in Medicine and Biology Society (IEMBS)},
file = {:home/tom/Work/phd/Papers/Radinsky, Galiana_2004_Improved Algorithm for Classification of Ocular Nystagmus.pdf:pdf},
isbn = {0780384393},
keywords = {eye movements,nystagmus analysis,vestibulo-ocular reflex,vor},
pages = {534--537},
title = {{Improved Algorithm for Classification of Ocular Nystagmus}},
year = {2004}
}
@article{He2011,
author = {He, Bingsheng and Tao, Min and Xu, Minghua and Yuan, Xiaoming},
doi = {10.1080/02331934.2011.611885},
file = {:home/tom/Work/phd/Papers/He et al._2011_An alternating direction-based contraction method for linearly constrained separable convex programming problems.pdf:pdf},
issn = {0233-1934},
journal = {Optimization},
keywords = {90c06,90c22,90c25,alternating direction method,ams subject classifications,constraint,contraction method,convex programming,linear,separable structure},
number = {January},
pages = {1--24},
title = {{An alternating direction-based contraction method for linearly constrained separable convex programming problems}},
year = {2011}
}
@article{Qiu2002,
abstract = {In this paper, we present a method to represent achromatic and chromatic image signals independently for content-based image indexing and retrieval for image database applications. Starting from an opponent colour representation, human colour vision theories and modern digital signal processing technologies are applied to develop a compact and computationally efficient visual appearance model for coloured image patterns. We use the model to compute the statistics of achromatic and chromatic spatial patterns of colour images for indexing and content-based retrieval. Two types of colour images databases, one colour texture database and another photography colour image database are used to evaluate the performance of the developed method in content-based image indexing and retrieval. Experimental results are presented to show that the new method is superior or competitive to state-of-the-art content-based image indexing and retrieval techniques. ?? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {Qiu, G.},
doi = {10.1016/S0031-3203(01)00162-5},
file = {:home/tom/Work/phd/Papers/Qiu_2002_Indexing chromatic and achromatic patterns for content-based colour image retrieval.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Achromatic patterns,Chromatic patterns,Colour imaging,Colour vision,Content-based image indexing and retrieval,Image database,Vector quantization},
number = {8},
pages = {1675--1686},
title = {{Indexing chromatic and achromatic patterns for content-based colour image retrieval}},
volume = {35},
year = {2002}
}
@inproceedings{Moreau2015,
author = {Moreau, Thomas and Oudre, Laurent and Vayatis, Nicolas},
booktitle = {NIPS Workshop on Nonparametric Methods for Large Scale Representation Learning},
file = {:home/tom/Work/phd/Papers/Moreau, Oudre, Vayatis_2015_Distributed Convolutional Sparse Coding via Message Passing Interface ( MPI ).pdf:pdf},
title = {{Distributed Convolutional Sparse Coding via Message Passing Interface ( MPI )}},
url = {http://www.cs.cmu.edu/$\sim$andrewgw/rep/MorOudVay.pdf},
year = {2015}
}
@inproceedings{Mairal2014,
abstract = {An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.3332v1},
author = {Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {arXiv:1406.3332v1},
file = {:home/tom/Work/phd/Papers/Mairal et al._2014_Convolutional Kernel Networks.pdf:pdf},
pages = {2627--2635},
title = {{Convolutional Kernel Networks}},
url = {http://arxiv.org/abs/1406.3332},
year = {2014}
}
@inproceedings{Rudi2015,
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1507.04717},
author = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1507.04717},
file = {:home/tom/Work/phd/Papers/Rudi, Camoriano, Rosasco_2015_Less is More Nystr{\"{o}}m Computational Regularization.pdf:pdf},
pages = {1657--1665},
title = {{Less is More: Nystr{\"{o}}m Computational Regularization}},
url = {http://arxiv.org/abs/1507.04717},
year = {2015}
}
@article{Tseng2009,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tseng, Paul and Yun, Sangwoon},
doi = {10.1007/s10957-008-9458-3},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Work/phd/Papers/Tseng, Yun_2009_Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization.pdf:pdf},
isbn = {9780874216561},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Bilevel optimization,Complexity bound,Coordinate gradient descent,Global convergence,Linear constraints,Linear convergence rate,Nonsmooth optimization,Support vector machines,ℓ1-regularization},
number = {3},
pages = {513--535},
pmid = {15991970},
title = {{Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization}},
volume = {140},
year = {2009}
}
@inproceedings{Kurtek2011,
address = {Grenada, Spain},
author = {Kurtek, Sebastian and Srivastava, Anuj and Wu, Wei},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/Kurtek, Srivastava, Wu_2011_Signal Estimation Under Random Time-Warpings and Nonlinear Signal Alignment.pdf:pdf},
isbn = {9781618395993},
pages = {575--683},
title = {{Signal Estimation Under Random Time-Warpings and Nonlinear Signal Alignment}},
year = {2011}
}
@article{Rubinstein2008,
author = {Rubinstein, Ron and Zibulevsky, Michael and Elad, Michael},
doi = {10.1.1.182.9978},
file = {:home/tom/Work/phd/Papers/Rubinstein, Zibulevsky, Elad_2008_Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit.pdf:pdf},
journal = {CS Technion},
number = {8},
pages = {1--15},
title = {{Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit}},
url = {http://cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2008/CS/CS-2008-08.revised.pdf},
volume = {40},
year = {2008}
}
@inproceedings{Alexandrov2005,
address = {St. Petersburg, Russia},
author = {Alexandrov, Theodore and Golyandina, Nina},
booktitle = {Workshop on Simulation},
file = {:home/tom/Work/phd/Papers/Alexandrov, Golyandina_2005_Automatic extraction and forecast of time series cyclic components within the framework of SSA.pdf:pdf},
pages = {45--50},
title = {{Automatic extraction and forecast of time series cyclic components within the framework of SSA}},
url = {http://www.math.uni-bremen.de/$\sim$theodore/mywiki/uploads/Main/AlexandrovGolyandina2005AutoSSA_cycles_WorkshopOnSimulation05.pdf},
year = {2005}
}
@article{Rozell2008,
abstract = {While evidence indicates that neural systems may be employing sparse approximations to represent sensed stimuli, the mechanisms underlying this ability are not understood. We describe a locally competitive algorithm (LCA) that solves a collection of sparse coding principles minimizing a weighted combination of mean-squared error and a coefficient cost function. LCAs are designed to be implemented in a dynamical system composed of many neuron-like elements operating in parallel. These algorithms use thresholding functions to induce local (usually one-way) inhibitory competitions between nodes to produce sparse representations. LCAs produce coefficients with sparsity levels comparable to the most popular centralized sparse coding algorithms while being readily suited for neural implementation. Additionally, LCA coefficients for video sequences demonstrate inertial properties that are both qualitatively and quantitatively more regular (i.e., smoother and more predictable) than the coefficients produced by greedy algorithms.},
author = {Rozell, Christopher J and Johnson, Don H and Baraniuk, Richard G and Olshausen, Bruno A},
doi = {10.1162/neco.2008.03-07-486},
file = {:home/tom/Work/phd/Papers/Rozell et al._2008_Sparse coding via thresholding and local competition in neural circuits.pdf:pdf},
isbn = {9781424495290},
issn = {08997667},
journal = {Neural Computation},
keywords = {algorithms,models,neurological,neurons,neurons physiology},
number = {10},
pages = {2526--63},
pmid = {18439138},
title = {{Sparse coding via thresholding and local competition in neural circuits.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18439138%5Cnhttp://redwood.berkeley.edu/bruno/papers/rozell-sparse-coding-nc08.pdf},
volume = {20},
year = {2008}
}
@techreport{Kong2013,
author = {Kong, Bailey and Fowlkes, Charless C},
doi = {10.1109/CVPR.2013.57},
file = {:home/tom/Work/phd/Papers/Kong, Fowlkes_2013_Fast Convolutional Sparse Coding.pdf:pdf},
institution = {Department of Computer Science, University of California, Irvine},
isbn = {1063-6919 VO -},
keywords = {convolution,dictionary learning,sparse coding},
pages = {1--7},
title = {{Fast Convolutional Sparse Coding}},
url = {http://hiltonbristow.com/static/papers/2013_CVPR_Bristow.pdf},
year = {2013}
}
@book{rose2006human,
author = {Rose, J and Gamble, J G and Adams, J M},
publisher = {Lippincott Williams \& Wilkins Philadelphia},
title = {{Human walking}},
year = {2006}
}
@article{Ahmed1974,
author = {Ahmed, N. and Natarajan, T. and Rao, K.R.},
doi = {10.1109/T-C.1974.223784},
file = {:home/tom/Work/phd/Papers/Ahmed, Natarajan, Rao_1974_Discrete Cosine Transform.pdf:pdf},
isbn = {0018-9340},
issn = {0018-9340},
journal = {IEEE Transactions on Computers},
number = {1},
pages = {90--93},
title = {{Discrete Cosine Transform}},
url = {http://dasan.sejong.ac.kr/$\sim$dihan/dip/p5_DCT.pdf},
volume = {C-23},
year = {1974}
}
@article{Zou2006a,
abstract = {penalty; the lasso penalty (via the elastic net) can then be directly integrated into the regression criterion, leading to a modified with loadings.},
annote = {* True sparse PCA - with orthogonality constraints},
archivePrefix = {arXiv},
arxivId = {1205.0121v2},
author = {Zou, H and Hastie, T and Tibshirani, R},
doi = {10.1198/106186006X113430},
eprint = {1205.0121v2},
file = {:home/tom/Work/phd/Papers/Zou, Hastie, Tibshirani_2006_Sparse principal component analysis.pdf:pdf},
isbn = {106186006X},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
number = {2},
pages = {262--286},
pmid = {21811560},
title = {{Sparse principal component analysis}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed&cmd=Retrieve&dopt=AbstractPlus&list_uids=10700132108553319406related:7nMSBHaBfpQJ},
volume = {15},
year = {2006}
}
@inproceedings{zhuang2011two,
address = {Ft. Lauderdale, FL, USA},
author = {Zhuang, Jinfeng and Tsang, Ivor W and Hoi, Steven C H},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {909--917},
title = {{Two-Layer Multiple Kernel Learning.}},
year = {2011}
}
@article{Papyan2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1607.08194v1},
author = {Papyan, Vardan and Romano, Yaniv and Elad, Michael},
eprint = {arXiv:1607.08194v1},
file = {:home/tom/Work/phd/Papers/Papyan, Romano, Elad_2016_Convolutional Neural Networks Analyzed via Convolutional Sparse Coding.pdf:pdf},
journal = {arXiv preprint},
number = {08194},
title = {{Convolutional Neural Networks Analyzed via Convolutional Sparse Coding}},
volume = {arXiv:1607},
year = {2016}
}
@article{Loh2014,
abstract = {We demonstrate that the primal-dual witness proof method may be used to establish variable selection consistency and $\ell_\infty$-bounds for sparse regression problems, even when the loss function and/or regularizer are nonconvex. Using this method, we derive two theorems concerning support recovery and $\ell_\infty$-guarantees for the regression estimator in a general setting. Our results provide rigorous theoretical justification for the use of nonconvex regularization: For certain nonconvex regularizers with vanishing derivative away from the origin, support recovery consistency may be guaranteed without requiring the typical incoherence conditions present in $\ell_1$-based methods. We then derive several corollaries that illustrate the wide applicability of our method to analyzing composite objective functions involving losses such as least squares, nonconvex modified least squares for errors-in variables linear regression, the negative log likelihood for generalized linear models, and the graphical Lasso. We conclude with empirical studies to corroborate our theoretical predictions.},
archivePrefix = {arXiv},
arxivId = {1412.5632},
author = {Loh, Po-Ling and Wainwright, Martin J.},
eprint = {1412.5632},
file = {:home/tom/Work/phd/Papers/Loh, Wainwright_2014_Support recovery without incoherence A case for nonconvex regularization.pdf:pdf},
journal = {arXiv preprint},
number = {5632},
title = {{Support recovery without incoherence: A case for nonconvex regularization}},
url = {http://arxiv.org/abs/1412.5632},
volume = {arXiv:1412},
year = {2014}
}
@inproceedings{Razavian2014,
address = {Colombus, OH, USA},
author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
file = {:home/tom/Work/phd/Papers/Razavian et al._2014_CNN Features off-the-Shelf an Astounding Baseline for Recognition.pdf:pdf},
pages = {806--813},
title = {{CNN Features off-the-Shelf: an Astounding Baseline for Recognition}},
year = {2014}
}
@inproceedings{shin2007adaptive,
author = {Shin, S H and Park, C G and Kim, J W and Hong, H S and Lee, J M},
booktitle = {Sensors Applications Symposium, 2007. SAS'07. IEEE},
organization = {IEEE},
pages = {1--5},
title = {{Adaptive step length estimation algorithm using low-cost MEMS inertial sensors}},
year = {2007}
}
@article{Oculo27,
author = {Raudnitz, R. W.},
journal = {Jb Kinderheilkd},
pages = {145},
title = {{Zur Lehre von Spasmus Nutans}},
volume = {45},
year = {1897}
}
@article{Oudre2012,
abstract = {In this paper, we introduce a novel nonparametric classification technique based on the use of the Wasserstein distance. The proposed scheme is applied in a biomedical context for the analysis of recorded accelerometer data: the aim is to retrieve three types of periodic activities (walking, biking, and running) from a time-frequency representation of the data. The main interest of the use of the Wasserstein distance lies in the fact that it is less sensitive to the location of the frequency peaks than to the global structure of the frequency pattern, allowing us to detect activities almost independently of their speed or incline. Our system is tested on a 24-subject corpus: results show that the use of Wasserstein distance combined with some supervised learning techniques allows us to compare with some more complex classification systems.},
author = {Oudre, Laurent and Jakubowicz, J{\'{e}}r{\'{e}}mie and Bianchi, Pascal and Simon, Chantal},
doi = {10.1109/TBME.2012.2190930},
file = {:home/tom/Work/phd/Papers/Oudre et al._2012_Classification of periodic activities using the Wasserstein distance.pdf:pdf},
isbn = {0018-9294},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Accelerometer signals,Wasserstein distance,biomedical signal processing,classification},
number = {6},
pages = {1610--1619},
pmid = {22434794},
title = {{Classification of periodic activities using the Wasserstein distance}},
volume = {59},
year = {2012}
}
@article{Bellman1952,
author = {Bellman, Richard},
doi = {10.1073/pnas.38.8.716},
file = {:home/tom/Work/phd/Papers/Bellman_1952_On the Theory of Dynamic Programming.pdf:pdf},
isbn = {00278424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {8},
pages = {716--719},
pmid = {16589166},
title = {{On the Theory of Dynamic Programming}},
volume = {38},
year = {1952}
}
@article{Becker2009,
author = {Becker, Stephen and Bobin, J{\'{e}}r{\^{o}}me and Cand{\`{e}}s, Emmanuel J.},
file = {:home/tom/Work/phd/Papers/Becker, Bobin, Cand{\`{e}}s_2009_Nesta a fast and accurate first-order method for sparse recovery ´.pdf:pdf},
issn = {19364954},
journal = {SIAM Journal on Imaging Sciences},
keywords = {1 mini-,compressed sensing,continuation methods,duality in convex optimization,functions,mization,nesterov,s method,smooth approximations of nonsmooth,total-variation},
number = {1},
pages = {1--37},
title = {{Nesta: a fast and accurate first-order method for sparse recovery ´}},
volume = {91125},
year = {2009}
}
@book{Hastie2009,
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {Springer},
doi = {10.1007/b94608},
file = {:home/tom/Work/phd/Papers/Hastie, Tibshirani, Friedman_2009_The Elements of Statistical Learning.pdf:pdf},
isbn = {9780387848570},
issn = {03436993},
pages = {1----694},
pmid = {15512507},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/10.1007/b94608},
year = {2009}
}
@article{cortes1995support,
author = {Cortes, Corinna and Vapnik, Vladimir},
journal = {Machine learning},
number = {3},
pages = {273--297},
publisher = {Springer},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@article{Kurtek2012,
author = {Kurtek, Sebastian and Wu, Wei and Srivastava, Anuj},
doi = {10.1080/02664763.YYYY.XXXXXX},
file = {:home/tom/Work/phd/Papers/Kurtek, Wu, Srivastava_2013_Segmentation and Statistical Analysis of Biosignals with Application to Disease Classification.pdf:pdf},
journal = {Journal of Applied Statistics},
keywords = {classification,electrocardiogram,gait,infarction,myocardial,signal segmentation,statistical functional analysis},
number = {6},
pages = {1270----1288},
title = {{Segmentation and Statistical Analysis of Biosignals with Application to Disease Classification}},
volume = {40},
year = {2013}
}
@inproceedings{Engan1999,
address = {Phoenix, AZ, USA},
author = {Engan, Kjersti and Aase, Sven Ole and Hus{\o}y, John H{\aa}kon},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:home/tom/Work/phd/Papers/Engan, Aase, Hus{\o}y_1999_Method of Optimal Directions for Frame Design.PDF:PDF},
pages = {2443----2446},
title = {{Method of Optimal Directions for Frame Design}},
year = {1999}
}
@article{Robert2016,
author = {Robert, Matthieu P. and Grill, Jacques and Moreau, Thomas and Grevent, David and Zambrowsky, Olivia and Varlet, Pascale and Contal, Emile and Martin, Gilles and Br{\'{e}}mond-Gignac, Dominique and Ingster-Moati, Isabelle and Dufour, Christelle and Brugi{\`{e}}res, Laurence and Vayatis, Nicolas and Boddaert, Nathalie and Sainte-Rose, Christian and Blauwblomme, Thomas and Puget, St{\'{e}}phanie and Vidal, Pierre-Paul},
journal = {submitted to Brain},
title = {{Optic pathway gliomas-associated nystagmus}},
year = {2016}
}
@article{Dasgupta2003,
author = {Dasgupta, Sanjoy and Gupta, Anupam},
doi = {10.1002/rsa.10073},
file = {:home/tom/Work/phd/Papers/Dasgupta, Gupta_2003_An Elementary Proof of a Theorem of Johnson and Lindenstrauss.pdf:pdf},
isbn = {1098-2418},
issn = {10429832},
journal = {Random Structures and Algorithms},
number = {1},
pages = {60--65},
title = {{An Elementary Proof of a Theorem of Johnson and Lindenstrauss}},
volume = {22},
year = {2003}
}
@article{Liu2015,
abstract = {We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate ($1/K$) on general convex functions. Near-linear speedup on a multicore system can be expected if the number of processors is $O(n^{1/2})$ in unconstrained optimization and $O(n^{1/4})$ in the separable-constrained case, where $n$ is the number of variables. We describe results from implementation on 40-core processors.},
archivePrefix = {arXiv},
arxivId = {1311.1873},
author = {Liu, Ji and Wright, Stephen J and R{\'{e}}, Christopher and Bittorf, Victor and Sridhar, Srikrishna},
eprint = {1311.1873},
file = {:home/tom/Work/phd/Papers/Liu et al._2015_An asynchronous parallel stochastic coordinate descent algorithm.pdf:pdf},
isbn = {9781634393973},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {asynchronous parallel optimization,stochastic coordinate descent},
pages = {285--322},
title = {{An asynchronous parallel stochastic coordinate descent algorithm}},
url = {http://arxiv.org/abs/1311.1873},
volume = {16},
year = {2015}
}
@inproceedings{hadsell2008deep,
author = {Hadsell, Raia and Erkan, Ayse and Sermanet, Pierre and Scoffier, Marco and Muller, Urs and LeCun, Yann},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
organization = {IEEE},
pages = {628--633},
title = {{Deep belief net learning in a long-range vision system for autonomous off-road driving}},
year = {2008}
}
@article{Chaudhari2017a,
archivePrefix = {arXiv},
arxivId = {arXiv:1704.04932v2},
author = {Chaudhari, Pratik and Oberman, Adam and Osher, Stanley and Soatto, Stefano and Carlier, Guillaume},
eprint = {arXiv:1704.04932v2},
file = {:home/tom/Work/phd/Papers/Chaudhari et al._2017_Deep relaxation partial differential equations for optimizing deep neural networks.pdf:pdf},
journal = {arXiv preprint},
keywords = {deep learning,entropy,inf-convolution,local,local convexity,neural networks,non-convex optimization,optimal control,partial differential equations,proximal,regularization,smoothing,stochastic gradient descent,viscous burgers},
number = {04932},
title = {{Deep relaxation: partial differential equations for optimizing deep neural networks}},
volume = {arXiv:1704},
year = {2017}
}
@article{kadri2015operator,
author = {Kadri, Hachem and DUFLOS, Emmanuel and Preux, Philippe and Stephane canu and Rakotomamonjy, Alain and Audiffren, Julien},
journal = {Journal of Machine Learning Research (JMLR)},
title = {{Operator-valued Kernels for Learning from Functional Response Data}},
year = {2015}
}
@inproceedings{Bengio2007,
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {citeulike-article-id:4640046},
eprint = {0500581},
file = {:home/tom/Work/phd/Papers/Bengio et al._2007_Greedy Layer-Wise Training of Deep Networks.pdf:pdf},
isbn = {0262195682},
issn = {01628828},
pages = {153--160},
pmid = {19018704},
primaryClass = {submit},
title = {{Greedy Layer-Wise Training of Deep Networks}},
url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
year = {2007}
}
@article{Rakotomamonjy2015,
author = {Rakotomamonjy, Alain and Gasso, Gilles},
journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)},
number = {1},
pages = {142--153},
publisher = {IEEE Press},
title = {{Histogram of gradients of time-frequency representations for audio scene classification}},
volume = {23},
year = {2015}
}
@inproceedings{Pham2013,
address = {Chicago, United States},
author = {Pham, Ninh and Pagh, Rasmus},
booktitle = {SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2487575.2487591},
file = {:home/tom/Work/phd/Papers/Pham, Pagh_2013_Fast and scalable polynomial kernels via explicit feature maps.pdf:pdf},
isbn = {9781450321747},
issn = {9781450321747},
keywords = {count sketch,fft,polynomial kernel,svm,tensor product},
pages = {239},
publisher = {ACM},
title = {{Fast and scalable polynomial kernels via explicit feature maps}},
url = {http://dl.acm.org/citation.cfm?doid=2487575.2487591},
year = {2013}
}
@article{Hall2014,
author = {Hall, Peter and Xue, Jing Hao},
doi = {10.1016/j.csda.2012.10.010},
file = {:home/tom/Work/phd/Papers/Hall, Xue_2014_On selecting interacting features from high-dimensional data.pdf:pdf},
isbn = {01679473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Classification,Correlation,Feature ranking,Generalised correlation},
pages = {694--708},
publisher = {Elsevier B.V.},
title = {{On selecting interacting features from high-dimensional data}},
url = {http://dx.doi.org/10.1016/j.csda.2012.10.010},
volume = {71},
year = {2014}
}
@article{Boyd2010,
author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
doi = {10.1561/2200000016},
file = {:home/tom/Work/phd/Papers/Boyd et al._2010_Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.pdf:pdf},
isbn = {1935823719358},
issn = {1935-8237},
journal = {Foundations and Trends in Machine Learning},
number = {1},
pages = {1--122},
title = {{Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers}},
url = {http://www.nowpublishers.com/product.aspx?product=MAL&doi=2200000016%5Cnpapers3://publication/uuid/4BDE54D0-4DE4-4136-BF94-8E1201C74798},
volume = {3},
year = {2010}
}
@inproceedings{Ge2016,
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {1605.07272},
author = {Ge, Rong and Lee, Jason and Ma, Tengyu},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1605.07272},
file = {:home/tom/Work/phd/Papers/Ge, Lee, Ma_2016_Matrix Completion has No Spurious Local Minimum.pdf:pdf},
pages = {2973--2981},
title = {{Matrix Completion has No Spurious Local Minimum}},
year = {2016}
}
@inproceedings{Giselsson2014,
address = {Los Angeles, CA},
author = {Giselsson, Pontus and Boyd, Stephen},
booktitle = {IEEE Conference on Decision and Control (CDC)},
file = {:home/tom/Work/phd/Papers/Giselsson, Boyd_2014_Monotonicity and Restart in Fast Gradient Methods.pdf:pdf},
keywords = {optimization,proximal method,sparse coding},
pages = {5058----5063},
publisher = {IEEE},
title = {{Monotonicity and Restart in Fast Gradient Methods}},
year = {2014}
}
@article{Lebrun2012,
author = {Lebrun, Marc and Leclaire, Arthur},
doi = {10.5201/ipol.2012.llm-ksvd},
file = {:home/tom/Work/phd/Papers/Lebrun, Leclaire_2012_An Implementation and Detailed Analysis of the K-SVD Image Denoising Algorithm.pdf:pdf},
issn = {2105-1232},
journal = {Image Processing On Line},
keywords = {denoising,dictionaries,learning,patches,sparse representation},
pages = {96--133},
title = {{An Implementation and Detailed Analysis of the K-SVD Image Denoising Algorithm}},
url = {/Users/kvitayau1/Desktop/Dissertation/literature/to_sort/006 - Denoising the image/An implementation and detailed analysis of the K-SVD image denoising algorithm.pdf%5Cnhttp://dx.doi.org/10.5201/ipol.2012.llm-ksvd},
volume = {2 VN - re},
year = {2012}
}
@article{Xiang2014,
abstract = {This paper is a survey of dictionary screening for the lasso problem. The lasso problem seeks a sparse linear combination of the columns of a dictionary to best match a given target vector. This sparse representation has proven useful in a variety of subsequent processing and decision tasks. For a given target vector, dictionary screening quickly identifies a subset of dictionary columns that will receive zero weight in a solution of the corresponding lasso problem. These columns can be removed from the dictionary, prior to solving the lasso problem, without impacting the optimality of the solution obtained. This has two potential advantages: it reduces the size of the dictionary, allowing the lasso problem to be solved with less resources, and it may speed up obtaining a solution. Using a geometrically intuitive framework, we provide basic insights for understanding useful lasso screening tests and their limitations. We also provide illustrative numerical studies on several datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.4897v1},
author = {Xiang, Zj and Wang, Yun and Ramadge, Pj},
eprint = {arXiv:1405.4897v1},
file = {:home/tom/Work/phd/Papers/Xiang, Wang, Ramadge_2014_Screening tests for lasso problems.pdf:pdf},
journal = {arXiv preprint},
keywords = {dictionary screening,dual lasso,feature selection,lasso,sparse representation},
number = {4897},
title = {{Screening tests for lasso problems}},
url = {http://arxiv.org/abs/1405.4897},
volume = {arXiv:1405},
year = {2014}
}
@article{Giryes2016b,
archivePrefix = {arXiv},
arxivId = {1712.04741},
author = {Vidal, Ren{\'{e}} and Bruna, Joan and Giryes, Raja and Soatto, Stefano},
doi = {10.1007/978-3-540-31299-4},
eprint = {1712.04741},
file = {:home/tom/Work/phd/Papers/Vidal et al._2016_Mathematics of Deep Learning.pdf:pdf},
isbn = {978-3-540-21992-7},
issn = {13534858},
journal = {preprint},
number = {04741},
title = {{Mathematics of Deep Learning}},
volume = {arXiv:1712},
year = {2016}
}
@article{cadieu2012learning,
author = {Cadieu, Charles F and Olshausen, Bruno A},
file = {:home/tom/Work/phd/Papers/Cadieu, Olshausen_2012_Learning intermediate-level representations of form and motion from natural movies.pdf:pdf},
journal = {Neural computation},
number = {4},
pages = {827--866},
publisher = {MIT Press},
title = {{Learning intermediate-level representations of form and motion from natural movies}},
volume = {24},
year = {2012}
}
@inproceedings{dahl2013improving,
address = {Vancouver, Canada},
author = {Dahl, George E and Sainath, Tara N and Hinton, Geoffrey E},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
organization = {IEEE},
pages = {8609--8613},
title = {{Improving deep neural networks for LVCSR using rectified linear units and dropout}},
year = {2013}
}
@inproceedings{Sprechmann2012,
address = {Edinburgh, Great Britain},
archivePrefix = {arXiv},
arxivId = {1206.4649},
author = {Sprechmann, Pablo and Bronstein, Alex and Sapiro, Guillermo},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1206.4649},
file = {:home/tom/Work/phd/Papers/Sprechmann, Bronstein, Sapiro_2012_Learning Efficient Structured Sparse Models.pdf:pdf},
isbn = {978-1-4503-1285-1},
pages = {615--622},
title = {{Learning Efficient Structured Sparse Models}},
year = {2012}
}
@inproceedings{Lee2009,
abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3605v3},
author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.1145/1553374.1553453},
eprint = {arXiv:1301.3605v3},
file = {:home/tom/Work/phd/Papers/Lee et al._2009_Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations.pdf:pdf},
isbn = {9781605585161},
issn = {02643294},
pages = {1--8},
pmid = {20957573},
title = {{Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553453},
year = {2009}
}
@inproceedings{faloutsos1994fast,
address = {Minneapolis, MN, USA},
author = {Faloutsos, C and Ranganathan, M and Manolopoulos, Y},
booktitle = {ACM SIGMOD International Conference on Management of Data},
number = {2},
pages = {419--429},
title = {{Fast subsequence matching in time-series databases}},
volume = {23},
year = {1994}
}
@inproceedings{tran2012high,
address = {Ho Chi Minh City, Vietnam},
author = {Tran, K and Le, T and Dinh, T},
booktitle = {International Symposium on Signal Processing and Information Technology (ISSPIT)},
organization = {IEEE},
pages = {213--217},
title = {{A high-accuracy step counting algorithm for iPhones using accelerometer}},
year = {2012}
}
@article{Jutten1991,
abstract = {The separation of independent sources from an array of sensors is a classical but difficult problem in signal processing. Based on some biological observations, an adaptive algorithm is proposed to separate simultaneously all the unknown independent sources. The adaptive rule, which constitutes an independence test using non-linear functions, is the main original point of this blind identification procedure. Moreover, a new concept, that of INdependent Components Analysis (INCA), more powerful than the classical Principal Components Analysis (in decision tasks) emerges from this work.},
author = {Jutten, C and Herault, J},
doi = {10.1016/0165-1684(91)90079-X},
file = {:home/tom/Work/phd/Papers/Jutten, Herault_1991_Blind seperation of sources, part I an adaptive algorithm based on neuromimetic architecture.pdf:pdf},
isbn = {0165-1684},
issn = {01651684},
journal = {Signal Processing},
keywords = {Separation of sources,high order moments,independent components,linear recursive adaptive filter.,neural networks,principal components},
number = {1},
pages = {1--10},
title = {{Blind seperation of sources, part I: an adaptive algorithm based on neuromimetic architecture}},
volume = {24},
year = {1991}
}
@article{Luo2016,
abstract = {In this paper, we discuss the problem of minimizing the sum of two convex functions: a smooth function plus a non-smooth function. Further, the smooth part can be expressed by the average of a large number of smooth component functions, and the non-smooth part is equipped with a simple proximal mapping. We propose a proximal stochastic second-order method, which is efficient and scalable. It incorporates the Hessian in the smooth part of the function and exploits multistage scheme to reduce the variance of the stochastic gradient. We prove that our method can achieve linear rate of convergence.},
archivePrefix = {arXiv},
arxivId = {1602.00223},
author = {Luo, Luo and Chen, Zihao and Zhang, Zhihua and Li, Wu-Jun},
eprint = {1602.00223},
file = {:home/tom/Work/phd/Papers/Luo et al._2016_Variance-Reduced Second-Order Methods.pdf:pdf},
journal = {arXiv preprint},
keywords = {ICML,boring formatting information,machine learning},
number = {00223},
title = {{Variance-Reduced Second-Order Methods}},
url = {http://arxiv.org/abs/1602.00223},
volume = {arXiv:1602},
year = {2016}
}
@inproceedings{Yellin2017,
address = {Melbourne, Australia},
author = {Yellin, Florence and Haeffele, Benjamin D. and Vidal, Ren{\'{e}}},
booktitle = {IEEE International Symposium on Biomedical Imaging (ISBI)},
file = {:home/tom/Work/phd/Papers/Yellin, Haeffele, Vidal_2017_Blood cell detection and counting in holographic lens-free imaging by convolutional sparse dictionary learn.pdf:pdf},
title = {{Blood cell detection and counting in holographic lens-free imaging by convolutional sparse dictionary learning and coding}},
year = {2017}
}
@article{Vu2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1211.0373v4},
author = {Vu, Vincent Q and Lei, Jing},
doi = {10.1214/13-AOS1151},
eprint = {arXiv:1211.0373v4},
file = {:home/tom/Work/phd/Papers/Vu, Lei_2014_Minimax sparse principal subspace estimation in high dimensions.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {62C20,62H12,62H25, 62H12, 62C20, Principal components analysis,Principal components analysis},
number = {6},
pages = {2905--2947},
title = {{Minimax sparse principal subspace estimation in high dimensions}},
volume = {41},
year = {2014}
}
@article{Daneshmand2016,
archivePrefix = {arXiv},
arxivId = {1612.07335},
author = {Daneshmand, Amir and Scutari, Gesualdo and Facchinei, Francisco},
eprint = {1612.07335},
file = {:home/tom/Work/phd/Papers/Daneshmand, Scutari, Facchinei_2016_Distributed Dictionary Learning.pdf:pdf},
journal = {arXiv preprint},
number = {07335},
title = {{Distributed Dictionary Learning}},
volume = {arXiv:1612},
year = {2016}
}
@book{Tseng2009a,
abstract = {We consider the problem of minimizing the sum of a smooth function and a separable convex function. This problem includes as special cases bound-constrained optimization and smooth optimization with ℓ1-regularization. We propose a (block) coordinate gradient descent method for solving this class of nonsmooth separable problems. We establish global convergence and, under a local Lipschitzian error bound assumption, linear convergence for this method. The local Lipschitzian error bound holds under assumptions analogous to those for constrained smooth optimization, e.g., the convex function is polyhedral and the smooth function is (nonconvex) quadratic or is the composition of a strongly convex function with a linear mapping. We report numerical experience with solving the ℓ1-regularization of unconstrained optimization problems from Mor{\'{e}} et al. in ACM Trans. Math. Softw. 7, 17–41, 1981 and from the CUTEr set (Gould and Orban in ACM Trans. Math. Softw. 29, 373–394, 2003). Comparison with L-BFGS-B and MINOS, applied to a reformulation of the ℓ1-regularized problem as a bound-constrained optimization problem, is also reported.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tseng, Paul and Yun, Sangwoon},
booktitle = {Mathematical Programming},
doi = {10.1007/s10107-007-0170-0},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Work/phd/Papers/Tseng, Yun_2009_A coordinate gradient descent method for nonsmooth separable minimization.pdf:pdf},
isbn = {1010700701700},
issn = {00255610},
keywords = {Coordinate descent,Error bound,Global convergence,Linear convergence rate,Nonsmooth optimization},
number = {1-2},
pages = {387--423},
pmid = {15003161},
title = {{A coordinate gradient descent method for nonsmooth separable minimization}},
volume = {117},
year = {2009}
}
@article{Vishwanathan2007,
author = {Vishwanathan, S. V. N. and Smola, Alexander J. and Vidal, Ren{\'{e}}},
file = {:home/tom/Work/phd/Papers/Vishwanathan, Smola, Vidal_2007_Binet-Cauchy Kernels on Dynamical Systems and its Application to the Analysis of Dynamic Scenes.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {arma models and dynamical,binet-cauchy theorem,sylvester,systems},
number = {1},
pages = {95--119},
title = {{Binet-Cauchy Kernels on Dynamical Systems and its Application to the Analysis of Dynamic Scenes}},
volume = {73},
year = {2007}
}
@article{Zou2005,
author = {Zou, Hui and Hastie, Trevor},
file = {:home/tom/Work/phd/Papers/Zou, Hastie_2005_Regularization and variable selection via the elastic-net.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (statistical methodology)},
keywords = {grouping effect,lars algorithm,lasso,penalization,variable selection},
number = {2},
pages = {301--320},
title = {{Regularization and variable selection via the elastic-net}},
volume = {67},
year = {2005}
}
@inproceedings{Cadieu2008,
address = {Vancouver, Canada},
author = {Cadieu, Charles F. and Olshausen, Bruno A.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/Cadieu, Olshausen_2008_Learning Transformational Invariants from Natural Movies.pdf:pdf},
isbn = {9781605609492},
pages = {292--292},
title = {{Learning Transformational Invariants from Natural Movies}},
year = {2008}
}
@article{willemsen1990automatic,
author = {Willemsen, A and Bloemhof, F and Boom, H},
journal = {IEEE Transactions on Biomedical Engineering},
number = {12},
pages = {1201--1208},
publisher = {IEEE},
title = {{Automatic stance-swing phase detection from accelerometer data for peroneal nerve stimulation}},
volume = {37},
year = {1990}
}
@article{Duchi2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
eprint = {arXiv:1103.4296v1},
file = {:home/tom/Work/phd/Papers/Duchi, Hazan, Singer_2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@article{Ghaoui2012,
archivePrefix = {arXiv},
arxivId = {1009.4219},
author = {{El Ghaoui}, Laurent and Viallon, Vivian and Rabbani, Tarek},
eprint = {1009.4219},
file = {:home/tom/Work/phd/Papers/El Ghaoui, Viallon, Rabbani_2012_Safe feature elimination for the LASSO and sparse supervised learning problems.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {feature elimination,lasso,logistic regression,sparse regression,svm},
number = {4},
pages = {667--698},
title = {{Safe feature elimination for the LASSO and sparse supervised learning problems}},
url = {http://adsabs.harvard.edu/abs/2010arXiv1009.4219E},
volume = {8},
year = {2012}
}
@inproceedings{Frasconi1993,
address = {San Francisco, CA, USA},
author = {Frasconi, Paolo and Gori, Marco and Tesi, Alberto},
booktitle = {International Conference on Neural Networks (ICNN)},
file = {:home/tom/Work/phd/Papers/Frasconi, Gori, Tesi_1993_Backpropagation for Lineraly-Separable Patterns a Detailed Analysis(2).pdf:pdf},
pages = {1818--1822},
title = {{Backpropagation for Lineraly-Separable Patterns: a Detailed Analysis}},
year = {1993}
}
@article{xin2016maximal,
author = {Xin, Bo and Wang, Yizhou and Gao, Wen and Wipf, David},
journal = {arXiv preprint},
number = {01636},
title = {{Maximal Sparsity with Deep Networks?}},
volume = {arXiv:1605},
year = {2016}
}
@inproceedings{Freeman2017,
address = {Toulon, France},
archivePrefix = {arXiv},
arxivId = {1611.01540},
author = {Freeman, C. Daniel and Bruna, Joan},
booktitle = {International Conference on Learning Representation (ICLR)},
eprint = {1611.01540},
file = {:home/tom/Work/phd/Papers/Freeman, Bruna_2017_Topology and Geometry of Deep Rectified Network Optimization Landscapes.pdf:pdf},
title = {{Topology and Geometry of Deep Rectified Network Optimization Landscapes}},
url = {http://arxiv.org/abs/1611.01540},
year = {2017}
}
@inproceedings{Oquab2014,
address = {Colombus, OH, USA},
author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2014.222},
file = {:home/tom/Work/phd/Papers/Oquab et al._2014_Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks(2).pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
pages = {1717--1724},
title = {{Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks}},
year = {2014}
}
@inproceedings{Pascanu2014,
abstract = {We study the complexity of functions computable by deep feedforward neural networks with piece-wise linear activations in terms of the number of regions of linearity that they have. Deep networks are able to sequentially map portions of each layer's input space to the same output. In this way, deep models compute functions with a compositional structure that is able to re-use pieces of computation exponentially often in terms of their depth. This note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece-wise linear activation functions.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1869v1},
author = {Mont\'ufar, Guido F. and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1007/978-1-4471-5779-3_4},
eprint = {arXiv:1402.1869v1},
file = {:home/tom/Work/phd/Papers/Mont'ufar et al._2014_On the Number of Linear Regions of Deep Neural Networks.pdf:pdf},
isbn = {978-1-4471-5779-3; 978-1-4471-5778-6},
issn = {10495258},
keywords = {deep learning,input space partition,maxout,neural network,rectifier},
pages = {2924--2932},
title = {{On the Number of Linear Regions of Deep Neural Networks}},
year = {2014}
}
@inproceedings{Tishby2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.02406v1},
author = {Tishby, Naftali and Zaslavsky, Noga},
booktitle = {IEEE Information Theory Workshop (ITW)},
eprint = {arXiv:1503.02406v1},
file = {:home/tom/Work/phd/Papers/Tishby, Zaslavsky_2015_Deep Learning and the Information Bottleneck Principle.pdf:pdf},
title = {{Deep Learning and the Information Bottleneck Principle}},
year = {2015}
}
@inproceedings{Shalev2009,
abstract = {We describe and analyze two stochastic methods for ℓ1 regularized loss minimization problems, such as the Lasso. The ﬁrst method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature or example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets.},
address = {Montreal, Canada},
author = {Shalev-Shwartz, Shai and Tewari, A},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.1145/1553374.1553493},
file = {:home/tom/Work/phd/Papers/Shalev-Shwartz, Tewari_2009_Stochastic Methods for $ell_1$-regularized Loss Minimization.pdf:pdf},
isbn = {9781605585161},
keywords = {coordinate descent,l1 regularization,mirror descent,optimization,sparsity},
pages = {929--936},
title = {{Stochastic Methods for $\ell_1$-regularized Loss Minimization}},
url = {http://eprints.pascal-network.org/archive/00005418/},
year = {2009}
}
@inproceedings{Keogh2003,
abstract = {On pr{\'{e}}sente les biais qui peuvent apparaitre avec une m{\'{e}}thode de temporal data mining du fait des donn{\'{e}}es},
address = {Washington, United States},
author = {Keogh, Eamonn and Kasetty, Shruti},
booktitle = {SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1023/A:1024988512476},
file = {:home/tom/Work/phd/Papers/Keogh, Kasetty_2003_On the Need for Time Series Data Mining Benchmarks A Survey and Empirical Demonstration.pdf:pdf},
isbn = {158113567X},
issn = {13845810},
keywords = {Data mining,Experimental evaluation,Time series},
number = {4},
pages = {349--371},
publisher = {ACM},
title = {{On the Need for Time Series Data Mining Benchmarks: A Survey and Empirical Demonstration}},
volume = {7},
year = {2003}
}
@article{Yuan2006,
author = {Yuan, Ming and Lin, Yi},
file = {:home/tom/Work/phd/Papers/Yuan, Lin_2006_Model selection and estimation in regression with.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {analysis of variance,lasso,least angle regression,non-negative garrotte,piecewise linear solution path},
number = {1},
pages = {49--67},
title = {{Model selection and estimation in regression with}},
volume = {68},
year = {2006}
}
@article{Hebiri2013,
archivePrefix = {arXiv},
arxivId = {1204.1605},
author = {Hebiri, Mohamed and Lederer, Johannes},
eprint = {1204.1605},
file = {:home/tom/Work/phd/Papers/Hebiri, Lederer_2013_How Correlations Influence Lasso Prediction.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
keywords = {correlations,lars algorithm,lasso,restricted eigenvalue,tun-},
number = {3},
pages = {1846--1854},
title = {{How Correlations Influence Lasso Prediction}},
volume = {59},
year = {2013}
}
@article{Qu2016,
abstract = {We study the problem of minimizing the sum of a smooth convex function and a convex block-separable regularizer and propose a new randomized coordinate descent method, which we call ALPHA. Our method at every iteration updates a random subset of coordinates, following an arbitrary distribution. No coordinate descent methods capable to handle an arbitrary sampling have been studied in the literature before for this problem. ALPHA is a remarkably flexible algorithm: in special cases, it reduces to deterministic and randomized methods such as gradient descent, coordinate descent, parallel coordinate descent and distributed coordinate descent -- both in nonaccelerated and accelerated variants. The variants with arbitrary (or importance) sampling are new. We provide a complexity analysis of ALPHA, from which we deduce as a direct corollary complexity bounds for its many variants, all matching or improving best known bounds.},
archivePrefix = {arXiv},
arxivId = {1412.8060},
author = {Qu, Zheng and Richt{\'{a}}rik, Peter},
doi = {10.1080/10556788.2016.1190360},
eprint = {1412.8060},
file = {:home/tom/Work/phd/Papers/Qu, Richt{\'{a}}rik_2016_Coordinate descent with arbitrary sampling I algorithms and complexity†.pdf:pdf},
issn = {10294937},
journal = {Optimization Methods and Software},
keywords = {accelerated coordinate descent,arbitrary sampling,complexity analysis,coordinate descent},
number = {5},
pages = {829--857},
title = {{Coordinate descent with arbitrary sampling I: algorithms and complexity†}},
volume = {31},
year = {2016}
}
@inproceedings{Smaragdis2007,
author = {Smaragdis, Paris},
booktitle = {IEEE Transaction on Audio, Speech and Languages Processing},
file = {:home/tom/Work/phd/Papers/Smaragdis_2007_Convolutive Speech Bases and Their Application to Supervised Speech Separation.pdf:pdf},
number = {1},
pages = {1--12},
title = {{Convolutive Speech Bases and Their Application to Supervised Speech Separation}},
volume = {15},
year = {2007}
}
@article{Barrois2016,
author = {Barrois, R. and Gregory, Th and Oudre, Laurent and Moreau, Th and Truong, Ch and Pulini, A. Aram and Vienne, A. and Labourdette, Ch and Vayatis, N. and Buffat, S. and Yelnik, A. and {De Waele}, C. and Laporte, S. and Vidal, P. P. and Ricard, D.},
doi = {10.1371/journal.pone.0164975},
file = {:home/tom/Work/phd/Papers/Barrois et al._2016_An automated recording method in clinical consultation to rate the limp in lower limb osteoarthritis.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {10},
pages = {e0164975},
pmid = {27776168},
title = {{An automated recording method in clinical consultation to rate the limp in lower limb osteoarthritis}},
volume = {11},
year = {2016}
}
@article{Papyan2017,
abstract = {The recently proposed Multi-Layer Convolutional Sparse Coding (ML-CSC) model, consisting of a cascade of convolutional sparse layers, provides a new interpretation of Convolutional Neural Networks (CNNs). Under this framework, the computation of the forward pass in a CNN is equivalent to a pursuit algorithm aiming to estimate the nested sparse representation vectors -- or feature maps -- from a given input signal. Despite having served as a pivotal connection between CNNs and sparse modeling, a deeper understanding of the ML-CSC is still lacking: there are no pursuit algorithms that can serve this model exactly, nor are there conditions to guarantee a non-empty model. While one can easily obtain signals that approximately satisfy the ML-CSC constraints, it remains unclear how to simply sample from the model and, more importantly, how one can train the convolutional filters from real data. In this work, we propose a sound pursuit algorithm for the ML-CSC model by adopting a projection approach. We provide new and improved bounds on the stability of the solution of such pursuit and we analyze different practical alternatives to implement this in practice. We show that the training of the filters is essential to allow for non-trivial signals in the model, and we derive an online algorithm to learn the dictionaries from real data, effectively resulting in cascaded sparse convolutional layers. Last, but not least, we demonstrate the applicability of the ML-CSC model for several applications in an unsupervised setting, providing competitive results. Our work represents a bridge between matrix factorization, sparse dictionary learning and sparse auto-encoders, and we analyze these connections in detail.},
archivePrefix = {arXiv},
arxivId = {1708.08705},
author = {Sulam, Jeremias and Papyan, Vardan and Romano, Yaniv and Elad, Michael},
eprint = {1708.08705},
file = {:home/tom/Work/phd/Papers/Sulam et al._2017_Multi-Layer Convolutional Sparse Modeling Pursuit and Dictionary Learning.pdf:pdf},
journal = {arXiv preprint},
number = {08705},
title = {{Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary Learning}},
url = {http://arxiv.org/abs/1708.08705},
volume = {arXiv:1708},
year = {2017}
}
@article{brunelli1993face,
author = {Brunelli, R and Poggio, T},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {10},
pages = {1042--1052},
publisher = {IEEE Computer Society},
title = {{Face recognition: Features versus templates}},
volume = {15},
year = {1993}
}
@article{Macqueen1967,
abstract = {This paper describes a number of applications of the 'k-means', a procedure for classifying a random sample of points in E sub N. The procedure consists of starting with k groups which each consist of a single random point, and thereafter adding the points one after another to the group whose mean each point is nearest. After a point is added to a group, the mean of that group is adjusted so as to take account of the new point. Thus at each stage there are in fact k means, one for each group. After the sample is processed in this way, the points are classified on the basis of nearness to the final means. The portions which result tend to be fficient in the sense of having low within class variance. Applications are suggested for the problems of non-linear prediction, efficient communication, non-parametric tests of independence, similarity grouping, and automatic file construction. The extension of the methods to general metric spaces is indicated.},
author = {Macqueen, J},
doi = {citeulike-article-id:6083430},
file = {:home/tom/Work/phd/Papers/Macqueen_1967_Some methods for classification and analysis of multivariate observations.pdf:pdf},
isbn = {1595931619},
issn = {00970433},
journal = {Proceedings of the 5-th Berkeley Symposium on Mathematical Statistics and Probability},
number = {233},
pages = {281--297},
pmid = {17121457},
title = {{Some methods for classification and analysis of multivariate observations}},
volume = {1},
year = {1967}
}
@techreport{Author2016,
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {1608.04062},
author = {Author, Anonymous and Address, Affiliation},
eprint = {1608.04062},
file = {:home/tom/Work/phd/Papers/Author, Address_2016_Stacked Approximated Regression Machine A Simple Deep Learning Approach.pdf:pdf},
title = {{Stacked Approximated Regression Machine : A Simple Deep Learning Approach}},
year = {2016}
}
@inproceedings{Kowalski2011,
address = {Grenada, Spain},
author = {Kowalski, Matthieu and Gramfort, Alexandre and Weiss, Pierre and Anthoine, Sandrine},
booktitle = {NIPS Workshop on Optimization for Machine Learning (OPT)},
file = {:home/tom/Work/phd/Papers/Kowalski et al._2011_Accelerating ISTA with an active set strategy.pdf:pdf},
keywords = {active set strategy,elerating ista with an},
pages = {1--6},
title = {{Accelerating ISTA with an active set strategy}},
year = {2011}
}
@inproceedings{Gregor10,
address = {Haifa, Israel},
author = {Gregor, Karol and Lecun, Yann},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.1049/ip-vis:20045082},
file = {:home/tom/Work/phd/Papers/Gregor, Lecun_2010_Learning Fast Approximations of Sparse Coding Karol.pdf:pdf},
isbn = {9781605589077},
issn = {1350245X},
keywords = {deep learning,feature extraction,sparse coding},
pages = {399--406},
title = {{Learning Fast Approximations of Sparse Coding Karol}},
url = {http://link.aip.org/link/IVIPEK/v152/i3/p318/s1&Agg=doi},
year = {2010}
}
@inproceedings{Yang2011,
address = {Barcelona, Spain},
author = {Yang, Meng and Zhang, L},
booktitle = {International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2011.6126286},
file = {:home/tom/Work/phd/Papers/Yang, Zhang_2011_Fisher discrimination dictionary learning for sparse representation.pdf:pdf},
isbn = {978-1-4577-1102-2},
pages = {543--550},
publisher = {IEEE},
title = {{Fisher discrimination dictionary learning for sparse representation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126286%5Cnhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6126286},
year = {2011}
}
@article{Nesterov2005,
author = {Nesterov, Yuri},
doi = {10.1007/s10107-004-0552-5},
file = {:home/tom/Work/phd/Papers/Nesterov_2005_Smooth minimization of non-smooth functions.pdf:pdf},
isbn = {1010700405},
issn = {00255610},
journal = {Mathematical Programming},
keywords = {Complexity theory,Convex optimization,Non-smooth optimization,Optimal methods,Structural optimization},
number = {1},
pages = {127--152},
title = {{Smooth minimization of non-smooth functions}},
volume = {103},
year = {2005}
}
@mastersthesis{thuer2008step,
address = {Belgium},
author = {Th{\"{u}}er, G and Verwimp, T},
booktitle = {E-Lab Masters Thesis, from the Artesis University College of Antwerp, Antwerp, Belgium},
school = {Artesis University College of Antwerp},
title = {{Step detection algorithms for accelerometers}},
year = {2008}
}
@inproceedings{Steeg,
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.1222v1},
author = {Steeg, Greg Ver and Rey, Marina},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {arXiv:1406.1222v1},
file = {:home/tom/Work/phd/Papers/Steeg, Rey_2014_Discovering Structure in High-Dimensional Data Through Correlation Explanation.pdf:pdf},
pages = {577--585},
title = {{Discovering Structure in High-Dimensional Data Through Correlation Explanation}},
year = {2014}
}
@inproceedings{Chaudhari2017,
address = {Toulon, France},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01838v5},
author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and Lecun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
booktitle = {International Conference on Learning Representation (ICLR)},
eprint = {arXiv:1611.01838v5},
file = {:home/tom/Work/phd/Papers/Chaudhari et al._2017_Entropy-SGD Biasing Gradient Descent into Wide Valleys.pdf:pdf},
title = {{Entropy-SGD: Biasing Gradient Descent into Wide Valleys}},
year = {2017}
}
@article{Brady1989,
author = {Brady, Martin L. and Raghavan, Raghu and Slawny, Joseph},
doi = {10.1109/31.31314},
file = {:home/tom/Work/phd/Papers/Brady, Raghavan, Slawny_1989_Back Propagation Fails to Separate Where Perceptrons Succeed.pdf:pdf},
issn = {00984094},
journal = {IEEE Transactions on Circuits and Systems},
number = {5},
pages = {665--674},
title = {{Back Propagation Fails to Separate Where Perceptrons Succeed}},
volume = {36},
year = {1989}
}
@article{Mallat2016,
abstract = {Deep convolutional networks provide state of the art classifications and regressions results over many high-dimensional problems. We review their architecture, which scatters data with a cascade of linear filter weights and non-linearities. A mathematical framework is introduced to analyze their properties. Computations of invariants involve multiscale contractions, the linearization of hierarchical symmetries, and sparse separations. Applications are discussed.},
archivePrefix = {arXiv},
arxivId = {1601.04920},
author = {Mallat, St{\'{e}}phane},
doi = {10.1098/rsta.2015.0203},
eprint = {1601.04920},
file = {:home/tom/Work/phd/Papers/Mallat_2016_Understanding Deep Convolutional Networks.pdf:pdf},
isbn = {1581136625},
issn = {1364503X},
journal = {Philosophical Transaction of the Royal Society A},
number = {2065},
pmid = {26953183},
title = {{Understanding Deep Convolutional Networks}},
url = {http://arxiv.org/abs/1601.04920%0Ahttp://dx.doi.org/10.1098/rsta.2015.0203},
volume = {374},
year = {2016}
}
@book{Goodfellow2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {MIT Press},
doi = {10.1016/B978-0-12-391420-0.09987-X},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Work/phd/Papers/Goodfellow, Bengio, Courville_2016_Deep Learning.pdf:pdf},
isbn = {3540620583, 9783540620587},
issn = {1432122X},
keywords = {machine learning},
pmid = {21728107},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B978012391420009987X},
year = {2016}
}
@misc{Olshausen1997,
abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
author = {Olshausen, B a and Field, D J},
booktitle = {Vision Research},
doi = {http://dx.doi.org/10.1016/S0042-6989(97)00169-7},
file = {:home/tom/Work/phd/Papers/Olshausen, Field_1997_Sparse coding with an incomplete basis set a strategy employed by protect{V1}.pdf:pdf},
isbn = {0042-6989},
issn = {00426989},
keywords = {Coding,Gabor-wavelet,Natural images,V1},
number = {23},
pages = {3311--3325},
pmid = {9425546},
title = {{Sparse coding with an incomplete basis set: a strategy employed by \protect{V1}}},
url = {http://www.sciencedirect.com/science/article/pii/S0042698997001697},
volume = {37},
year = {1997}
}
@article{Zhang2015,
archivePrefix = {arXiv},
arxivId = {1602.07017},
author = {Zhang, Zheng and Xu, Yong and Yang, Jian and Li, Xuelong and Zhang, David},
doi = {10.1109/ACCESS.2015.2430359},
eprint = {1602.07017},
file = {:home/tom/Work/phd/Papers/Zhang et al._2015_A Survey of Sparse Representation Algorithms and Applications.pdf:pdf},
isbn = {2014041717},
issn = {21693536},
journal = {IEEE Access},
keywords = {Sparse representation,compressive sensing,constrained optimization,dictionary learning,greedy algorithm,homotopy algorithm,proximal algorithm},
pages = {490--530},
title = {{A Survey of Sparse Representation: Algorithms and Applications}},
volume = {3},
year = {2015}
}
@inproceedings{Telgarsky2016,
abstract = {For any positive integer $k$, there exist neural networks with $\Theta(k^3)$ layers, $\Theta(1)$ nodes per layer, and $\Theta(1)$ distinct parameters which can not be approximated by networks with $\mathcal{O}(k)$ layers unless they are exponentially large --- they must possess $\Omega(2^k)$ nodes. This result is proved here for a class of nodes termed "semi-algebraic gates" which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, sum-product networks, and boosted decision trees (in this last case with a stronger separation: $\Omega(2^{k^3})$ total tree nodes are required).},
address = {New-York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1602.04485},
author = {Telgarsky, Matus},
booktitle = {Conference on Learning Theory (COLT)},
eprint = {1602.04485},
file = {:home/tom/Work/phd/Papers/Telgarsky_2016_Benefits of depth in neural networks.pdf:pdf},
keywords = {1,a model of real-valued,a neural network is,approximation,as follows,computation defined by a,connected directed graph,depth hierarchy,neural networks,nodes await real numbers,on their incoming edges,representation,setting and main results,thereafter computing a function},
pages = {1--23},
title = {{Benefits of depth in neural networks}},
url = {http://arxiv.org/abs/1602.04485},
year = {2016}
}
@article{Gabor1946,
author = {Gabor, Dennis},
doi = {10.1049/ji-3-2.1946.0074},
file = {:home/tom/Work/phd/Papers/Gabor_1946_Theory of Communication.pdf:pdf},
isbn = {6173845350},
issn = {09252312},
journal = {Journal of the Institution of Electrical Engineers},
number = {26},
pages = {429--457},
pmid = {17190757},
title = {{Theory of Communication}},
url = {citeulike-article-id:4452465},
volume = {93(26)},
year = {1946}
}
@inproceedings{Oudre2011,
address = {Barcelona, Spain},
author = {Oudre, Laurent and Lung-Yut-Fong, Alexandre and Bianchi, Pascal},
booktitle = {European Signal Processing Conference},
file = {:home/tom/Work/phd/Papers/Oudre, Lung-Yut-Fong, Bianchi_2011_Segmentation of accelerometer signals recorded during continuous treadmill walking.pdf:pdf},
isbn = {2076-1465 VO -},
issn = {22195491},
pages = {1564--1568},
title = {{Segmentation of accelerometer signals recorded during continuous treadmill walking}},
year = {2011}
}
@inproceedings{Suzuki2015,
address = {Lille, France},
author = {Suzuki, Taiji},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Work/phd/Papers/Suzuki_2015_Convergence rate of Bayesian tensor estimator and its minimax optimality.pdf:pdf},
pages = {1--9},
title = {{Convergence rate of Bayesian tensor estimator and its minimax optimality}},
year = {2015}
}
@article{LoPresti2016,
abstract = {In recent years, there has been a proliferation of works on human action classification from depth sequences. These works generally present methods and/or feature representations for the classification of actions from sequences of 3D locations of human body joints and/or other sources of data, such as depth maps and RGB videos. This survey highlights motivations and challenges of this very recent research area by presenting technologies and approaches for 3D skeleton-based action classification. The work focuses on aspects such as data pre-processing, publicly available benchmarks and commonly used accuracy measurements. Furthermore, this survey introduces a categorization of the most recent works in 3D skeleton-based action classification according to the adopted feature representation. This paper aims at being a starting point for practitioners who wish to approach the study of 3D action classification and gather insights on the main challenges to solve in this emerging field.},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.0402},
author = {{Lo Presti}, Liliana and {La Cascia}, Marco},
doi = {10.1016/j.patcog.2015.11.019},
eprint = {arXiv:1212.0402},
file = {:home/tom/Work/phd/Papers/Lo Presti, La Cascia_2016_3D skeleton-based human action classification A survey.pdf:pdf},
isbn = {9783319106052},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Action classification,Action recognition,Body joint,Body pose representation,Skeleton},
pages = {130--147},
publisher = {Elsevier},
title = {{3D skeleton-based human action classification: A survey}},
url = {http://dx.doi.org/10.1016/j.patcog.2015.11.019},
volume = {53},
year = {2016}
}
@article{Oculo16,
author = {Hung, GK and Semmlow, JL and Ciuffreda, KJ},
journal = {IEEE Transaction on Biomedical Engineering},
number = {33},
pages = {1021--1028},
title = {{A dual-mode dynamic model of the vergence eye movement system}},
volume = {Nov},
year = {1986}
}
@article{oymak2015sharp,
author = {Oymak, Samet and Recht, Benjamin and Soltanolkotabi, Mahdi},
journal = {arXiv preprint},
number = {04793},
title = {{Sharp Time--Data Tradeoffs for Linear Inverse Problems}},
volume = {arXiv:1507},
year = {2015}
}
@article{Gray2006,
author = {Gray, Robert M},
file = {:home/tom/Work/phd/Papers/Gray_2006_Toeplitz and Circulant Matrices A Review(2).pdf:pdf},
journal = {Foundations and Trends in Communications and Information Theory},
number = {3},
pages = {155--239},
title = {{Toeplitz and Circulant Matrices: A Review}},
volume = {2},
year = {2006}
}
@techreport{Saux2015,
author = {Saux, Patrick},
file = {:home/tom/Work/phd/Papers/Saux_2015_Rough Paths Theory A Signature-Based Approach to Data Analysis.pdf:pdf},
pages = {1----60},
title = {{Rough Paths Theory A Signature-Based Approach to Data Analysis}},
year = {2015}
}
@inproceedings{Shah2015,
address = {Montreal, Canada},
author = {Shah, Parikshit and Rao, Nikhil and Tang, Gongguo},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {2548--2556},
title = {{Sparse and Low-Rank Tensor Decomposition}},
year = {2015}
}
@misc{Werbos1982,
author = {Werbos, PaulJ.},
booktitle = {System Modeling and Optimization},
doi = {10.1007/BFb0006203},
file = {:home/tom/Work/phd/Papers/Werbos_1982_Applications of advances in nonlinear sensitivity analysis.pdf:pdf},
isbn = {978-3-540-11691-2},
pages = {762--770},
title = {{Applications of advances in nonlinear sensitivity analysis}},
volume = {38},
year = {1982}
}
@inproceedings{You2016,
address = {Barcelona, Spain},
author = {You, Yang and Lian, Xiangru and Liu, Ji and Yu, Hsiang-Fu and Dhillon, Inderjit S. and Demmel, James and Hsieh, Cho-Jui},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Work/phd/Papers/You et al._2016_Asynchronous Parallel Greedy Coordinate Descent._2016_Asynchronous Parallel Greedy Coordinate Descent:_2016_Asynchronous Parallel Greedy Coordinate Descent},
pages = {4682--4690},
title = {{Asynchronous Parallel Greedy Coordinate Descent}},
year = {2016}
}
@article{Oculo21b,
author = {May, EF and Truxal, AR},
journal = {Journal of Neuro-Ophthalmology},
number = {17},
pages = {84--85},
title = {{Loss of vision alone may result in seesaw nystagmus.}},
volume = {Jun},
year = {1997}
}
@inproceedings{saxe2011random,
address = {Bellevue, United States},
author = {Saxe, Andrew and Koh, Pang W and Chen, Zhenghao and Bhand, Maneesh and Suresh, Bipin and Ng, Andrew Y},
booktitle = {International Conference on Machine Learning (ICML)},
pages = {1089--1096},
title = {{On random weights and unsupervised feature learning}},
year = {2011}
}
@article{Mairal2012,
archivePrefix = {arXiv},
arxivId = {1009.5358},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean},
doi = {10.1109/TPAMI.2011.156},
eprint = {1009.5358},
file = {:home/tom/Work/phd/Papers/Mairal, Bach, Ponce_2012_Task-driven dictionary learning.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Basis pursuit,Lasso,compressed sensing,dictionary learning,matrix factorization,semi-supervised learning},
number = {4},
pages = {791--804},
pmid = {21808090},
title = {{Task-driven dictionary learning}},
volume = {34},
year = {2012}
}
@article{Nesterov1983,
author = {Nesterov, Yuri},
journal = {Soviet Mathematics Doklady},
number = {2},
pages = {372--376},
title = {{A method of solving a convex programming problem with convergence rate O (1/k2)}},
volume = {27},
year = {1983}
}
@article{Donoho2002,
author = {Donoho, David L and Elad, Michael},
file = {:home/tom/Work/phd/Papers/Donoho, Elad_2002_Maximal Sparsity Representation via l 1 Minimization.pdf:pdf},
journal = {submitted to IEEE Transactions on Information Theory},
keywords = {atomic decomposition,basic pursuit,convex optimization,linear,matching pursuit,programming,sparse representation},
pages = {1--28},
title = {{Maximal Sparsity Representation via l 1 Minimization}},
year = {2002}
}
@article{Group2001,
author = {Group, CW},
journal = {The National Eye Institute Publications},
title = {{A national eye institute sponsored workshop and publication on the classification of eye movement abnormalities and strabismus (CEMAS)}},
year = {2001}
}
@article{Anden2014,
archivePrefix = {arXiv},
arxivId = {1304.6763},
author = {And{\'{e}}n, Joakim and Mallat, St{\'{e}}phane},
doi = {10.1109/TSP.2014.2326991},
eprint = {1304.6763},
file = {:home/tom/Work/phd/Papers/And{\'{e}}n, Mallat_2014_Deep Scattering Spectrum.pdf:pdf},
journal = {IEEE Transaction on Signal Processing},
number = {16},
pages = {4114--4128},
title = {{Deep Scattering Spectrum}},
url = {http://arxiv.org/abs/1304.6763%0Ahttp://dx.doi.org/10.1109/TSP.2014.2326991},
volume = {62},
year = {2014}
}
@inproceedings{Zhang2016,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
address = {Toulon, France},
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
booktitle = {International Conference on Learning Representation (ICLR)},
doi = {10.1109/TKDE.2015.2507132},
eprint = {1611.03530},
file = {:home/tom/Work/phd/Papers/Zhang et al._2017_Understanding deep learning requires rethinking generalization.pdf:pdf},
isbn = {1506.02142},
issn = {10414347},
pmid = {88045},
title = {{Understanding deep learning requires rethinking generalization}},
url = {http://arxiv.org/abs/1611.03530},
year = {2017}
}
@article{Fercoq2016,
author = {Fercoq, Olivier and Richtarik, Peter},
file = {:home/tom/Work/phd/Papers/Fercoq, Richtarik_2016_Optimization in High Dimensions via Accelerated , Parallel , and Proximal Coordinate Descent.pdf:pdf},
journal = {SIAM Journal on Optimization},
number = {4},
pages = {1997----2023},
title = {{Optimization in High Dimensions via Accelerated , Parallel , and Proximal Coordinate Descent}},
volume = {25},
year = {2016}
}
@article{auvinet2002reference,
author = {Auvinet, B and Berrut, G and Touzard, C and Moutel, L and Collet, N and Chaleil, D and Barrey, E},
journal = {Gait \& posture},
number = {2},
pages = {124--134},
publisher = {Elsevier},
title = {{Reference data for normal subjects obtained with an accelerometric device}},
volume = {16},
year = {2002}
}
@article{Brockwell1988,
author = {Brockwell, Peter J. and Davis, Richard A.},
doi = {10.1016/0304-4149(88)90063-4},
file = {:home/tom/Work/phd/Papers/Brockwell, Davis_1988_Simple consistent estimation of the coefficients of a linear filter.pdf:pdf},
issn = {03044149},
journal = {Stochastic Processes and their Applications},
keywords = {ARMA process,asymptotic distribution,identification,innovations,linear filter,linear process,preliminary estimation},
number = {1},
pages = {47--59},
title = {{Simple consistent estimation of the coefficients of a linear filter}},
volume = {28},
year = {1988}
}
@misc{Neyshabur2017a,
abstract = {In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.},
archivePrefix = {arXiv},
arxivId = {1709.01953},
author = {Neyshabur, Behnam},
eprint = {1709.01953},
file = {:home/tom/Work/phd/Papers/Neyshabur_2017_Implicit Regularization in Deep Learning.pdf:pdf},
howpublished = {PhD Thesis},
title = {{Implicit Regularization in Deep Learning}},
url = {http://arxiv.org/abs/1709.01953},
year = {2017}
}
@article{Papadakis2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1304.5784v2},
author = {Papadakis, Nicolas and Peyr{\'{e}}, Gabriel and Oudet, Edouard},
doi = {10.1137/130920058},
eprint = {arXiv:1304.5784v2},
file = {:home/tom/Work/phd/Papers/Papadakis, Peyr{\'{e}}, Oudet_2014_Optimal transport with proximal splitting.pdf:pdf},
issn = {19364954},
journal = {SIAM Journal on Imaging Sciences},
keywords = {Deconvolution,transport},
mendeley-tags = {Deconvolution,transport},
pages = {1--22},
title = {{Optimal transport with proximal splitting}},
url = {http://epubs.siam.org/doi/abs/10.1137/130920058},
year = {2014}
}
@book{Mallat2008,
author = {Mallat, St\'ephane},
file = {:home/tom/Work/phd/Papers/Mallat_2008_A Wavelet Tour of Signal Processing.pdf:pdf},
isbn = {9780123743701},
publisher = {Academic press},
title = {{A Wavelet Tour of Signal Processing}},
year = {2008}
}
@article{Oculo23,
author = {Mossman, SS and Bronstein, Alex M. and Gresty, M. and Kendall, B and Rudge, P},
journal = {Archive Ophthalmology},
number = {47},
pages = {357--359},
title = {{Convergence nystagmus associated with Arnold-Chiari malformation}},
volume = {MAr},
year = {1990}
}
@inproceedings{Keogh2001,
abstract = {In recent years, there has been an explosion of interest in mining time series databases. As with most computer science problems, representation of the data is the key to efficient and effective solutions. One of the most commonly used representations is piecewise linear approximation. This representation has been used by various researchers to support clustering, classification, indexing and association rule mining of time series data. A variety of algorithms have been proposed to obtain this representation, with several algorithms having been independently rediscovered several times. In this paper, we undertake the first extensive review and empirical comparison of all proposed iechniques. We show that all these algorithms have fatal flaws from a data mining perspective. We introduce a novel algorithm that we empirically show to be superior to all others in the literature.},
address = {San Jose, United States},
author = {Keogh, E. and Chu, S. and Hart, D. and Pazzani, M.},
booktitle = {IEEE International Conference on Data Mining (ICDM)},
doi = {10.1109/ICDM.2001.989531},
file = {:home/tom/Work/phd/Papers/Keogh et al._2001_An online algorithm for segmenting time series.pdf:pdf},
isbn = {0-7695-1119-8},
issn = {15504786},
pages = {289--296},
publisher = {IEEE},
title = {{An online algorithm for segmenting time series}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=989531},
year = {2001}
}
@inproceedings{Jung2014,
archivePrefix = {arXiv},
arxivId = {1402.4078},
author = {Jung, Alexander and Eldar, Yonina C. and G{\"{o}}rtz, Norbert},
booktitle = {European Signal Processing Conference (EUSIPCO)},
eprint = {1402.4078},
file = {:home/tom/Work/phd/Papers/Jung, Eldar, G{\"{o}}rtz_2014_Performance Limits of Dictionary Learning for Sparse Coding.pdf:pdf},
title = {{Performance Limits of Dictionary Learning for Sparse Coding}},
url = {http://arxiv.org/abs/1402.4078},
year = {2014}
}
@article{Zanella2009,
abstract = {Random matrices play a crucial role in the design and analysis of multiple-input multiple-output (MIMO) systems. In particular, performance of MIMO systems depends on the statistical properties of a subclass of random matrices known as Wishart when the propagation environment is characterized by Rayleigh or Rician fading. This paper focuses on the stochastic analysis of this class of matrices and proposes a general methodology to evaluate some multiple nested integrals of interest. With this methodology we obtain a closed-form expression for the joint probability density function of k consecutive ordered eigenvalues and, as a special case, the PDF of the lscrth ordered eigenvalue of Wishart matrices. The distribution of the largest eigenvalue can be used to analyze the performance of MIMO maximal ratio combining systems. The PDF of the smallest eigenvalue can be used for MIMO antenna selection techniques. Finally, the PDF the kth largest eigenvalue finds applications in the performance analysis of MIMO singular value decomposition systems.},
author = {Zanella, Alberto and Chiani, Marco and Win, Moe Z.},
doi = {10.1109/TCOMM.2009.04.070143},
file = {:home/tom/Work/phd/Papers/Zanella, Chiani, Win_2009_On the marginal distribution of the eigenvalues of wishart matrices(2).pdf:pdf},
isbn = {9781424420742},
issn = {00906778},
journal = {IEEE Transactions on Communications},
keywords = {Eeigenvalue distribution,Marginal distribution,Multiple-input multiple-output (MIMO),Wishart matrices},
number = {4},
pages = {1050--1060},
title = {{On the marginal distribution of the eigenvalues of wishart matrices}},
volume = {57},
year = {2009}
}
@article{Candes2011,
archivePrefix = {arXiv},
arxivId = {0912.3599},
author = {Candes, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
eprint = {0912.3599},
file = {:home/tom/Work/phd/Papers/Candes et al._2011_Robust Principal Component Analysis.pdf:pdf},
isbn = {0780362780},
issn = {0899-7667},
journal = {Journal of the Association for Computing Machinery (JACM)},
keywords = {duality,low-rank matrices,minimization,nuclear-norm minimization,principal components,robustness vis-a-vis outliers,sparsity,video surveillance},
number = {3},
pages = {11},
title = {{Robust Principal Component Analysis?}},
url = {http://arxiv.org/abs/0912.3599},
volume = {58},
year = {2011}
}
@misc{Nemirovski2012,
author = {Nemirovski, Arkadi},
file = {:home/tom/Work/phd/Papers/Nemirovski_2012_Tutorial Mirror Descent Algorithms for Large-Scale Deterministic and stochastic convex Optimization.pdf:pdf},
title = {{Tutorial: Mirror Descent Algorithms for Large-Scale Deterministic and stochastic convex Optimization}},
year = {2012}
}
@techreport{krizhevsky2009learning,
author = {Krizhevsky, Alex},
institution = {University of Toronto},
title = {{Learning multiple layers of features from tiny images}},
type = {Master's thesis},
year = {2009}
}
@inproceedings{Becker2012,
archivePrefix = {arXiv},
arxivId = {1206.1156},
author = {Becker, Stephen and {Jalal Fadili}, M},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1206.1156},
pages = {1--9},
title = {{A quasi-Newton proximal splitting method}},
year = {2012}
}
@article{Buchstaber1994,
author = {Buchstaber, V M},
journal = {Translations of the American Mathematical Society-Series 2},
pages = {1--18},
publisher = {Providence [etc.] American Mathematical Society, 1949-},
title = {{Time series analysis and grassmannians}},
volume = {162},
year = {1994}
}
@article{Schwartz-ziv2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.00810v3},
author = {Schwartz-ziv, Ravid and Tishby, Naftali},
eprint = {arXiv:1703.00810v3},
file = {:home/tom/Work/phd/Papers/Schwartz-ziv, Tishby_2017_Opening the black box of Deep Neural Networks via Information.pdf:pdf},
journal = {arXiv preprint},
number = {00810},
title = {{Opening the black box of Deep Neural Networks via Information}},
volume = {arXiv:1703},
year = {2017}
}
@article{Papyan2016a,
archivePrefix = {arXiv},
arxivId = {1607.02005},
author = {Papyan, Vardan and Sulam, Jeremias and Elad, Michael},
eprint = {1607.02005},
file = {:home/tom/Work/phd/Papers/Papyan, Sulam, Elad_2016_Working Locally Thinking Globally - Part II Theoretical Guarantees for Convolutional Sparse Coding.pdf:pdf;:home/tom/Work/phd/Papers/Papyan, Sulam, Elad_2016_Working Locally Thinking Globally - Part II Theoretical Guarantees for Convolutional Sparse Coding.pdf:pdf},
journal = {arXiv preprint},
number = {02009},
title = {{Working Locally Thinking Globally - Part II: Theoretical Guarantees for Convolutional Sparse Coding}},
url = {http://arxiv.org/abs/1607.02005},
volume = {arXiv:1607},
year = {2016}
}
@article{pan1985real,
author = {Pan, J and Tompkins, W J},
journal = {IEEE Transactions on Biomedical Engineering},
number = {3},
pages = {230--236},
publisher = {IEEE},
title = {{A real-time {QRS} detection algorithm}},
volume = {32},
year = {1985}
}
@inproceedings{Rolfe2013,
abstract = {We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.},
address = {Scottsdale, AZ, USA},
archivePrefix = {arXiv},
arxivId = {1301.3775},
author = {Rolfe, Jason Tyler and LeCun, Yan},
booktitle = {International Conference on Learning Representation (ICLR)},
eprint = {1301.3775},
file = {:home/tom/Work/phd/Papers/Rolfe, LeCun_2013_Discriminative Recurrent Sparse Auto-Encoders.pdf:pdf},
pages = {15},
title = {{Discriminative Recurrent Sparse Auto-Encoders}},
url = {http://arxiv.org/abs/1301.3775},
year = {2013}
}
@book{Combettes2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Combettes, Patrick L and Bauschke, Heinz H.},
booktitle = {Springer Science & Business Media},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Work/phd/Papers/Combettes, Bauschke_2011_Convex Analysis and Monotone Operator Theory in Hilbert Spaces.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {1--468},
pmid = {25246403},
publisher = {Springer},
title = {{Convex Analysis and Monotone Operator Theory in Hilbert Spaces}},
year = {2011}
}
@article{Mahoney2011,
abstract = {Randomized algorithms for very large matrix problems have received a great deal of attention in recent years. Much of this work was motivated by problems in large-scale data analysis. Although this work had its origins within theoretical computer science, where researchers were interested in proving worst-case bounds, i.e., bounds without any assumptions at all on the input data, researchers from numerical linear algebra, statistics, applied mathematics, data analysis, and machine learning, as well as domain scientists have subsequently extended and applied these methods in important ways. Although this has been great for the development of the area and for the technology transfer of theoretical ideas into practical applications, this interdisciplinarity has thus far sometimes obscured the underlying simplicity and generality of the core ideas. This review will provide a detailed overview of recent work on randomized algorithms for matrix problems, with an emphasis on a few simple core ideas that underlie not only recent theoretical advances but also the usefulness of these tools in large-scale data applications. Crucial in this context is the connection with concept of statistical leverage. This concept has long been used in statistical regression diagnostics to identify outliers; and it has recently proved crucial in the development of improved worst-case matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists. This connection arises naturally when one explicitly decouples the effect of randomization in these matrix algorithms from the underlying linear algebraic structure. This decoupling also permits much finer control in the application of randomization, as well as the easier exploitation of domain knowledge.},
archivePrefix = {arXiv},
arxivId = {1104.5557},
author = {Mahoney, Michael W.},
doi = {10.1561/2200000035},
eprint = {1104.5557},
file = {:home/tom/Work/phd/Papers/Mahoney_2011_Randomized algorithms for matrices and data.pdf:pdf},
isbn = {9781601985064},
issn = {1935-8237},
journal = {Foundations and Trends in Machine Learning},
number = {2},
pages = {123----224},
title = {{Randomized algorithms for matrices and data}},
url = {http://arxiv.org/abs/1104.5557},
volume = {3},
year = {2011}
}
@article{andreao2007combining,
author = {Andre{\~{a}}o, R V and Boudy, J},
journal = {EURASIP Journal on Applied Signal Processing},
number = {1},
pages = {95--103},
title = {{Combining wavelet transform and hidden Markov models for ECG segmentation}},
volume = {2007},
year = {2007}
}
@article{Blumensath2012,
author = {Blumensath, Thomas},
doi = {10.1016/j.sigpro.2011.09.017},
file = {:home/tom/Work/phd/Papers/Blumensath_2012_Accelerated iterative hard thresholding.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Compressed sensing,Iterative hard thresholding},
number = {3},
pages = {752--756},
publisher = {Elsevier},
title = {{Accelerated iterative hard thresholding}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0165168411003197},
volume = {92},
year = {2012}
}
@inproceedings{Chen2016,
address = {Phoenix, United States},
author = {Chen, Boheng and Li, Jie and Ma, Biyun and Gang, Wei},
booktitle = {IEEE International Conference on Image Processing (ICIP)},
file = {:home/tom/Work/phd/Papers/Chen et al._2016_Convolutional Sparse Coding Classification Model for Image Classification.pdf:pdf},
pages = {1918----1922},
title = {{Convolutional Sparse Coding Classification Model for Image Classification}},
year = {2016}
}
@inproceedings{Chaudhry2013,
address = {Melbourne, Australia},
author = {Chaudhry, Rizwan and Vidal, Ren{\'{e}}},
booktitle = {IEEE Conference on Decision and Control (CDC)},
doi = {10.1109/CDC.2013.6760735},
file = {:home/tom/Work/phd/Papers/Chaudhry, Vidal_2013_Initial-state invariant binet-cauchy kernels for the comparison of linear dynamical systems.pdf:pdf},
isbn = {9781467357173},
issn = {01912216},
pages = {5377--5384},
title = {{Initial-state invariant binet-cauchy kernels for the comparison of linear dynamical systems}},
year = {2013}
}
@article{Hornik1989,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Work/phd/Papers/Hornik, Stinchcombe, White_1989_Multilayer feedforward networks are universal approximators.pdf:pdf},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{Hahnloser2003,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hahnloser, Richard H R and Seung, H Sebastian and Slotine, Jean-Jacques},
doi = {10.1162/089976603321192103},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Work/phd/Papers/Hahnloser, Seung, Slotine_2003_Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks.pdf:pdf},
isbn = {0899-7667 (Print)\r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural Computation},
number = {3},
pages = {621--638},
pmid = {12620160},
title = {{Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks}},
url = {http://www.mitpressjournals.org/doi/10.1162/089976603321192103},
volume = {15},
year = {2003}
}
@article{Oculo9,
author = {Gamlin, P and Mitchell, K},
journal = {Society of Neuroscience Abstr.},
pages = {346},
title = {{Reversible lesions of nucleus reticularis tegmenti pontis affect convergence and ocular accommodation}},
year = {1993}
}
@inproceedings{Zhong2014,
abstract = {We consider the class of optimization problems arising from computationally intensive L1-regularized M-estimators, where the function or gradient values are very expensive to compute. A particular instance of interest is the L1-regularized MLE for learning Conditional Random Fields (CRFs), which are a popular class of statistical models for varied structured prediction problems such as sequence labeling, alignment, and classification with label taxonomy. L1-regularized MLEs for CRFs are particularly expensive to optimize since computing the gradient values requires an expensive inference step. In this work, we propose the use of a carefully constructed proximal quasi-Newton algorithm for such computationally intensive M-estimation problems, where we employ an aggressive active set selection technique. In a key contribution of the paper, we show that the proximal quasi-Newton method is provably super-linearly convergent, even in the absence of strong convexity, by leveraging a restricted variant of strong convexity. In our experiments, the proposed algorithm converges considerably faster than current state-of-the-art on the problems of sequence labeling and hierarchical classification.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.7321v1},
author = {Zhong, Kai and Yen, Ian E.H. and Dhillon, Inderjit S. and Ravikumar, Pradeep},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {arXiv:1406.7321v1},
file = {:home/tom/Work/phd/Papers/Zhong et al._2014_Proximal Quasi-Newton for Computationally Intensive $ell_1$-regularizedM-estimators.pdf:pdf},
issn = {10495258},
pages = {2375--2383},
title = {{Proximal Quasi-Newton for Computationally Intensive $\ell_1$-regularizedM-estimators}},
year = {2014}
}
@article{Sun2015,
address = {Lille, France},
author = {Sun, Ju and Qu, Qinq and Wright, John},
file = {:home/tom/Work/phd/Papers/Sun, Qu, Wright_2015_Complete Dictionary Recovery Using Nonconvex Optimization.pdf:pdf},
journal = {International Conference on Machine Learning (ICML)},
title = {{Complete Dictionary Recovery Using Nonconvex Optimization}},
volume = {37},
year = {2015}
}
@inproceedings{Fyshe2012,
address = {La Palma, Canary Islands},
author = {Fyshe, Alona and Fox, Emily B and Dunson, David B and Mitchell, Tom M},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
file = {:home/tom/Work/phd/Papers/Fyshe et al._2012_Hierarchical Latent Dictionaries for Models of Brain Activation.pdf:pdf},
pages = {409--421},
title = {{Hierarchical Latent Dictionaries for Models of Brain Activation}},
url = {http://jmlr.csail.mit.edu/proceedings/papers/v22/fyshe12.html},
year = {2012}
}
@article{Cason2011,
author = {Cason, T. P. and Absil, P. A. and {Van Dooren}, P.},
doi = {10.1007/978-94-007-0602-6_4},
file = {:home/tom/Work/phd/Papers/Cason, Absil, Van Dooren_2011_Comparing two matrices by means of isometric projections.pdf:pdf},
isbn = {9789400706019},
issn = {18761100},
journal = {Lecture Notes in Electrical Engineering},
pages = {77--93},
title = {{Comparing two matrices by means of isometric projections}},
volume = {80 LNEE},
year = {2011}
}
@article{ben2015comparison,
author = {{Ben Mansour}, K and Rezzoug, N and Gorce, P},
journal = {Computer methods in biomechanics and biomedical engineering},
pages = {1--2},
publisher = {Taylor \& Francis},
title = {{Comparison between several locations of gyroscope for gait events detection}},
year = {2015}
}
@article{Oculo32,
author = {Toledano, H and Muhsinoglu, O and Luckman, J and Goldenberg-Cohen, N and Michowiz, S},
journal = {European Journal of Pediatric Neurology},
number = {19},
pages = {694--700},
title = {{Acquired nystagmus as the initial presenting sign of chiasmal glioma in young children}},
volume = {Nov},
year = {2015}
}
@article{Haeffele2017a,
archivePrefix = {arXiv},
arxivId = {1708.07850},
author = {Haeffele, Benjamin D. and Vidal, Rene},
eprint = {1708.07850},
file = {:home/tom/Work/phd/Papers/Haeffele, Vidal_2017_Structured Low-Rank Matrix Factorization Global Optimality, Algorithms, and Applications.pdf:pdf},
journal = {arXiv preprint},
number = {07850},
title = {{Structured Low-Rank Matrix Factorization: Global Optimality, Algorithms, and Applications}},
url = {http://arxiv.org/abs/1708.07850},
volume = {arXiv:1708},
year = {2017}
}
@article{rabiner1989tutorial,
author = {Rabiner, Lawrence},
journal = {Proceedings of the IEEE},
number = {2},
pages = {257--286},
publisher = {IEEE},
title = {{A tutorial on hidden Markov models and selected applications in speech recognition}},
volume = {77},
year = {1989}
}
@article{erhan2010does,
author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
journal = {Journal of Machine Learning Research (JMLR)},
number = {Feb},
pages = {625--660},
title = {{Why does unsupervised pre-training help deep learning?}},
volume = {11},
year = {2010}
}
@inproceedings{Moreau2017,
abstract = {Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, that are optimal in the class of first-order methods for non-smooth, convex functions, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). However, these methods don't exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks was proposed in \cite{Gregor10}, coined LISTA, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately. In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.},
archivePrefix = {arXiv},
arxivId = {1609.00285},
author = {Moreau, Thomas and Bruna, Joan},
booktitle = {International Conference on Learning Representation (ICLR)},
eprint = {1609.00285},
file = {:home/tom/Work/phd/Papers/Moreau, Bruna_2017_Understanding Neural Sparse Coding with Matrix Factorization.pdf:pdf},
title = {{Understanding Neural Sparse Coding with Matrix Factorization}},
year = {2017}
}
@article{Gilles2013,
author = {Gilles, Jerome},
doi = {10.1109/TSP.2013.2265222},
file = {:home/tom/Work/phd/Papers/Gilles_2013_Empirical wavelet transform.pdf:pdf},
isbn = {1053-587X VO - 61},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Adaptive filtering,empirical mode decomposition,wavelet},
number = {16},
pages = {3999--4010},
title = {{Empirical wavelet transform}},
volume = {61},
year = {2013}
}
@article{Oculo31,
author = {Thompson, DA and Liasis, A},
journal = {Journal of Pediatric Ophthalmology and Strabismus},
pages = {55--62},
title = {{Visual electrophysiology: how it can help you and your patient}},
volume = {4},
year = {2012}
}
@article{bengio2007scaling,
author = {Bengio, Yoshua and LeCun, Yann},
journal = {Large-scale kernel machines},
number = {5},
pages = {1--41},
title = {{Scaling learning algorithms towards AI}},
volume = {34},
year = {2007}
}
@inproceedings{Spielman2013,
abstract = {We consider the problem of learning sparsely used dictionaries with an arbitrary square dictionary and a random, sparse coefficient matrix. We prove that $O (n \log n)$ samples are sufficient to uniquely determine the coefficient matrix. Based on this proof, we design a polynomial-time algorithm, called Exact Recovery of Sparsely-Used Dictionaries (ER-SpUD), and prove that it probably recovers the dictionary and coefficient matrix when the coefficient matrix is sufficiently sparse. Simulation results show that ER-SpUD reveals the true dictionary as well as the coefficients with probability higher than many state-of-the-art algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.5882v1},
author = {Spielman, Daniel A. and Wang, Huan and Wright, John},
booktitle = {Conference on Learning Theroy (COLT)},
eprint = {arXiv:1206.5882v1},
file = {:home/tom/Work/phd/Papers/Spielman, Wang, Wright_2013_Exact recovery of sparsely-used dictionaries.pdf:pdf},
isbn = {9781577356332},
issn = {10450823},
keywords = {dictionary learning,matrix decomposition,matrix sparsification},
pages = {3087--3090},
title = {{Exact recovery of sparsely-used dictionaries}},
volume = {23},
year = {2013}
}
@article{Duval2013,
archivePrefix = {arXiv},
arxivId = {1306.6909},
author = {Duval, Vincent and Peyr{\'{e}}, Gabriel},
eprint = {1306.6909},
file = {:home/tom/Work/phd/Papers/Duval, Peyr{\'{e}}_2013_Exact support recovery for sparse spikes deconvolution.pdf:pdf},
issn = {00405736},
journal = {arXiv preprint},
keywords = {Deconvolution,transport},
mendeley-tags = {Deconvolution,transport},
number = {6909},
title = {{Exact support recovery for sparse spikes deconvolution}},
volume = {arXiv:1306},
year = {2013}
}
@inproceedings{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1411.1792},
file = {:home/tom/Work/phd/Papers/Yosinski et al._2014_How transferable are features in deep neural networks.pdf:pdf},
pages = {3320--3328},
title = {{How transferable are features in deep neural networks?}},
url = {http://arxiv.org/abs/1411.1792},
year = {2014}
}
@inproceedings{Abalov2014,
address = {Cox's Bazar, Bangladesh},
author = {Abalov, N V and Gubarev, V V},
booktitle = {International Forum on Strategic Technology (IFOST)},
file = {:home/tom/Work/phd/Papers/Abalov, Gubarev_2014_Automated grouping of decomposition components of time series for singular spectrum analysis.pdf:pdf},
keywords = {considerable amount of manual,dialog-based input from,grouping,identification,singular spectrum analysis,ssa,time seires},
title = {{Automated grouping of decomposition components of time series for singular spectrum analysis}},
year = {2014}
}
@article{Oculo1,
author = {Arnoldi, KA and Tychsen, L.},
journal = {Journal of Pediatric Ophthalmology and Strabismus},
number = {32},
pages = {296--301},
title = {{Prevalence of intracranial lesions in children initially diagnosed with disconjugate nystagmus (spasmus nutans).}},
volume = {Sep-Oct},
year = {1995}
}
@article{Mallat2012,
abstract = {Pattern classification often requires using translation invariant representations, which are stable and hence Lipschitz continuous to deformations. A Fourier transform does not provide such Lipschitz stability. Scattering operators are obtained by iterating on wavelet transforms and modulus operators. The resulting representation is proved to be translation invariant and Lipschitz continuous to deformations, up to a log term. It is computed with a non-linear convolution network, which scatters functions along an infinite set of paths. Invariance to the action of any compact Lie subgroup of the general linear group is obtained with a combined scattering, which iterates over wavelet transforms defined on this group. Scattering representations yield new metrics on stationary processes, which are stable to random deformations.},
archivePrefix = {arXiv},
arxivId = {1101.2286},
author = {Mallat, St{\'{e}}phane},
doi = {10.1002/cpa.21413},
eprint = {1101.2286},
file = {:home/tom/Work/phd/Papers/Mallat_2012_Group Invariant Scattering.pdf:pdf},
issn = {00103640},
journal = {Communications on Pure and Applied Mathematics},
number = {10},
pages = {1331--1398},
title = {{Group Invariant Scattering}},
volume = {65},
year = {2012}
}
@article{Nesterov2010,
author = {Nesterov, Yuri},
file = {:home/tom/Work/phd/Papers/Nesterov_2012_Efficiency of coordinate descent methods on huge-scale optimization problems.pdf:pdf},
journal = {SIAM Journal on Optimization},
keywords = {1 universit{\'{e}} catholique de,and,b-1348 louvain-la-neuve,be,belgium,convex optimization,coordinate relaxation,core and inma,e-mail,fast gradient,google problem,louvain,member of ecore,nesterov,schemes,the association between core,this author is also,uclouvain,worst-case efficiency estimates,yurii},
number = {2},
pages = {341--362},
title = {{Efficiency of coordinate descent methods on huge-scale optimization problems}},
volume = {22},
year = {2012}
}
@inproceedings{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
address = {Corfu, Greece},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, D.G.},
booktitle = {International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.1999.790410},
eprint = {0112017},
file = {:home/tom/Work/phd/Papers/Lowe_1999_Object recognition from local scale-invariant features.pdf:pdf},
isbn = {0-7695-0164-8},
issn = {0-7695-0164-8},
pages = {1150--1157},
pmid = {15806121},
primaryClass = {cs},
publisher = {IEEE},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/document/790410/},
year = {1999}
}
@article{Goldfarb2013,
archivePrefix = {arXiv},
arxivId = {0912.4571},
author = {Goldfarb, Donald and Ma, Shiqian and Scheinberg, Katya},
doi = {10.1007/s10107-012-0530-2},
eprint = {0912.4571},
file = {:home/tom/Work/phd/Papers/Goldfarb, Ma, Scheinberg_2013_Fast alternating linearization methods for minimizing the sum of two convex functions.pdf:pdf},
isbn = {0025-5610},
issn = {00255610},
journal = {Mathematical Programming},
keywords = {Alternating direction method,Alternating linearization method,Augmented Lagrangian method,Convex optimization,Gauss-Seidel method,Optimal gradient method,Peaceman-Rachford method,Variable splitting},
number = {1-2},
pages = {349--382},
title = {{Fast alternating linearization methods for minimizing the sum of two convex functions}},
volume = {141},
year = {2013}
}
@article{Oculo30,
author = {Smith, JL and Flynn, JT and Spiro, HJ},
journal = {Journal of Clinical Neuro-Ophthalmology},
number = {2},
pages = {85--91},
title = {{Monocular vertical oscillations of amblyopia: The Heimann- Bielschowsky phenomenon}},
volume = {Jun},
year = {1982}
}
@article{Kane2010,
abstract = {We give two different and simple constructions for dimensionality reduction in $\ell_2$ via linear mappings that are sparse: only an $O(\varepsilon)$-fraction of entries in each column of our embedding matrices are non-zero to achieve distortion $1+\varepsilon$ with high probability, while still achieving the asymptotically optimal number of rows. These are the first constructions to provide subconstant sparsity for all values of parameters, improving upon previous works of Achlioptas (JCSS 2003) and Dasgupta, Kumar, and Sarl\'{o}s (STOC 2010). Such distributions can be used to speed up applications where $\ell_2$ dimensionality reduction is used.},
archivePrefix = {arXiv},
arxivId = {1012.1577},
author = {Kane, Daniel M. and Nelson, Jelani},
eprint = {1012.1577},
file = {:home/tom/Work/phd/Papers/Kane, Nelson_2014_Sparser Johnson-Lindenstrauss Transforms.pdf:pdf},
journal = {Journal of the Association for Computing Machinery (JACM)},
number = {1},
pages = {1--23},
title = {{Sparser Johnson-Lindenstrauss Transforms}},
url = {http://arxiv.org/abs/1012.1577},
volume = {61},
year = {2014}
}
@article{Oculo10,
author = {Good, Wv and Jan, JE and Hoyt, CS and Billson, FA and Schoettker, PJ and Klaeger, K},
journal = {Developmental Medicine and Child Neurology},
number = {39},
pages = {421--424},
title = {{Monocular vision loss can cause bilateral nystagmus in young children}},
volume = {Jun},
year = {1997}
}
@article{Beck2009,
abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an ex- tension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising nu- merical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude. Key},
annote = {FISTA is a proximal algorithm (ISTA) with a nesterov momentum.
This paper give convergence rate for divers algorithms.},
author = {Beck, Amir and Teboulle, Marc},
doi = {10.1137/080716542},
file = {:home/tom/Work/phd/Papers/Beck, Teboulle_2009_A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems.pdf:pdf},
isbn = {SJISBI000002000001000183000001},
issn = {1936-4954},
journal = {SIAM Journal on Imaging Sciences},
keywords = {algorithms,ams subject classifications,deconvolution,global rate of convergence,image deblurring,iterative shrinkage-thresholding algorithm,l 1 regularization problems,least squares and,linear inverse problem,optimal gradient method,two-step iterative},
number = {1},
pages = {183--202},
pmid = {18005159},
title = {{A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems}},
volume = {2},
year = {2009}
}
@article{Zhou2013,
abstract = {Classical regression methods treat covariates as a vector and estimate a corresponding vector of regression coefficients. Modern applications in medical imaging generate covariates of more complex form such as multidimensional arrays (tensors). Traditional statistical and computational methods are proving insufficient for analysis of these high-throughput data due to their ultrahigh dimensionality as well as complex structure. In this article, we propose a new family of tensor regression models that efficiently exploit the special structure of tensor covariates. Under this framework, ultrahigh dimensionality is reduced to a manageable level, resulting in efficient estimation and prediction. A fast and highly scalable estimation algorithm is proposed for maximum likelihood estimation and its associated asymptotic properties are studied. Effectiveness of the new methods is demonstrated on both synthetic and real MRI imaging data.},
archivePrefix = {arXiv},
arxivId = {1203.3209},
author = {Zhou, Hua and Li, Lexin and Zhu, Hongtu},
doi = {10.1080/01621459.2013.776499},
eprint = {1203.3209},
file = {:home/tom/Work/phd/Papers/Zhou, Li, Zhu_2013_Tensor regression with applications in neuroimaging data analysis.pdf:pdf},
isbn = {978-1-909493-43-8},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Brain imaging,Dimension reduction,Generalized linear model,Magnetic resonance imaging,Multidimensional array,Tensor regression},
number = {502},
pages = {540--552},
pmid = {24791032},
title = {{Tensor regression with applications in neuroimaging data analysis}},
volume = {108},
year = {2013}
}
@article{Vautard1989,
author = {Vautard, Robert and Ghil, Michael},
file = {:home/tom/Work/phd/Papers/Vautard, Ghil_1989_Deterministic chaos, stochastic processes, and dimension.pdf:pdf},
journal = {Physica D: Nonlinear Phenomena},
number = {3},
pages = {395--424},
title = {{Deterministic chaos, stochastic processes, and dimension}},
volume = {35},
year = {1989}
}
